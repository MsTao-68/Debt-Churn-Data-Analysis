{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3819834-0605-46e1-a4dc-410ecab26191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb  # 模型\n",
    "import pandas as pd  # 数据处理包\n",
    "import numpy as np  # 数据处理包\n",
    "from sklearn import metrics  # 混淆句子\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split  # 分层五折验证包、寻找最优参函数、切分数据\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix  # 准确率、roc计算、auc计算、混淆矩阵\n",
    "import itertools  # 处理混淆矩阵\n",
    "import gc  # 处理缓存，有兴趣的可以搜搜怎么使用\n",
    "import warnings  # 忽略普通警告，不打印太多东西\n",
    "from sklearn.feature_selection import RFE, RFECV  # 递归消除选特征，前者是自己选优化到多少位，后者是自动cv优化到最佳\n",
    "from imblearn.under_sampling import RandomUnderSampler  # 朴素随机过采样，由于是比较旧的这里不做例子\n",
    "from imblearn.over_sampling import SMOTE, ADASYN  # 目前流行的过采样\n",
    "# SMOTE: 对于少数类样本a, 随机选择一个最近邻的样本b, 然后从a与b的连线上随机选取一个点c作为新的少数类样本;\n",
    "# ADASYN: 关注的是在那些基于K最近邻分类器被错误分类的原始样本附近生成新的少数类样本;\n",
    "\n",
    "\n",
    "def over_smote_(X, y, num):\n",
    "    \"\"\"\n",
    "    功能: 二分类过采样，以smote举例。\n",
    "    why: 当正负样本比例相差过大时，一般为1：20以内。举例：如果正负样本为1：99，那么相当于告诉模型只要判断为负，则正确率就为99%，那么模型就会这么做。\n",
    "    X: 数据X（df型/无label）\n",
    "    y: 数据y（df型/label）\n",
    "    num: 过采样的个数\n",
    "    reture: \n",
    "        X_resampled: 过采样后的X\n",
    "        y_resampled: 过采样后的y\n",
    "    \"\"\"\n",
    "    ss = pd.Series(y).value_counts()\n",
    "    smote = SMOTE(sampling_strategy={0:ss[0],1:ss[1]+num},random_state=2019)  # radom_state为随机值种子，1:ss[1]+表示label为1的数据增加多少个\n",
    "    # adasyn = ADASYN(sampling_strategy={0:ss[0],1:ss[1]+800},random_state=2019) # 改变正样本数量参数\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    print(\"过采样个数为：\", num)\n",
    "    check_num_X = X_resampled.shape[0] - X.shape[0]\n",
    "    check_num_y = y_resampled.shape[0] - y.shape[0]\n",
    "    if (check_num_X == check_num_y) and (check_num_X == num):\n",
    "        print(\"过采样校验：成功\")\n",
    "        return X_resampled, y_resampled\n",
    "    else:\n",
    "        print(\"过采样校验：失败\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7188129-2fbc-421a-86ed-c71abb06d48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>AGN_CNT_RCT_12_MON</th>\n",
       "      <th>ICO_CUR_MON_ACM_TRX_TM</th>\n",
       "      <th>NB_RCT_3_MON_LGN_TMS_AGV</th>\n",
       "      <th>AGN_AGR_LATEST_AGN_AMT</th>\n",
       "      <th>ICO_CUR_MON_ACM_TRX_AMT</th>\n",
       "      <th>PUB_TO_PRV_TRX_AMT_CUR_YEAR</th>\n",
       "      <th>MON_12_EXT_SAM_NM_TRSF_OUT_CNT</th>\n",
       "      <th>MON_12_EXT_SAM_AMT</th>\n",
       "      <th>MON_12_TRX_AMT_MAX_AMT_PCTT</th>\n",
       "      <th>...</th>\n",
       "      <th>REG_DT</th>\n",
       "      <th>OPN_TM</th>\n",
       "      <th>HLD_FGN_CCY_ACT_NBR</th>\n",
       "      <th>CAGR_YEARLY_AGV_TRX_CNT</th>\n",
       "      <th>MON_12_EXR_SAM_NET_AMT</th>\n",
       "      <th>ENCASH_CNTR_RATIO</th>\n",
       "      <th>MON_12_ACT_NET_50_UP_CNT_PTY_QTY</th>\n",
       "      <th>AGN_CUR_YEAR_WAGE_RATIO</th>\n",
       "      <th>MON_12_ACM_NET_ACT_CNT</th>\n",
       "      <th>DMD_GAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2642.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>425.3</td>\n",
       "      <td>759421.45</td>\n",
       "      <td>25879985.3</td>\n",
       "      <td>660732.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>-134350571.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1708.45</td>\n",
       "      <td>416.84</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.569538</td>\n",
       "      <td>134350573.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>0.736807</td>\n",
       "      <td>3430.0</td>\n",
       "      <td>-2.009249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2282.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>178.7</td>\n",
       "      <td>1964626.40</td>\n",
       "      <td>14755499.8</td>\n",
       "      <td>126608.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>199.42</td>\n",
       "      <td>195.87</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.043937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.601778</td>\n",
       "      <td>-2700.0</td>\n",
       "      <td>-1.286316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2642.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>178.7</td>\n",
       "      <td>4599822.30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1135.55</td>\n",
       "      <td>1122.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.056218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736807</td>\n",
       "      <td>-320.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2642.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>759421.45</td>\n",
       "      <td>11620267.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>...</td>\n",
       "      <td>98.45</td>\n",
       "      <td>92.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.077670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>0.736807</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2642.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>178.7</td>\n",
       "      <td>759421.45</td>\n",
       "      <td>11620267.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>...</td>\n",
       "      <td>728.13</td>\n",
       "      <td>355.87</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.121698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736807</td>\n",
       "      <td>-840.0</td>\n",
       "      <td>-0.056457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL  AGN_CNT_RCT_12_MON  ICO_CUR_MON_ACM_TRX_TM  \\\n",
       "0    0.0              2642.0                    72.0   \n",
       "1    0.0              2282.0                   222.0   \n",
       "2    0.0              2642.0                     2.0   \n",
       "3    1.0              2642.0                   102.0   \n",
       "4    1.0              2642.0                   102.0   \n",
       "\n",
       "   NB_RCT_3_MON_LGN_TMS_AGV  AGN_AGR_LATEST_AGN_AMT  ICO_CUR_MON_ACM_TRX_AMT  \\\n",
       "0                     425.3               759421.45               25879985.3   \n",
       "1                     178.7              1964626.40               14755499.8   \n",
       "2                     178.7              4599822.30                      2.0   \n",
       "3                      22.0               759421.45               11620267.6   \n",
       "4                     178.7               759421.45               11620267.6   \n",
       "\n",
       "   PUB_TO_PRV_TRX_AMT_CUR_YEAR  MON_12_EXT_SAM_NM_TRSF_OUT_CNT  \\\n",
       "0                     660732.0                           492.0   \n",
       "1                     126608.3                             2.0   \n",
       "2                          2.0                             2.0   \n",
       "3                          2.0                             2.0   \n",
       "4                          2.0                             2.0   \n",
       "\n",
       "   MON_12_EXT_SAM_AMT  MON_12_TRX_AMT_MAX_AMT_PCTT  ...   REG_DT   OPN_TM  \\\n",
       "0        -134350571.8                          2.1  ...  1708.45   416.84   \n",
       "1                 2.0                          2.0  ...   199.42   195.87   \n",
       "2                 2.0                          2.2  ...  1135.55  1122.00   \n",
       "3                 2.0                          3.1  ...    98.45    92.00   \n",
       "4                 2.0                          2.2  ...   728.13   355.87   \n",
       "\n",
       "   HLD_FGN_CCY_ACT_NBR  CAGR_YEARLY_AGV_TRX_CNT  MON_12_EXR_SAM_NET_AMT  \\\n",
       "0                  2.0                 0.569538             134350573.8   \n",
       "1                  2.0                -0.043937                     0.0   \n",
       "2                  2.0                 0.056218                     0.0   \n",
       "3                  2.0                -0.077670                     0.0   \n",
       "4                  2.0                -0.121698                     0.0   \n",
       "\n",
       "   ENCASH_CNTR_RATIO  MON_12_ACT_NET_50_UP_CNT_PTY_QTY  \\\n",
       "0                1.0                            -180.0   \n",
       "1                1.0                              10.0   \n",
       "2                1.0                               0.0   \n",
       "3                1.0                             -10.0   \n",
       "4                1.0                               0.0   \n",
       "\n",
       "   AGN_CUR_YEAR_WAGE_RATIO  MON_12_ACM_NET_ACT_CNT   DMD_GAP  \n",
       "0                 0.736807                  3430.0 -2.009249  \n",
       "1                 0.601778                 -2700.0 -1.286316  \n",
       "2                 0.736807                  -320.0  0.000000  \n",
       "3                 0.736807                    30.0  0.000000  \n",
       "4                 0.736807                  -840.0 -0.056457  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('cleaned_train1.csv', index_col =0, header = 0)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3a7734-5840-4d3d-a8ea-cddb1c1b41cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGN_CNT_RCT_12_MON</th>\n",
       "      <th>ICO_CUR_MON_ACM_TRX_TM</th>\n",
       "      <th>NB_RCT_3_MON_LGN_TMS_AGV</th>\n",
       "      <th>AGN_AGR_LATEST_AGN_AMT</th>\n",
       "      <th>ICO_CUR_MON_ACM_TRX_AMT</th>\n",
       "      <th>PUB_TO_PRV_TRX_AMT_CUR_YEAR</th>\n",
       "      <th>MON_12_EXT_SAM_NM_TRSF_OUT_CNT</th>\n",
       "      <th>MON_12_EXT_SAM_AMT</th>\n",
       "      <th>MON_12_TRX_AMT_MAX_AMT_PCTT</th>\n",
       "      <th>CUR_YEAR_PUB_TO_PRV_TRX_PTY_CNT</th>\n",
       "      <th>...</th>\n",
       "      <th>REG_DT</th>\n",
       "      <th>OPN_TM</th>\n",
       "      <th>HLD_FGN_CCY_ACT_NBR</th>\n",
       "      <th>CAGR_YEARLY_AGV_TRX_CNT</th>\n",
       "      <th>MON_12_EXR_SAM_NET_AMT</th>\n",
       "      <th>ENCASH_CNTR_RATIO</th>\n",
       "      <th>MON_12_ACT_NET_50_UP_CNT_PTY_QTY</th>\n",
       "      <th>AGN_CUR_YEAR_WAGE_RATIO</th>\n",
       "      <th>MON_12_ACM_NET_ACT_CNT</th>\n",
       "      <th>DMD_GAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4602.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>205.3</td>\n",
       "      <td>87638.20</td>\n",
       "      <td>25246672.6</td>\n",
       "      <td>6102.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2865.87</td>\n",
       "      <td>2061.68</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.113021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-880.0</td>\n",
       "      <td>-0.126089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2642.0</td>\n",
       "      <td>2292.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>759421.45</td>\n",
       "      <td>324547596.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>-1200816.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2066.52</td>\n",
       "      <td>1640.39</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-0.108507</td>\n",
       "      <td>1200818.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>0.736807</td>\n",
       "      <td>-3780.0</td>\n",
       "      <td>-0.406949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90162.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>178.7</td>\n",
       "      <td>60669325.80</td>\n",
       "      <td>147173922.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>506.52</td>\n",
       "      <td>486.19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-740.0</td>\n",
       "      <td>-0.012881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3562.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>128.7</td>\n",
       "      <td>4099888.10</td>\n",
       "      <td>1825444.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>943.61</td>\n",
       "      <td>934.58</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.449134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.842087</td>\n",
       "      <td>-2570.0</td>\n",
       "      <td>-2.663800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>412.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>58.7</td>\n",
       "      <td>9000002.00</td>\n",
       "      <td>9000002.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>282.00</td>\n",
       "      <td>182.32</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-170.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGN_CNT_RCT_12_MON  ICO_CUR_MON_ACM_TRX_TM  NB_RCT_3_MON_LGN_TMS_AGV  \\\n",
       "0              4602.0                   102.0                     205.3   \n",
       "1              2642.0                  2292.0                    1442.0   \n",
       "2             90162.0                   102.0                     178.7   \n",
       "3              3562.0                   402.0                     128.7   \n",
       "4               412.0                    12.0                      58.7   \n",
       "\n",
       "   AGN_AGR_LATEST_AGN_AMT  ICO_CUR_MON_ACM_TRX_AMT  \\\n",
       "0                87638.20               25246672.6   \n",
       "1               759421.45              324547596.7   \n",
       "2             60669325.80              147173922.3   \n",
       "3              4099888.10                1825444.0   \n",
       "4              9000002.00                9000002.0   \n",
       "\n",
       "   PUB_TO_PRV_TRX_AMT_CUR_YEAR  MON_12_EXT_SAM_NM_TRSF_OUT_CNT  \\\n",
       "0                       6102.0                             2.0   \n",
       "1                          2.0                           632.0   \n",
       "2                          2.0                             2.0   \n",
       "3                          2.0                             2.0   \n",
       "4                          2.0                             2.0   \n",
       "\n",
       "   MON_12_EXT_SAM_AMT  MON_12_TRX_AMT_MAX_AMT_PCTT  \\\n",
       "0                 2.0                          3.1   \n",
       "1          -1200816.4                          2.3   \n",
       "2                 2.0                          2.1   \n",
       "3                 2.0                          2.2   \n",
       "4                 2.0                          2.0   \n",
       "\n",
       "   CUR_YEAR_PUB_TO_PRV_TRX_PTY_CNT  ...   REG_DT   OPN_TM  \\\n",
       "0                             12.0  ...  2865.87  2061.68   \n",
       "1                             42.0  ...  2066.52  1640.39   \n",
       "2                             42.0  ...   506.52   486.19   \n",
       "3                             42.0  ...   943.61   934.58   \n",
       "4                             42.0  ...   282.00   182.32   \n",
       "\n",
       "   HLD_FGN_CCY_ACT_NBR  CAGR_YEARLY_AGV_TRX_CNT  MON_12_EXR_SAM_NET_AMT  \\\n",
       "0                 12.0                -0.113021                     0.0   \n",
       "1                 32.0                -0.108507               1200818.4   \n",
       "2                  2.0                 0.012132                     0.0   \n",
       "3                  2.0                -0.449134                     0.0   \n",
       "4                  2.0                 0.019048                     0.0   \n",
       "\n",
       "   ENCASH_CNTR_RATIO  MON_12_ACT_NET_50_UP_CNT_PTY_QTY  \\\n",
       "0                1.0                              10.0   \n",
       "1                1.0                             -60.0   \n",
       "2                1.0                              50.0   \n",
       "3                1.0                              10.0   \n",
       "4                1.0                             -10.0   \n",
       "\n",
       "   AGN_CUR_YEAR_WAGE_RATIO  MON_12_ACM_NET_ACT_CNT   DMD_GAP  \n",
       "0                 1.000000                  -880.0 -0.126089  \n",
       "1                 0.736807                 -3780.0 -0.406949  \n",
       "2                 1.000000                  -740.0 -0.012881  \n",
       "3                 0.842087                 -2570.0 -2.663800  \n",
       "4                 1.000000                  -170.0  0.000000  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('cleaned_test1.csv', index_col = 0, header = 0)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0a7838-17e5-4334-8e3d-48077d7a2705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样个数为： 20000\n",
      "过采样校验：成功\n"
     ]
    }
   ],
   "source": [
    "X_resampled, y_resampled = over_smote_(df1.iloc[:, 1:], df1['LABEL'], 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7b249d80-1ad2-4490-9a2c-9ad511735df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGN_CNT_RCT_12_MON</th>\n",
       "      <th>AGN_AGR_LATEST_AGN_AMT</th>\n",
       "      <th>PUB_TO_PRV_TRX_AMT_CUR_YEAR</th>\n",
       "      <th>MON_12_EXT_SAM_AMT</th>\n",
       "      <th>CUR_MON_COR_DPS_MON_DAY_AVG_BAL</th>\n",
       "      <th>REG_CPT</th>\n",
       "      <th>REG_DT</th>\n",
       "      <th>OPN_TM</th>\n",
       "      <th>MON_12_ACM_NET_ACT_CNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4602.0</td>\n",
       "      <td>87638.20</td>\n",
       "      <td>6102.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19937726.2</td>\n",
       "      <td>1.678688e+08</td>\n",
       "      <td>2865.87</td>\n",
       "      <td>2061.68</td>\n",
       "      <td>-880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2642.0</td>\n",
       "      <td>759421.45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1200816.4</td>\n",
       "      <td>62706772.3</td>\n",
       "      <td>5.000000e+07</td>\n",
       "      <td>2066.52</td>\n",
       "      <td>1640.39</td>\n",
       "      <td>-3780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90162.0</td>\n",
       "      <td>60669325.80</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7019872.5</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>506.52</td>\n",
       "      <td>486.19</td>\n",
       "      <td>-740.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3562.0</td>\n",
       "      <td>4099888.10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>635059193.8</td>\n",
       "      <td>1.000000e+09</td>\n",
       "      <td>943.61</td>\n",
       "      <td>934.58</td>\n",
       "      <td>-2570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>412.0</td>\n",
       "      <td>9000002.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2705598.7</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>282.00</td>\n",
       "      <td>182.32</td>\n",
       "      <td>-170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>2642.0</td>\n",
       "      <td>33541914.10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-209810998.0</td>\n",
       "      <td>142916.0</td>\n",
       "      <td>8.366700e+09</td>\n",
       "      <td>1242.65</td>\n",
       "      <td>817.16</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>2642.0</td>\n",
       "      <td>759421.45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>137154592.8</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>728.13</td>\n",
       "      <td>869.74</td>\n",
       "      <td>-220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>2642.0</td>\n",
       "      <td>759421.45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7161.2</td>\n",
       "      <td>5.000000e+08</td>\n",
       "      <td>997.48</td>\n",
       "      <td>202.65</td>\n",
       "      <td>-30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>2642.0</td>\n",
       "      <td>759421.45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>166200002.0</td>\n",
       "      <td>14055711.4</td>\n",
       "      <td>2.500000e+08</td>\n",
       "      <td>527.81</td>\n",
       "      <td>219.42</td>\n",
       "      <td>-30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>2642.0</td>\n",
       "      <td>759421.45</td>\n",
       "      <td>10330002.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4973031.5</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>622.65</td>\n",
       "      <td>617.48</td>\n",
       "      <td>-540.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AGN_CNT_RCT_12_MON  AGN_AGR_LATEST_AGN_AMT  \\\n",
       "0                  4602.0                87638.20   \n",
       "1                  2642.0               759421.45   \n",
       "2                 90162.0             60669325.80   \n",
       "3                  3562.0              4099888.10   \n",
       "4                   412.0              9000002.00   \n",
       "...                   ...                     ...   \n",
       "11995              2642.0             33541914.10   \n",
       "11996              2642.0               759421.45   \n",
       "11997              2642.0               759421.45   \n",
       "11998              2642.0               759421.45   \n",
       "11999              2642.0               759421.45   \n",
       "\n",
       "       PUB_TO_PRV_TRX_AMT_CUR_YEAR  MON_12_EXT_SAM_AMT  \\\n",
       "0                           6102.0                 2.0   \n",
       "1                              2.0          -1200816.4   \n",
       "2                              2.0                 2.0   \n",
       "3                              2.0                 2.0   \n",
       "4                              2.0                 2.0   \n",
       "...                            ...                 ...   \n",
       "11995                          2.0        -209810998.0   \n",
       "11996                          2.0                 2.0   \n",
       "11997                          2.0                 2.0   \n",
       "11998                          2.0         166200002.0   \n",
       "11999                   10330002.0                52.0   \n",
       "\n",
       "       CUR_MON_COR_DPS_MON_DAY_AVG_BAL       REG_CPT   REG_DT   OPN_TM  \\\n",
       "0                           19937726.2  1.678688e+08  2865.87  2061.68   \n",
       "1                           62706772.3  5.000000e+07  2066.52  1640.39   \n",
       "2                            7019872.5  1.000000e+08   506.52   486.19   \n",
       "3                          635059193.8  1.000000e+09   943.61   934.58   \n",
       "4                            2705598.7  1.000000e+07   282.00   182.32   \n",
       "...                                ...           ...      ...      ...   \n",
       "11995                         142916.0  8.366700e+09  1242.65   817.16   \n",
       "11996                      137154592.8  1.000000e+08   728.13   869.74   \n",
       "11997                           7161.2  5.000000e+08   997.48   202.65   \n",
       "11998                       14055711.4  2.500000e+08   527.81   219.42   \n",
       "11999                        4973031.5  1.000000e+08   622.65   617.48   \n",
       "\n",
       "       MON_12_ACM_NET_ACT_CNT  \n",
       "0                      -880.0  \n",
       "1                     -3780.0  \n",
       "2                      -740.0  \n",
       "3                     -2570.0  \n",
       "4                      -170.0  \n",
       "...                       ...  \n",
       "11995                    20.0  \n",
       "11996                  -220.0  \n",
       "11997                   -30.0  \n",
       "11998                   -30.0  \n",
       "11999                  -540.0  \n",
       "\n",
       "[12000 rows x 9 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = ['AGN_CNT_RCT_12_MON', \n",
    "       'AGN_AGR_LATEST_AGN_AMT', \n",
    "       'PUB_TO_PRV_TRX_AMT_CUR_YEAR', 'MON_12_EXT_SAM_AMT',\n",
    "       'CUR_MON_COR_DPS_MON_DAY_AVG_BAL', \n",
    "       'REG_CPT', 'REG_DT', 'OPN_TM', \n",
    "       'MON_12_ACM_NET_ACT_CNT']\n",
    "X_resampled2 = X_resampled[train_features]\n",
    "X_resampled2\n",
    "X_pred2 = df2[train_features]\n",
    "X_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f5941bae-f085-4760-8fe1-3691a4dc59b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGN_CNT_RCT_12_MON</th>\n",
       "      <th>AGN_AGR_LATEST_AGN_AMT</th>\n",
       "      <th>PUB_TO_PRV_TRX_AMT_CUR_YEAR</th>\n",
       "      <th>MON_12_EXT_SAM_AMT</th>\n",
       "      <th>CUR_MON_COR_DPS_MON_DAY_AVG_BAL</th>\n",
       "      <th>REG_CPT</th>\n",
       "      <th>REG_DT</th>\n",
       "      <th>OPN_TM</th>\n",
       "      <th>MON_12_ACM_NET_ACT_CNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2642.000000</td>\n",
       "      <td>7.594214e+05</td>\n",
       "      <td>6.607320e+05</td>\n",
       "      <td>-1.343506e+08</td>\n",
       "      <td>4.400083e+06</td>\n",
       "      <td>1.200000e+09</td>\n",
       "      <td>1708.450000</td>\n",
       "      <td>416.840000</td>\n",
       "      <td>3430.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2282.000000</td>\n",
       "      <td>1.964626e+06</td>\n",
       "      <td>1.266083e+05</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>4.009026e+07</td>\n",
       "      <td>5.000002e+06</td>\n",
       "      <td>199.420000</td>\n",
       "      <td>195.870000</td>\n",
       "      <td>-2700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2642.000000</td>\n",
       "      <td>4.599822e+06</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.447626e+06</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>1135.550000</td>\n",
       "      <td>1122.000000</td>\n",
       "      <td>-320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2642.000000</td>\n",
       "      <td>7.594214e+05</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.231850e+07</td>\n",
       "      <td>5.000020e+05</td>\n",
       "      <td>98.450000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2642.000000</td>\n",
       "      <td>7.594214e+05</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.120476e+08</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>728.130000</td>\n",
       "      <td>355.870000</td>\n",
       "      <td>-840.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>297.599628</td>\n",
       "      <td>1.211496e+05</td>\n",
       "      <td>4.601538e+04</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>5.343307e+06</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>1573.873839</td>\n",
       "      <td>1505.822114</td>\n",
       "      <td>-864.523113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>4546.183896</td>\n",
       "      <td>1.721275e+06</td>\n",
       "      <td>1.522481e+06</td>\n",
       "      <td>-3.548538e+07</td>\n",
       "      <td>4.153997e+06</td>\n",
       "      <td>1.253478e+08</td>\n",
       "      <td>1938.993743</td>\n",
       "      <td>926.836155</td>\n",
       "      <td>-1561.390387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>2642.000000</td>\n",
       "      <td>7.594214e+05</td>\n",
       "      <td>1.000495e+06</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>5.243493e+06</td>\n",
       "      <td>3.000000e+07</td>\n",
       "      <td>661.937927</td>\n",
       "      <td>641.939476</td>\n",
       "      <td>-844.433553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>2642.000000</td>\n",
       "      <td>7.594214e+05</td>\n",
       "      <td>8.241645e+05</td>\n",
       "      <td>-4.619167e+05</td>\n",
       "      <td>4.522325e+06</td>\n",
       "      <td>5.000002e+06</td>\n",
       "      <td>1489.087994</td>\n",
       "      <td>1487.056047</td>\n",
       "      <td>-700.840969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>1170.785612</td>\n",
       "      <td>3.391114e+05</td>\n",
       "      <td>1.562379e+03</td>\n",
       "      <td>5.000002e+06</td>\n",
       "      <td>5.428601e+06</td>\n",
       "      <td>1.985457e+08</td>\n",
       "      <td>719.756810</td>\n",
       "      <td>343.238825</td>\n",
       "      <td>-119.164508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AGN_CNT_RCT_12_MON  AGN_AGR_LATEST_AGN_AMT  \\\n",
       "0             2642.000000            7.594214e+05   \n",
       "1             2282.000000            1.964626e+06   \n",
       "2             2642.000000            4.599822e+06   \n",
       "3             2642.000000            7.594214e+05   \n",
       "4             2642.000000            7.594214e+05   \n",
       "...                   ...                     ...   \n",
       "59995          297.599628            1.211496e+05   \n",
       "59996         4546.183896            1.721275e+06   \n",
       "59997         2642.000000            7.594214e+05   \n",
       "59998         2642.000000            7.594214e+05   \n",
       "59999         1170.785612            3.391114e+05   \n",
       "\n",
       "       PUB_TO_PRV_TRX_AMT_CUR_YEAR  MON_12_EXT_SAM_AMT  \\\n",
       "0                     6.607320e+05       -1.343506e+08   \n",
       "1                     1.266083e+05        2.000000e+00   \n",
       "2                     2.000000e+00        2.000000e+00   \n",
       "3                     2.000000e+00        2.000000e+00   \n",
       "4                     2.000000e+00        2.000000e+00   \n",
       "...                            ...                 ...   \n",
       "59995                 4.601538e+04        2.000000e+00   \n",
       "59996                 1.522481e+06       -3.548538e+07   \n",
       "59997                 1.000495e+06        2.000000e+00   \n",
       "59998                 8.241645e+05       -4.619167e+05   \n",
       "59999                 1.562379e+03        5.000002e+06   \n",
       "\n",
       "       CUR_MON_COR_DPS_MON_DAY_AVG_BAL       REG_CPT       REG_DT  \\\n",
       "0                         4.400083e+06  1.200000e+09  1708.450000   \n",
       "1                         4.009026e+07  5.000002e+06   199.420000   \n",
       "2                         1.447626e+06  1.000000e+08  1135.550000   \n",
       "3                         1.231850e+07  5.000020e+05    98.450000   \n",
       "4                         2.120476e+08  1.000000e+08   728.130000   \n",
       "...                                ...           ...          ...   \n",
       "59995                     5.343307e+06  1.000000e+08  1573.873839   \n",
       "59996                     4.153997e+06  1.253478e+08  1938.993743   \n",
       "59997                     5.243493e+06  3.000000e+07   661.937927   \n",
       "59998                     4.522325e+06  5.000002e+06  1489.087994   \n",
       "59999                     5.428601e+06  1.985457e+08   719.756810   \n",
       "\n",
       "            OPN_TM  MON_12_ACM_NET_ACT_CNT  \n",
       "0       416.840000             3430.000000  \n",
       "1       195.870000            -2700.000000  \n",
       "2      1122.000000             -320.000000  \n",
       "3        92.000000               30.000000  \n",
       "4       355.870000             -840.000000  \n",
       "...            ...                     ...  \n",
       "59995  1505.822114             -864.523113  \n",
       "59996   926.836155            -1561.390387  \n",
       "59997   641.939476             -844.433553  \n",
       "59998  1487.056047             -700.840969  \n",
       "59999   343.238825             -119.164508  \n",
       "\n",
       "[60000 rows x 9 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5b3e12be-f5ba-4cc6-9852-9e44634c4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # 数据处理包\n",
    "import numpy as np  # 数据处理包\n",
    "import matplotlib.pyplot as plt  # 图形处理包\n",
    "import gc  # 处理缓存，有兴趣的可以搜搜怎么使用\n",
    "import warnings  # 忽略普通警告，不打印太多东西\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.sans-serif']=['SimHei']  # 让图形可以显示中文\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "import seaborn as sns  # 画图工具包\n",
    "\n",
    "\n",
    "def corr_plt(data, feats, start=0, end=20, png_savename=0):\n",
    "    \"\"\"\n",
    "    功能: 画相关系数热力图。\n",
    "    why: 大于0.75的特征只留一个，不然会造成特征冗余模型效果差，但是现实情况中，一般去掉其中一个就会导致模型效果变差，请慎用。\n",
    "    data: 数据集（df型）\n",
    "    feats: 特征集（list性/一般是去掉id和label），可用该方法生成 feats = [x for x in data.columns if x not in ['id','label']]\n",
    "    start: 用以画相关系数特征的开始点，默认0（int型）\n",
    "    end: 用以画相关系数特征的结束点，默认30，-1为最终点（int型）\n",
    "    png_savename: 保存图片的名字，默认不保存（str型）\n",
    "    reture:\n",
    "        输出相关系数图，可返回图片\n",
    "    \"\"\"\n",
    "    corr = data[feats[start:end]].corr()  # 获取相关系数值 \n",
    "    plt.figure(figsize=(20,10))    # 画布大小\n",
    "    sns.heatmap(corr, annot=True)   ### 画出热力图， annot是否在方格中置入值\n",
    "    if png_savename:\n",
    "        plt.savefig(\"%s_相关系数图.png\" % png_savename)  # 保存相关系数图\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "42a58277-5e35-42ff-ad1e-85c198fb72a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJUAAALSCAYAAABkueAwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAADEE0lEQVR4nOzdd3xUVfrH8e+TGIUESCKQENZVUXQFaVKlhdBRdwUsiAXLqlgAu4LrCojYRUVUMKBiQ+lSV6lBmhQBF12XVX9WlOKKJAFdNTm/P2YIKTPJDGZmksnn7Wtezpw898w5987cO5x5zhlzzgkAAAAAAAAIRkykGwAAAAAAAIDKh0ElAAAAAAAABI1BJQAAAAAAAASNQSUAAAAAAAAEjUElAAAAAAAABI1BJQAAAAAAAASNQSUAAAAAAAAEjUElAAAAAACAKsDMUs1sdSl/jzOzhWa2zsz+WlZ9DCoBAAAAAABEOTNLlvSypIRSwoZJ2uyc6yDpz2ZWs7Q6jyrH9gEh8ev3/+ci3QZIfzrt/Eg3AV47c7+PdBNQSEJctUg3AV7OcbmoKH7O+zXSTYBXvsuPdBPgZbJINwGF5OXnRboJ8Pr1l51R/eYI579nj6578nWSBhcqynTOZRZ6nCfpIknzSqkmQ9II7/11klpLWukvmEElAAAAAACASs47gJRZyt+zJcms1HG8BEk7vfezJaWWFsygEgAAAAAAQChUvqy4XEnVJe2XVMP72C/WVAIAAAAAAIAkvS+pk/d+c0lflBZMphIAAAAAAEAVY2bdJDV2zj1TqPhlSYvNrLOkxpI2lFYHg0oAAAAAAAChUAF/MME5l+H9/wpJK4r97Usz6ylPttJI51yp8/cYVAIAAAAAAIAkyTn3raQZgcQyqAQAAAAAABAK+RUvU6k8sVA3AAAAAAAAgkamEgAAAAAAQAi4CrimUnkiUwkAAAAAAABBI1MJAAAAAAAgFFhTCQAAAAAAACiKTCUAAAAAAIBQYE0lAAAAAAAAoCgylQAAAAAAAEIhPy/SLQgpMpUAAAAAAAAQNDKVAAAAAAAAQoE1lQAAAAAAAICiyFQCAAAAAAAIhXwylQAAAAAAAIAiGFQCAAAAAABA0Jj+BgAAAAAAEAKOhboBAAAAAACAoshUAgAAAAAACAUW6gYAAAAAAACKYlAJAAAUkZScqIyuHXVs7eRIN6XK83UsOD4AAFQiLj98twhgUCkAZnasmeWaWTXzeNrM1pjZHDM72symmtlD3tjRZja6lLruN7N1ZjbXzGr42tbMhppZlpn95P1/fz91TTWzrWa23sxmmlmcmcWaWaaZrTazl80sxszGmtl7ZrbPW1/7UtrXyMzmFXp8hpmtNbN3zWxUKdt9YWZDvPd3mdmVZnaadz9tNLNbCsX19t7PMrMM/3s+enz/wz5dfsMdkW5G1Hr4qVGauXiqhtx2TcAxl151oabNm6xp8yZr4co3NXbcPT7LEJxJkx5T1sq5GjHipqBiipfVqlVT8+e9osWLXteM6ZMVFxcX8rZHo6effVBvL5uu2++8MeCY1NS6mj5rslq2aqb5i15V7TrHFsTWrVtbWWvm+asKpXj62Yf0zrIZuv2u0o5F0RjPsZiiVq2bacHi11S7zrE+y3DkJk58VCtXztHw4cOCiklJqaNly2aGo4lRqbyuFZLnWCxfPrvE9rNnvajmzU8v34ZHoUmTPK/vESP8vwd8xRQvS0pK1FtvTdXy5bM0YcKDkqTY2Fh98sl6LVkyXUuWTNfpp/8ptJ2JMpnPP653V83T3XffHHRc8bLY2Fh99ulGLVs6U8uWzlSTJqeFtO2oWhhUCkxPScdISpfUXdKJzrlOkj6UdKE35hozq1ZaJWbWQVJnSR0lLZE02Ne2zrlnnHMZknY65zKcc3NLqXaYc669pFxJPSRdJOkY51xnSbsk9XPO/V3SQEnve+tb76d9J0t6TFJioeLrJfV3zqVLGmBmib629WpmZqmSUr2Px0t60Nvnm8zsj5KcJP9XrSi0PztH94wdp59+/jnSTYlKvc/pppjYGF149pVKrVdXJ550fEAxr780U5f0vVaX9L1Wm97bqjdfmeOzDIHr27ePYmNjldG1v+qnparhyScGFOOr7OKB/TX+6ck6+5xLtXv3XvXulRH2/lR2fz63l2JiY9Wnx0Wql5aik04+IaCY0xqdontGPKgnHp+oFctXq3nzxgXxYx4YoWrVjwlnN6LCn8/tpdjYGPXuMUBpaal+j0XxGM+xeEDjHpuo5ctWq3nz032W4ch4zj0x6tr1PKWlpepkv+esojFJSbU0efI4xcfHh7/RUaA8rxVJSYl6YcqTSoivXmT7gQP76fPPv9QHH3wUpl5VTof2adnvgaIxvsouueQ8vfHGXHXvfoFq1kxQy5bN1LRpI02fPl+9el2kXr0u0kcf7Qh/Jyupfv3OUmxsrNK79PW83hs2CDjOV1mzpo00fcZb6tHzQvXoeaE+/PDfYe5RFZefF75bBDCoFJg+kp71/j9DUpa3fIKkld77H0q6tIx6ekta7Jxzkt6R9EkQ2/plZiaphqRfvM+xyPun6ZL2BlFVjqTzCxc4565zzu0xszh5FnY/6GfbryQ1kNRc0sfesnaS3nXO/U/SFkktJeVJqmlmDYNoV6UWGxujx8fcrYQEPnyGQruOrbV43lJJ0vrVm9S6XYugYlLr1VWdusfqww8+LrUMZeuS3l6zZy2QJK3MWqsOHdsGFOOr7PnMV7R8+WpJUp06x2rP3u/D1Ivo0bFTO701Z7EkafWq93Rm+9YBxazKWqfNm7apfcc2atmquTZt3CZJ6px+pg4e/El7dnMsgtWp8+H9/O6q9T6Pha+YQ8eiQ8c2atW6mTZt3OqzDEcmPf1MzZ7t+ci0atU6dejQJqCYvLx8DRo0VDk5OWFtb7Qoz2tFXl6eLr3sRmXn5BZsm5ycpEcevlf7ftyvLl38JudDUnp6e82atVCSlJW1Th07+noPlIzxVfbDD/t06qknKzGxlo47rr6+/nqn2rY9Q3379taKFbM1dep4xcbGhq9zlVyX9PaaWej13tHH+clfnK+ydu1aqW/fs5S1cq5eeXkCxwLlikGlwLSXNFaeLKW6krLNbJCkBZLO88Y8K+m6MupJlfSDJDnn/s85tyCIbf2ZIOkLSbslrSj2HFucc6sDrcg5t8c7AOTLbZKmOed+9fP3fHkGjM6Q9L63rKakA977ByXV8t4frzKylcxssJltNrPNU155I9AuVEg1EhJUs0ZCpJsRteLjq2vXd3skSTk5uapTt3ZQMYOuvkivTy06hcFXGcoWnxCvnd/ukuTZzykpdQKKKW27du1aKjk5URv5h3PQEhKq67tvd0vy7Ne6KSXfG6XF9D/vbP3666/Ky8tTXFyc7hwxVPeNeiw8jY8y8fHxh/dztp/3Rikx/c8/R7/++pvy8vJKLUNw4uPj9a333JOdnaPUVN/HpXhMTk6usrMZUDpS5Xmt8HUsbrrpGs2Zs0hTpryuSy+9QH8+p2eIe1R5JSRUL3h95+TkKCWlbkAxvsrWrdukhg1P1JAhV2nHjs+0b99+vf/+B+rZc4C6dTtfP/6YrT59uoWvc5XMc88+UjA1bdnSmRo69Ooi556U1JLHRpISEuJLxPkq27x5m7p3P18ZXfvrxx+zddZZHIuwYk2lqs3MmkmqI2mWpBMlHS2ppnPuVUmjJSV5Q3dJ+rc8mUz+ZMuTUSQza2tmdwaxrT/DJE2U9Jk3A6rwc/Qzs8uOoM4izKydpLPlmcpWmk/kGWTb4n1c0BZJCd7HkjRPnr7W9FeRcy7TOdfaOdf6mssvPsKWoyo4cOCgqlXzTMdJSIhXTIwFHGNmOrNTG723ZnNBrK8yBOZA7gFVr+6ZyevZzyUvMb5i/G2XnJykJ5+4X4OvYz2yI3Eg9+Dh/VrD3/HwH3PX7fdp04at6t2nq2657Tq9kPmasvfzD+kjceDAAVUr61iUEnPnbaO1ccMW9T6ra6llCM6BA4fPPTVqJMjM93EpKwbBKe9rRXEtmp+uSZNe1u7dezV71gKlp5Ot5E9u4WtAQoLPz1C+YnyVjRlzl4YO/ZsefHC8duz4VFdcMUDbt/9bu3Z5vtTbseMzNWx4Yng6VgndOGR4wdS0Hj0v1IQJU1S92uFzj7/Xe27ugRJxvsr+uf3jQsfiUzVseFIYeoWqgitj2XpLetC7xtHTkrZ5yyTPVK/CnpTUpZS61sqzPpO8cT8FsW1pnpd0tZnFFnuOnpJ+PMI6JUlmdqKk5yRdUkqW0iFbJKVI2ud9vEFShpkdI8/Ut/clyTmXJ2mqtwz4XT784GO1PrOFJKlRk1P1zdffBhzTpn1LffD+9iKxvsoQmC1bt6tjB880hmbNGuvLL78JKMZXWVxcnKa9/pzuHfmwvvpqZ/g6EUW2bftQZ7ZvJUlq0uQ0ffVlyf3oK+amWwfroov7SZJqJdbU/v3Z6tK1g64efJnmL35NTZs20vhnHghbP6LBtq2F9nPTRvrKx3vDV8zNhY5FYmIt7f8xx2cZjsyWLdsLprw1bdpIX33l45wVQAyCU57XCl8+++wLNWjgWV+xZavmHLNSbN16+PXtb5/6ivFVVr16dTVpcppiYmLUps0Zcs7pxRefUtOmjRQTE6O+fXtr+3aWFQjUlq3b1aFj4X38dcBxvsqmTn1azZo19h6Ls/TPf/4rPB2BR35++G4RcFREnrVy6S3p0NfkKyQNlfS5ma2XZ0rXoTWV5JzbamarSqlrvqQeZrZO0veSLpbUOsBt/XLO7TOzFfKsh5Qp6UUzWyPpc0mLj6TOQh6RdKyk1z1LN+k655y/Vfbe1+EsJUm6RdILku6VNN459423DnnL7/udbQO0dPFKTV/4olLrpahL9w666dq7ddvdN+qJh57zG3N+7yskSeld22vj+i1F6vNVhsDMn/+OViyfrbS0VPXunaHLBg3R6NF3avTox/zGdE7vK+dcibKrrhyoM85oquHDh2n48GHKzHxVs2YtKOXZUdzihcu06J1pqpeWoh49u+jqK2/R3+69VQ/e/6TfmJ7dLlBMTIxeevlpDbpigD7+13+0YvkarVi+pmCb+Ytf081D+WXEYCxeuEyLl7yhemmp6tErXddccYvuGXmrHhjzpN+YXl0vkMXE6KVXntblVw7Qx//6RCuWr9b7739QogxHZsGCJVq2bKbS0lLVq1eGLr98qEaNukP33fe435guXfpFrsFRojyvFb6Me2KSJk18VMNHDNNPB3/SgIsG+4yDZz8vXz6rYJ8OGjRUo0ffodGjH/cbk57eT865EmWfffaFMjPH6fjj/6ANG7Zo+vR5eu+99/XyyxNkZlq0aKlWrFhTSmtQ2Lx5bytr5VzVT6un3n26qlOnv6hRo1M0cGB/jRr1aKlxzrkSZdu3f6xXX3lGZqYFC5dqxQquHSg/5pkxBVRcv37/f7xIK4A/nXZ+2UERUiuxpjplnKmN67fo+z3/PeKYymJnbsVdKDkpKVHdu3fWmjUbtHu3798J8BUTyHYVVUJcqT/8GVGJSbXUtWtHrVu7SXv2+H7dBBJTWVTkzzSJSbXUtVsnrVuzsfRjUUZMZfFzXlnJzRVDUlIt77lnYynnrLJjKrL8CK2xUZqqeK2QJFPJ6WWRVlWPhSTlReiXsgKVlJSoHj3StXr1e6XuY19xgW5bUfz6y86K9+YoR//7cGnYPqAc06Rn2Pclg0ohYmZZxYr2O+d8f6US5vrMrJ6kN4sV73DOlblYuJm1kPRUseJVzrlRR9KWQDCoVDFU5EGlqqYiDypVRRV5UKmq4TNNxVFZBpWqgoo4qFRVVcRBpaqsog8qVSUMKpWfSAwqMf0tRLxrMFXI+pxzu3Rki4LLObftSLcFAAAAAKBKidBaR+HCQt0AAAAAAAAIGoNKAAAAAAAACBrT3wAAAAAAAELAuehev4tMJQAAAAAAAASNTCUAAAAAAIBQiPJf4SRTCQAAAAAAAEEjUwkAAAAAACAU8slUAgAAAAAAAIogUwkAAAAAACAUWFMJAAAAAAAAKIpMJQAAAAAAgFDIz4t0C0KKTCUAAAAAAAAEjUwlAAAAAACAUGBNJQAAAAAAAKAoMpUAAAAAAABCIZ9MJQAAAAAAAKAIMpUAAAAAAABCgTWVAAAAAAAAgKIYVAIAAAAAAEDQmP4GAAAAAAAQCizUDQAAAAAAABRFphIAAAAAAEAokKkEAAAAAAAAFEWmEiq8P512fqSbAEk7/j070k2AV83jMiLdBBTyZnyrSDcBXhcd2BzpJgCAX0fFxEa6CSgkLz8v0k1AFeFcdL/WyFQCAAAAAABA0MhUAgAAAAAACAXWVAIAAAAAAACKIlMJAAAAAAAgFByZSgAAAAAAAEARZCoBAAAAAACEAmsqAQAAAAAAAEWRqQQAAAAAABAKrKkEAAAAAAAAFEWmEgAAAAAAQCiwphIAAAAAAABQFINKAAAAAAAACBrT3wAAAAAAAEKBhboBAAAAAACAoshUAgAAAAAACAUW6gYAAAAAAACKIlMJAAAAAAAgFMhUAgAAAAAAAIoiUwkAAAAAACAU+PU3AAAAAAAAoCgylQAAAAAAAEKBNZUAAAAAAACAoshUAgAAAAAACAXWVAIAAAAAAACKIlMJAAAAAAAgFFhTCQAAAAAAACiKQSUAVcr3P+zT5TfcEelmAEBQkpITldG1o46tnRzppgAAKonk5CR1795Ztbl2RJbLD98tAqJyUMnMjjWzXDOrZh5Pm9kaM5tjZkeb2VQze8gbO9rMRgdSl/exv/q2mtl6M5tpZnF+6ppqZp18lLcws88LPR5rZu+Z2T4zyzKz9oWeI8t7q2dmp5vZSjPbYGZDvdtOMbNtZvadN65BKX3rZ2YrCz1ONrNF3r7d5y37wsx6e+9nmVlGEPV9YWZDvPd3mdmVwbSvonv4qVGauXiqhtx2TcAxl151oabNm6xp8yZr4co3NXbcPT7LEBr7s3N0z9hx+unnnyPdlKg1adKjWrlyjkaMGBZUTPGypKREvfXWVC1fPksTJjwY8nZHu8ZPXqe2C8eowa39S407um6izlz2kCTJYmPU+f1n1HrOSLWeM1I1Gv0xHE2Nek8/+5DeWTZDt991Y8Axqal1NX3WFLVq3UwLFr+m2nWODVdzo97EiZ5zz/Dh/s9ZvmJSUupo2bKZ4WhiVJo06TFlrZyrESNuCiqmeFmtWjU1f94rWrzodc2YPllxcXEafO0gLVkyQ0uWzNDGDW/r2WceCnl/os2zEx/WshWzdNfwoQHH1KpVU3PeeknzF7yqN96cpLg4n/8cQhkyn39c766ap7vvvjnouOJl9eqlaP68V9SmzRlatnSm6tQ5VrVq1dSC+a/qH4vf0MyZUzhOKBdROagkqaekYySlS+ou6UTnXCdJH0q60BtzzaGBoiDqUin1DXPOtZeUK6lHkO3tLek4MztVkpxzf5c0UNL7zrkM59z6Qs+R4b3tknSvpOGSOki63MxinXPXSLpF0iJv3Oclnq3o87Y1sxrex7dLWu7tW4aZHSfJSfL/Sav0+iSpmZmlSkr19i2Y9lVYvc/pppjYGF149pVKrVdXJ550fEAxr780U5f0vVaX9L1Wm97bqjdfmeOzDKERGxujx8fcrYSE+Eg3JSr17dtHsbGx6tr1PKWlperkk08MKMZX2SWXnKc33pir7t0vUM2aCWrZsln4OxQlUs5uI4uJ0cY/j1S11GTFN6jnN/bUUZcpttrRkqQajY/XrrnrtPm8Mdp83hjlfvx1uJoctf58bi/Fxsaod48BSktL1UknnxBQzGmNTtE9Ix7QuMcmavmy1Wre/PQItD76eM49MQGcs4rGJCXV0uTJ4xQfz7XkSBw652d07a/6aalqWMq1onCMr7KLB/bX+Kcn6+xzLtXu3XvVu1eGMie/ql69BqhXrwFau3ajprwwLfydrMTO7dtbsTGx6tHtAqWlpfh8X/iKuWhgX014+gWd+5dB2r17r3r26hL+xldy/fqdpdjYWKV36et5jTf0/d27rzhfZY0b/0l33DFaDz/8tJYsWaUzzmiqSy4+T0+Nz9RZZ1+s3bv2qnfvjPB2ElEpWgeV+kh61vv/DElZ3vIJkg5l0nwo6dIg61Ip9cnMTFINSb8E2d7exZ4jUDslXSuplnOurXMuL8jtO0t6XVI37+MMHe7bXyX9KClPUk0za3gE9X0lqYGk5pI+DqZhZjbYzDab2ebsn78PZtOwaNextRbPWypJWr96k1q3axFUTGq9uqpT91h9+MHHpZahfNVISFDNGgmRbkbUSk9vr1mzFkqSsrLWqWPHNgHF+Cr74Yd9OvXUk5WYWEvHHVdfX3+9M3wdiTLJHRtr93zPdxM/rPlISe3+5DPu2E6nK+/gz/rfnv2SpKRWpyjl7DZqM3+0mj43VBYbrR8ZwqdT53Z6a85iSdK7q9brzPatA4pZlbVOmzdtU4eObdSqdTNt2rg1rO2OVunpZ2r27EWSpFWr1qlDB1/nrJIxeXn5GjRoqHJycsLa3mjRJb29Zs9aIElambVWHTq2DSjGV9nzma9o+fLVkqQ6dY7Vnr2HPzPWr19PKSl1tHXr9lB3Kap07nym5szxvuaz1qt9h5LnKV8xkzNf08oVayRJderU1t49Fe/ze0XXJb29ZhZ6jXf0cU7yF+erbMWK1dqwcYs6dWqnNm1a6L333tek518+/J6pW1t79/w3DD2D8vPDd4uAaP2E2F7SWHmyiupKyjazQZIWSDrPG/OspOuCrEul1DdB0heSdktaEWhDvVk9tSVNkWdwqTQTvFPGDuVbj5D0uaTNZnZVoM/pfd5TJX0taX6h5z3Ut3GSZks6zVs+XmVkK/mpL1+eQakzJL0fTPucc5nOudbOuda1qtUJZtOwiI+vrl3f7ZEk5eTkqk7d2kHFDLr6Ir0+tWjavK8yoDJJSKiub7/dJUnKyclRSkrdgGJ8la1bt0kNG56oIUOu0o4dn2nfvv3h60iUiY2vpp+/+0GS9FvOTzq6bmKJGIuL1Um3na9Pxr5RULZ/22fa1O8+bTp3tH7df0B1epwRtjZHq/j4eH337W5JUk52rlJSSl7fSovpf/45+vXX35SXF+x3SPAlPj6+4NyTnZ2j1FTfx6N4TE5OrrKzGVA6UvEJ8dpZcM738z7wEVPadu3atVRycqI2Fhpwvf76K5Q5+dVQdiUqxSdU17fec1C23+PjP6Zt2zOUlFRLmzZtC0t7K7Pnnn1Ey5bOLLgNHXp1kfNNSmrJz1GSlJAQXyLOV9khAy48t8S148x2rZSclKgNG7eEqnuoQqJuUMnMmkmqI2mWpBMlHS2ppnPuVUmjJSV5Q3dJ+rc82TkB1WVmf5S03099wyRNlPSZc84F0eRu8gwqPSOpvZkdU0rsoelvh6bcNXPOPSipjaQ7zOykIJ63j6RT5BmYOjRd71Dfbpf0tqRD0wPnybOfagZZnyR9Is/AW1SdsQ4cOKhq1TyHKiEhXjExFnCMmenMTm303prNBbG+yoDKJjf3oKpX95w2EhISfL4vfMX4Khsz5i4NHfo3PfjgeO3Y8amuuGJA+DoSZfIO/KzY6p4pbbEJx8hiSl76Gwzrq69feke/ZR8sKMv511f6Zc+PkqQDn35b6rQ5BObAgQOqdui1XiNeMT6ORWkxd942Whs3bFHvs7qGpb3R7sCBAwXnnho1EmTm+3iUFYPgHMg9UOic7+d94CPG33bJyUl68on7Nfi6wz/CYWbqkt5eq1atL1E3Sncg96Cqez+/1vB3nvITk5ycqMefGK0brh8evgZXYjcOGa4ePS8suE2YMEXVqx0+3/ja95KUm3ugRJyvskNuuvkerX9vs845p6ckz3vmqafu17WDbwtl91AYmUqVTm9JDzrnMiQ9LWmbDmfONC8W+6Sk0ib8Fq+rt6S1pdT3vKSrzSw2yPbe5H2ORfJMIQvUs2Z2gnPuB3mmwgWz0lpvSQO86yft8U5vW6vDU/AK+uadVjdVUssg65M8g0kpkvYF0bYK78MPPlbrM1tIkho1OVXffP1twDFt2rfUB+8XTcX2VQZUNlu3bi+YPtKsWWN9+eU3AcX4KqtevbqaNDlNMTExatPmDAU3Vo/Csj/4PyW19SSe1jz9BP309d4SMbXTm+qPV/VW6zkjVbPJCWr8xGA1fWaIajQ+XooxpZ7dRjn/+jLcTY8627Z+qDPbt5IkNWnaSF/5eI/4irn51sG66OJ+kqTExFra/yNZMuVhy5bD556mTRvpq69KHo9AYhCcLVu3q2MHz5Q3f9cKXzG+yuLi4jTt9ed078iH9dVXh6dJd+rUTps2MU30SGzd+qHaF3rN+76Wl4yJi4vTK68+o1EjH2PK+hHasnW7OnQs/HnI91qGvuJ8ld1xx4267LILJElJibX044/7FRcXpzemTdI9f3+oyHsG+D2OinQDQqC3pENfVayQNFTS52a2XtJBFVoDyTm31cxWBVHXEHkW5u7jp759ZrZC0vmSZvipc4qZ5XrvPyjPQuD3FnqOPpKW+dl2gpkdmgMySp5Fumea2a+SVjnndpTSlwLebKhm8gy4FX7eByRNM7OzJBVfffIFSfcFWZ/kmfYWVVlKkrR08UpNX/iiUuulqEv3Drrp2rt129036omHnvMbc37vKyRJ6V3ba+P6orvEVxlCZ+ozj0a6CVFp/vx3tHz5LKWlpap37wwNGjRUo0ffodGjH/cbk57eT865EmWfffaFMjPH6fjj/6ANG7Zo+vR5EexZ5bbnH5vVdv5oHVMvWXW6tdA/rxuvhiMG6NOHD1+mNvU7fHpvPWek/nVbpmqcdpyaThwmmWnvO+/rh3c/jEDro8vihcu0eMkbqpeWqh690nXNFbfonpG36oExT/qN6dX1AllMjF565WldfuUAffyvT7TCux4Gfp8FC5Zo2bKZSktLVa9eGbr88qEaNeoO3Xff435junTpF7kGR4n589/RiuWzC875lw0aotGj79To0Y/5jemc3lfOuRJlV105UGec0VTDhw/T8OHDlJn5qmbNWqCePbto9ZoNEexl5bVwwRItWTpDaWkp6tUrQ1dccZNGjrpdY+4b5zema8Z5uuLKAWpxRlPdddcQ3XXXEE2Z/FrBemQIzLx5bytr5VzVT6un3n26qlOnv6hRo1M0cGB/jRr1aKlxzrkSZTExMXpj2iT99aqL9dFHO7R06SpdN/hytWzZVHePuEl3j7hJz2e+qpkz50ew11VElH85anz7i4rupDpnVMgXaa3EmuqUcaY2rt+i7/0schdITGWx49+zI90EeNU8LiPSTfArKSlR3bt31po1G7R7d8mMGH8xgWxXUc1P7BDpJpTpqMQE1e7SVPvWf6xf9kbv+lQXHaj4U4gTk2qpa7dOWrdmo/b4Wcg2kJiK7ue8XyPdhIAkJdXynns2lnLOKjumIst3kZkOUZqqeK2QpLiYyvF9flJSLXXr1llr1m7Qnt2+z0GBxFR0//st2N9WCr2kpET16JGu1avfK/U17isu0G0rol9/2VlyzYQo8tP0+8L279nqF40K+75kUMnLzLKKFe13zvU9wrrqSXqzWPEO51wgC4OXKzNrIempYsWrnHOjKkJ9gaiog0pVDYNKFUdFHlSqiirDoFJVURkGlaqKyjKoVBVUxEGlqqqyDCpVFRVxUKmqivpBpTdGhW9Q6eL7wr4vObN5edc0Kq+6dqmUBcDDyTm3TeXYlvKuDwAAAAAAVE4MKgEAAAAAAIRChH6VLVyi8dffAAAAAAAAEGJkKgEAAAAAAIRClK9tR6YSAAAAAAAAgkamEgAAAAAAQCiwphIAAAAAAABQFJlKAAAAAAAAoeBcpFsQUmQqAQAAAAAAVAFm9oKZrTOzv/v5e7KZLTaz1WY2qaz6GFQCAAAAAAAIhfz88N3KYGbnSYp1znWQVN/MTvERNkjSa865zpJqmlnr0upkUAkAAAAAAKCSM7PBZra50G1wsZAMSTO891dI6uSjmv9K+pOZJUn6o6SvSntO1lQCAAAAAACo5JxzmZIySwlJkLTTez9bUkMfMWsknSPpJkn/lrSvtOdkUAkAAAAAACAUApiWFka5kqp779eQ79lrD0q63jmXbWa3SbpKpQxUMf0NAAAAAAAg+r2vw1Pemkv6wkdMvKSmZhYrqZ2kUn++jkElAAAAAACAUHD54buV7S1Jg8zsCUkDJH1kZmOLxTwkT2bSfknHSnqjtAqZ/gYAAAAAABDlvFPaMiT1lPSoc26XpA+KxWyUdHqgdTKoBAAAAAAAEAIuv9TZY2HnnNunw78A97sx/Q0AAAAAAABBI1MJAAAAAAAgFCrWr7+VOzKVAAAAAAAAEDQylQAAAAAAAEIhsF9lq7TIVAIAAAAAAEDQyFQCAAAAAAAIhQr262/ljUElVHg7c7+PdBMgqeZxGZFuArxyvsmKdBNQSI3jukS6CfA6KiY20k2Al3PR/QG6MuFYVBy/5v8W6SYAQLljUAkAAAAAACAU+PU3AAAAAAAAoCgylQAAAAAAAEKBTCUAAAAAAACgKAaVAAAAAAAAEDSmvwEAAAAAAIRClP8KJ5lKAAAAAAAACBqZSgAAAAAAAKHAQt0AAAAAAABAUWQqAQAAAAAAhEI+ayoBAAAAAAAARZCpBAAAAAAAEAqONZUAAAAAAACAIshUAgAAAAAACAXWVAIAAAAAAACKIlMJAAAAAAAgBFw+ayoBAAAAAAAARZCpBAAAAAAAEAqsqQQAAAAAAAAURaYSAAAAAABAKDjWVAIAAAAAAACKYFAJAAAAAAAAQWP6GwAAAAAAQCiwUDcAAAAAAABQFJlKAAAAAAAAoZDPQt0AAAAAAABAEQwqAQAAAAB+l+TkJHXv3lm1aydHuilAxZLvwneLgLAPKpnZaDP72MzeNbPlZrbEzDoV+ttlZpZlZuu9MU+XUtdYM3vPzPZ5t2lvZvXMbKl3+0dL2XaqmW01s9VmtsDMahR63s1mNsHMaprZF4W2ecLMrvRT3xQz22Zm33nraWBmn3vvbzWzEd641WZ2qpnVNrN/mpnfKYhmdouZvVTosTOzc8ws3szyzCzDzBaa2b8LPVd1P3Wlmtk7ZrbGzG7xln1a6O+fmtmJ3n25ysy2mFlvP3W1M7Pp3vsp3v3VoNBxyDrUX2/MNjPr4r1/pbetq81srZmd4K//Fd2kSY8pa+VcjRhxU1Axxctq1aqp+fNe0eJFr2vG9MmKi4sLedujzaRJj2rlyjkaMWJYUDHFy5KSEvXWW1O1fPksTZjwYMjbXdV9/8M+XX7DHZFuRtQJ5bkpNjZWn3zynpYsmaElS2bo9NNPC3l/osVzEx/R8hWzddfwoUHFpKTU0ZKlM0rENm58qubPfyUkbY1W5XWtOGT8+LE6++wekjzvl3nzXtaiRa9r+vRMruVBen7S41qV9ZbuLuW85S+ueFmtWjU1f/6rWrx4mmbOmMKxKEN5XTPq1UvRW3OnqnXrFlryzgzVqXOsBl87qOB6sXHD23r2mYdC3p9okvn843p31TzdfffNQcf52zYlpY42bXwnJO1F1RWpTKUHnHPpkl6S9F8/MRd6Y041s0a+Apxzf5c0UNL7zrkM59x6SfdJmiupg6ROZtahlHYMc851lrRB0qWFnre1pCaSjpP0pZk19v6tmySf70Ln3DWSbpG0yNuWzyXlOecyJLWW9Fczi5d0vzfuRklPOud+K6V9vb3PWVgzSU3lPXbOuT9LeljSC97n/clPXfdLetM510nSBWaW4ifufedcF0n9JY3x09cNkpLN7ERJQyQ9Kcnp8HHIcM49LElmVk9SY29fDnnBu98z5dkPlU7fvn0UGxurjK79VT8tVQ1PPjGgGF9lFw/sr/FPT9bZ51yq3bv3qnevjLD3pzI7tE+7dj1PaWmpOrmUY1E4xlfZJZecpzfemKvu3S9QzZoJatmyWfg7VEXsz87RPWPH6aeff450U6JKqM9NTZs20ozp89Sr1wD16jVAH3307/B3shI6t29vxcbEqnu38/2ep3zFJCXVUubkcUpIiC8R//DDf1fc0UeHofXRoTyvFZLUsWNbpabW1eLFyyRJAwf209NPT9E53vdLL67lAevX9yzFxsaoS0Y/pdVPVcOGDQKO81V28cX9NX58ps4++xLt2r1HvXtnhLdDlUh5XjMaNz5Vd951nx55ZIKWLl2lM1o0VebkVwuuF2vXbtSUF6aFv5OVVL9+Zyk2NlbpXfp69rG/94WPuNK2ffSRkapWvVq4uoFDXH74bhEQ6elvyZIu8vdHM4uVlCjJ30CJL+0kveucc5LWSWoTYDsKnsObPVRN0i/yDCL18A7COOfcd0G05ZBjJJl3+yWSTpV0jqRX/W1gZtUk/VHSB2Z26F+2H8szoNTcez8YvSUt8t5/UlJZn0RrytN/fx6VdLe33jfLeN5JknoewXNUWF3S22v2rAWSpJVZa9WhY9uAYnyVPZ/5ipYvXy1JqlPnWO3Z+32YehEd0tPba9ashZKkrKx16tix5FveV4yvsh9+2KdTTz1ZiYm1dNxx9fX11zvD15EqJjY2Ro+PudvnP5Zx5EJ9bmrXtqX69u2jFStma+rUpxUbGxumnlVunTufqTlzPOebVVnr1KFDyfOUr5i8vHxdPmiosrNzi8RefvmFevfd90Lf8ChSnteKo446Ss8997C+/PIb/fnPno83mZmvFnq/1NZeruUBS+/SXrNme/fxSt/vD39xvsqef/7wuatundras8ff99coz2vGihVrtHHjVnXq1E6t27TQexveL6ijfv16Skmpo61bt4ehV9GhS3p7zSy0jzv6eV/4ivO3bUZGRx04cFC7d+0JQw9QlURqUOkeM3tX0pmS5vmJmSnp35K+kfRlEHXXlHTAe/+gpFqlxE4ws9WS4iW9Ueh5d0qa65z7TNLbknrIkzG0JIh2SFKsmWVJ+kLSfYWyiN6T9O8yspTSJa32PuehLJ89kmrLM6j0vp/t/EmV9IMkOedmO+e+8RPXyszWy5PtdYu/ypxzyyS1lPSScy6v0LaHpr+d4y3rLel1Sa5QdtTVZvZPSRdKespX/WY22DutbnNeXq6vkIiKT4jXzm93SZJycnKVklInoJjStmvXrqWSkxO1cePWMPQgeiQkVNe3Bfs0RykpdQOK8VW2bt0mNWx4ooYMuUo7dnymffv2h68jVUyNhATVrJEQ6WZEnVCfmza//4F69LxQ3bqdr/0/ZuusPsWTaeFLQkK8vv12tyT/x8VXTE5OrrKzc4rEHXtskgYO7K+nnsoMfcOjSHleKy677Hx9/PEneuKJSWrTpoVuuOHKgjq4lpft2Wcf1tIlMwtuQ4f8Vd/u9Hxnm52To1Qfx0aSEuLjS8T5KjukXbuWSkpO1MaNW0Lco8orFNeMCy/4i3779Vfl5eUVlF1//RXKnOz3u3RIeu7ZR7Rs6cyC29ChVxece7Kzc5SS6ud9kRBfIs5XWVxcnP5+zy362z0s7xARrKkUEg8459Kdc5dK+rHY3w7tiQsl/UlSrKTLgqg7W1IN7/0E72N/hjnnOjvnbnDO/VroeedK+sT7eKukRpL6yM/Ut1Icmv62RdJHkmRmNeTJ2jnJzOqXsm0fSZ0lDfLePyTX257/C7ItBfvFzO73Tiks/Ko7dP99Sd0l/SbpwzLq/Mh7O6Tw9LdFZhYjKUPSY/IMah0aHHtB0mBJPzrnfH595JzLdM61ds61jo2t4Sskog7kHlB1b+poQkK8YmJKvpV8xfjbLjk5SU8+cb8GX8f6MsHKzT1YaJ8mKCbGAorxVTZmzF0aOvRvevDB8dqx41NdccWA8HUEKAehPjdt3/6xdnm/4dyx41O/6fgo6kDuQVWr5t2/NeJlPs5TgcRI0pj7h2vkqEf122+lfS+F4srzWtG8+el64YVp2r17r6ZNm6suXdpLkpKTE/XEE2N0HdfyUg0ZMkI9e11YcJvwzAuqVt2zJGgNP8dGknIPHCgR56tM8py7nnryfg0efHsYelR5lfc1Q5JuvuXvWv/e+wXrjZmZuqS316pV60PdnUrtxiHD1aPnhQW3CROmqLr3mlCjRoLPYyNJubkHSsT5KrvrriGaOOll7d9f2j+NgSMT6elvkrRb0kne+ydJ2nXoD865fEn75ck+CtQGSRlmZpI6Stp4BG16UtKt3jY4eTKL+kpacwR1SdI4SYeuakMlTZUnQ2d4Kdv0lNTFOddO0glmdugr/e3yTqULsg1rJfX0DvScJU/Wk8wswTvQVfB1gnPuoKQ58gxo/R6tJG32ro31VxVaV8k5956kY83slN/5HBGxZet2dezgSRFu1qyxvvyyZOKXrxhfZXFxcZr2+nO6d+TD+uorplsFa+vW7QWp8v6Oha8YX2XVq1dXkyanKSYmRm3anCHP2x+oPEJ9bnrpxafUtGkjxcTE6Ny+ffTP7f8KU88qN8/5prUkqWnTxvrK73mq9BhJ6tSpne6/f4T+8fabatassUaO4h/NgSjPa8Vnn32pBg2OlyS1atVMX321U3FxcXr99YkaOfIRruVB2rrlnwXTEf0dG39xvsri4uI0bdpE/f1ePleVpTyvGbfffoMuvfR8SVJSYi3t/9EzeNGpUztt2kTmXrC2bN2uDkVe218HHOerrHu3zrrh+iu0bOlMNW9+up6f9Fh4OgJJksvPD9stEvz+8lgYTZL0upldK+k7SSsl3StppndgaL8806cCNVrSa/IsvL3au3h3UJxzO7y/ZNbWObdRngylJOfcEa3/45xbamYPeX/pbJA8gy2/SBppZmnF12kys+PkyXI6lMWzVlJX7/33Fdwg2yG3y5MhdLukV5xz/zWzByS96/37/cXiJ0habGYvuMD/Zd3KO91Pkj6X9JmkFd7Ha+VZmH15ofhDg3eVbrHu+fPf0Yrls5WWlqrevTN02aAhGj36To0e/ZjfmM7pfeWcK1F21ZUDdcYZTTV8+DANHz5MmZmvapZ3HjTKNn/+O1q+fFbBPh00aKhGj75Do0c/7jcmPb2fnHMlyj777AtlZo7T8cf/QRs2bNH06f5m5wIVU6jPTQ88OF6vvDxBZqaFi5ZqxYoj/a6lalmwYImWLp2ptLRU9eyVoSuvGKaRo27XmPvG+Y3pmtHfZ10tmh+ecviPt98sUgf8K89rRX5+vp5//nENGHCujjrqKF1yyfW68sqLvO+XoRo+fKgyM1/jWh6gefPf0coVc5SWlqo+vbuqU+dz1ei0UzRwYD+NKnTu8hXnnCtRdtVVA9XyjKYaMfwmjRh+kzIzXylYXwZFlec1IyYmRtNef05XXXWx/vXRDi1dtkqS1LNnF61esyFSXay05s17W1kr56p+Wj317tNVnTr9RY0anaKBA/tr1KhHS41zzpUoe/PNtwq2WbZ0pq67/s4I9ArRyvgmHhXdMdX+WCFfpElJierevbPWrNmg3bv3BhwTyHYVkcl3OnpFUNWORc43WZFuAgqpcVyXSDehiKr2fijsqJiKu3B4UlItdevWWWvXbizluJQdU1nkRejb0tJU1fdGfoR+DSgYSUmJ6tG9s1aXsY99xQW6bUXg+b68Yqmq7wtJyq+A56nCkpIS1aNHulavfq/s90WxuEC3rSh+/WVnxXtzlKPc4eeF7d+zNR6ZE/Z9WSkGlbw/S1/8F8Z2OOeuC+W2fuproZKLS69yzo06wvr6SBpRrHi6c25iBalvhIqu6SRJdx9JBtiRqqiDSlVNRR5UqmoYVKpYKtqgUlVWkQeVqpqKOKhUVVWGQaWqoiIOKlVlFX1QqSphUKn8MKgE+MCgUsXAoFLFwaBSxcKgUsXBoFLFwaBSxcGgUsXBoFLFwqBSxRH1g0p39g/foNJjc8O+LyvCQt0AAAAAAACoZBhUAgAAAAAAQNAqwq+/AQAAAAAARJ8on4ZMphIAAAAAAACCRqYSAAAAAABAKORH9+9OkakEAAAAAACAoJGpBAAAAAAAEAKOTCUAAAAAAACgKDKVAAAAAAAAQoFMJQAAAAAAAKAoMpUAAAAAAABCIT8/0i0IKTKVAAAAAAAAEDQylQAAAAAAAEKBNZUAAAAAAACAoshUAgAAAAAACAUylQAAAAAAAICiyFQCAAAAAAAIAefIVAIAAAAAAACKYFAJAAAAAAAAQWP6GwAAAAAAQCiwUDcAAAAAAABQFJlKAAAAAAAAoUCmEgAAAAAAAFAUmUqo8BLiqkW6CZD0ZnyrSDcBXjWO6xLpJqCQ3G9WRboJ8OrV4rpINwFea/d+HOkmwMvMIt0EeJk4FhVJdOeOoCJxZCoBAAAAAAAARZGpBAAAAAAAEApkKgEAAAAAAABFkakEAAAAAAAQCvmRbkBokakEAAAAAACAoJGpBAAAAAAAEAL8+hsAAAAAAABQDJlKAAAAAAAAoUCmEgAAAAAAAFAUmUoAAAAAAAChwK+/AQAAAAAAAEUxqAQAAAAAAICgMf0NAAAAAAAgBBwLdQMAAAAAAABFkakEAAAAAAAQCizUDQAAAAAAABRFphIAAAAAAEAIsKYSAAAAAAAAUAyZSgAAAAAAAKHAmkoAAAAAAABAUWQqAQAAAAAAhIAjUwkAAAAAAAAoikwlAAAAAACAUCBTCQAAAAAAACiKTCUAAAAAAIAQYE0lAAAAAAAAoBgGlYAwS0pOVEbXjjq2dnKkmwIAkqTvf9iny2+4I9LNAKJKvXop6tatk2rUSIh0UwAAkZQfxlsEMKhUBjObamYzvfffNLNZZjbXzNaa2ctmdpQ35iFvzGgzG11KfalmtrrQ4+PNLMvMVphZpplZKe3Y6o3NMrN6Zva6mfUwszgz+9DMksxsipltM7PvvHEN/NR3upmtNLMNZja0UHk/M1tZ6PEXZjbEe3+XmV1ZSt/K3DbQ9lUGTz/7oN5eNl2333ljwDGpqXU1fdZktWzVTPMXvaradY4tiK1bt7ay1swLebujXeMnr1PbhWPU4Nb+pcYdXTdRZy57SJJksTHq/P4zaj1npFrPGakajf4YjqZGlUmTHlPWyrkaMeKmoGKKl9WqVVPz572ixYte14zpkxUXF6fY2Fh98sl7WrJkhpYsmaHTTz8t5P2pSvZn5+ieseP0088/R7opVcKdj9+mCW89pctuusRvTHKdJI2f/USJ8rEvjlHD008OZfOi0vOTHteqrLd0dynnJ39xgZalpNTRiuWzCx43bdJIr7/2nDq0b6Ply2YpLi6uHHtU+ZXXNUPy7PvlhfZ9/fr19NmnGwuuGXUKfdZCSZMmPaqVK+doxIhhQcUUL0tKStRbb03V8uWzNGHCgwVxnuMzK3QdqCIyn39cq1fN09/uvjnouJSUOspaMSfUTUQVxqBSYJp5/99c0l8kfeKc6yjpGEkDvH+7xsyqlVaJmSVLellS4a+srpN0g3Oum6Q/SmpaShXDnHMZ3tsuSWMl3SxpoKTZzrkfnXPXSLpF0iJv3Od+6rpX0nBJHSRdbmax3vLektqaWY3C/TezVEmppfUvkG2DaF+F9udzeykmNlZ9elykemkpOunkEwKKOa3RKbpnxIN64vGJWrF8tZo3b1wQP+aBEapW/ZhwdiPqpJzdRhYTo41/HqlqqcmKb1DPb+ypoy5TbLWjJUk1Gh+vXXPXafN5Y7T5vDHK/fjrcDU5KvTt20exsbHK6Npf9dNS1fDkEwOK8VV28cD+Gv/0ZJ19zqXavXuvevfKUNOmjTRj+jz16jVAvXoN0Ecf/Tv8nYxisbExenzM3UpIiI90U6Je57M6KSYmVsP63aI6qbX1hwZ/KBFTI7GGRjx1l6rFF/1I0aN/N3331Xf69KPPwtXcqNCv71mKjY1Rl4x+SqufqoYNfX+X5Ssu0LKkpES98MJTRd5DjRqdomsH366xDzypzz//Sg0a8GXFIeV5zUhKStQLU55UQnz1gm3btGmhRx6ZUHDN+P77H8LYu8rl0D7t2vU8paWl6uRSjkXhGF9ll1xynt54Y666d79ANWsmqGXLZkpKStSUKU8ovtDxQfD69TtLsbGx6tylr9LSSjmP+YhLSkrUSy88pXiu8QghBpUC84uZ1Zb0q6R/SHrXW75GUhvv/Q8lXVpGPXmSLpKUfajAOXePc+5j78Pakr4PtFHe7XIk3SnpqUC389op6VpJtZxzbZ1zed7yzpJel9TN+/grSQ3kGVD7uEQtRf2ebYsws8FmttnMNv/v1/3BbBoWHTu101tzFkuSVq96T2e2bx1QzKqsddq8aZvad2yjlq2aa9PGbZKkzuln6uDBn7Rnd8CHHz4kd2ys3fPXS5J+WPORktr9yWfcsZ1OV97Bn/W/PZ7XVlKrU5Rydhu1mT9aTZ8bKovl1BiMLuntNXvWAknSyqy16tCxbUAxvsqez3xFy5d7kjnr1DlWe/Z+r3ZtW6pv3z5asWK2pk59WrGxsSXqx5GrkZCgmkzPCYsW7Zspa+EqSdKWddvUtE2TEjH5efkac8NYHcw9WFBWM6mmbrj3OuXsz1GLDs3D1t5okN6lvWbNXihJylq5Th06tAk4LtCyvLw8XXrpDcrOzi2ob8bM+fryy2901lndlJScqE8//SKEvaxcyvOakZeXp0svu1HZOYf3fbu2LXXddZdrVdZbeuzRUWHoUeWVnt5es2Z5X89Z69SxY8n3h68YX2U//LBPp556shITa+m44+rr6693Ki8vT5ddNkQ5hY4Pgtclvb1mFXrtd/RzHvMVl5eXp4svvUE52Tlhay9Kcvnhu0UC/3IKzAfyDAZ9IKmmpAPe8oOSannvPytP1pFfzrls55zPERIzu0jSR865b0upYoJ3ytjMQmUrJP3gnNtXdjeKGCHpc0mbzewqbxtOlfS1pPnyZB1JnpmZeZLOkPS+v8p+z7a+OOcynXOtnXOtj4lLDGbTsEhIqK7vvt0tScrJyVXdlNpBxfQ/72z9+uuvysvLU1xcnO4cMVT3jXosPI2PYrHx1fTzd55vJH/L+UlH1y352rG4WJ102/n6ZOwbBWX7t32mTf3u06ZzR+vX/QdUp8cZYWtzNIhPiNfOb3dJ8rzWU1LqBBRT2nbt2rVUcnKiNm7cqs3vf6AePS9Ut27na/+P2TqrT7cS9QOVQbX4avr+O8+XBwdzDiq5blKJmIO5B3Ug52CRsguuOU9Zi97VgtcWqdf5PdWhZ/twNLdSevbZh7V0ycyC29Ahf9W3O7+TJGXn5Cg1pa7P7RLi40vEBVqWk5OrbB//YKtRI0EXnP8X7fvhRznnQtHdSqk8rxm+9v07S1aqS0Y/dcnop1NOaaAmTZgy7U9CQnV9W7BPc5Ti4/3hK8ZX2bp1m9Sw4YkaMuQq7djxmfbt2+/3vYHSPffsI1q+dGbBbdjQqw+/9rNzlJrq5zxW+D3ijeMYIByOinQDKoktkq6U9IakGt6b5JnGli0pWdIuSf+WlCEpK5jKzewkSXdI6lFG6DDn3JpC28VIulrSD2bW0jm3JYinbeace9DMJklabWarJPWRdIo8A06Fz1afSDpP0pul1Pd7tq10DuQeVPXqnqkJCTXiFRNTcny2tJi7br9Pf/v7Lerdp6sannKSXsh8Tdn7OeH/XnkHflZsdc+UttiEY2Q+jkuDYX319Uvv6Lfsw/9oy/nXV3K//CZJOvDpt6VOm0NJB3IPHH6tJ/h7P5SM8bddcnKSnnzifg28eLAkafv2j/XLL79Iknbs+NRv2jdQ0f104Gcd7Z3mXD2humIssO/2TmnSUBPvz9S+vfuUtXCVWnduqXVL14eyqZXWkCEjijweN+4+VavumXpTIyFBMTE+l65U7oEDJeICLfNn//5sXX3NrXrpxfFq3bqFNm3a+rv7Fw3K+5pR3Pr17xe6Znymhg0b6MMPmTbtS27hz6p+Xs++YnyVjRlzl4YO/ZtycnJ1003X6IorBuiFF6aFrzNR5MYhw4s8fmLcfape7dC/KRL8vvZzcw8EFIfwi1QGkT9m9oKkRpIWO+fGlhL3nKR/OOcWlFYfr7TAbJFnmtsWSWfJM3AkeaZ7bSwU96SkLsFU7F1n6Q1Jf/WXxVSKAfJMxbtXUrD5vc+a2QnOuR/kmQoXJ0+G0QDnXCdJe8ysoTd2i6QUSaVlQ/2ebSudbds+1JntW0mSmjQ5TV99uTOgmJtuHayLLu4nSaqVWFP792erS9cOunrwZZq/+DU1bdpI4595IGz9iDbZH/yfktp6vpGsefoJ+unrvSViaqc31R+v6q3Wc0aqZpMT1PiJwWr6zBDVaHy8FGNKPbuNcv71ZbibXqlt2bpdHTt4pi80a9ZYX375TUAxvsri4uI07fXndO/Ih/XVV5731UsvPqWmTRspJiZG5/bto39u/1eYegaUr//88z9q2uZ0SdLJjU/Srm92BbTdzi++Vf0T0iRJf2p2qnbv3BOyNkabrVv+WTClx9/5yV9coGW+TJjwoDp1aidJSkqqpf37K95U/kgpz2uGLwsXvqZ69VJUvXo19ezZRf/6aEeIelL5bd26vWBKqL996ivGV1n16tXVpMlpiomJUZs2Z5CdV462bN1ecN5p3qyxvvjS99qfgcahajOz8yTFOuc6SKpvZqf4iessqV5ZA0oSmUqB+kLSfyR9KWmhpJPMbJ08WTgz5Z3u5Zzb6s34CcYIScfLM7VNkkY55/zVMcHMDn0qGSXPQttnO+e+M7PqZnaGcy7Qr8GGS5ppZr9KWuXtYzNJ27x/XyFP9pHkmbrmNwvKzI450m0rq8ULl2nRO9NULy1FPXp20dVX3qK/3XurHrz/Sb8xPbtdoJiYGL308tMadMUAffyv/2jF8jVasbwg+UzzF7+mm4feE4kuRYU9/9istvNH65h6yarTrYX+ed14NRwxQJ8+PKMgZlO/+wrut54zUv+6LVM1TjtOTScOk8y095339cO7H0ag9ZXX/PnvaMXy2UpLS1Xv3hm6bNAQjR59p0aPfsxvTOf0vnLOlSi76sqBOuOMpho+fJiGDx+mzMxX9cCD4/XKyxNkZlq4aKlWrFhTSmtwpKY+82ikmxD11ryzTk/PeUJ1Umurbde2uv/GB/TXO6/Ui49NLXW7NyfO0B2P3abLhl2in3/6WSOvva/UeBw2b/47WrlijtLSUtWnd1d16nyuGp12igYO7KdRhc5RvuKccwGV+TJu3ES99OJ4Oee0bNm7+s9//i9cXa7wyvOa4csDDzypJe9M1y+//KrJk1/Vfz5h3/szf/47Wr58VsE+HTRoqEaPvkOjRz/uNyY9vZ+ccyXKPvvsC2VmjtPxx/9BGzZs0fTp/KJxeZk3721lrZyr+mn11LtPV3Xs9Bc1anSKLh7YXyNHPVpqHCqGcGYqmdlgSYMLFWU65zILPc6QdOgfRyskdZJnXKNwHXGSJktabGZ9nXOlvqGNUWRUdMfWPKVCvkgTk2qpa9eOWrd2k/bs8b3AdiAxlcWb8a0i3YSAHJWYoNpdmmrf+o/1y97o/Gb4L/vXRroJJSQlJap7985as2aDdu8umSHmLyaQ7Sq63G+C/S4BodKrRalLG1YINRJrqHXnlvpgw3bt2xtVSbxFrN0b1O9zhFRSUqJ6dO+s1WWcZ3zFBVpWkXm/tKxQquo1w8SxqEh+y88rO6iCSEpKVI8e6Vq9+r2yz2MBxFU0v/2ys+K9OcrR7q5dwvbv2dSVq0rdl96pb0875z4ws16SWjrnHi4Wc7WkcyTdKGmYpF3OuQl+62RQKTTMLKtY0X7nnO+vVIpuV08l1x/a4Zw7ok/KZtZCJX8ZbpVz7oh+DqO86wtERR1Uqmoqy6BSVVARB5WqMgaVKo7KMKhUVVSkQaWqriIOKlVVFXFQqSqrTINK0S7qB5UyMsI3qJSVVdag0nhJbzjn3vNOhTvNOfdgsZhnJC10zr1tZo0kPeCcO89fnUx/CxHnXMYRbrdLh9dsKo92bKvI9QEAAAAAgLB4X54pb+9Jai7J18Jzn0o6yXu/tTzLAPnFoBIAAAAAAEAIVLBff3tLnl9/ry/Pj5ANNLOxzrm/F4p5QdKLZjZQnh/0uqC0ChlUAgAAAAAAiHLOuWwzy5DUU9Kj3plSHxSLyZF0YaB1MqgEAAAAAAAQAi6/Yi0Z5Zzbp8O/APe7xZRXRQAAAAAAAKg6yFQCAAAAAAAIgQq2plK5I1MJAAAAAAAAQSNTCQAAAAAAIAScq1hrKpU3MpUAAAAAAAAQNAaVAAAAAAAAEDSmvwEAAAAAAIQAC3UDAAAAAAAAxZCpBAAAAAAAEAIun4W6AQAAAAAAgCLIVAIAAAAAAAgB5yLdgtAiUwkAAAAAAABBI1MJAAAAAAAgBFhTCQAAAAAAACiGTCUAAAAAAIAQIFMJAAAAAAAAKIZMJQAAAAAAgBDg198AAAAAAACAYshUAgAAAAAACIFoX1OJQSVUeC7a8wUriYsObI50E+B1VExspJuAQnq1uC7STYDXkm3PR7oJ8Lq99d2RbgK8pu7dFOkmwKv6UUdHugko5L8/5US6CUBUYFAJAAAAAAAgBJyL7kwl1lQCAAAAAABA0BhUAgAAAAAAQNCY/gYAAAAAABACLj/SLQgtMpUAAAAAAAAQNDKVAAAAAAAAQiCfhboBAAAAAACAoshUAgAAAAAACAFHphIAAAAAAABQFJlKAAAAAAAAIeDyyVQCAAAAAAAAiiBTCQAAAAAAIASci3QLQotMJQAAAAAAAASNTCUAAAAAAIAQYE0lAAAAAAAAoBgylQAAAAAAAEIg35GpBAAAAAAAABRBphIAAAAAAEAIODKVAAAAAAAAgKIYVAIAAAAAAEDQmP4GAAAAAAAQAs5FugWhRaYSAAAAAAAAgkamEgAAAAAAQAjks1A3AAAAAAAAUBSZSgAAAAAAACHgyFQCAAAAAAAAimJQCQizpOREZXTtqGNrJ5dahvDiGAAAUHklJyeqa7dOXMcBVDjOhe8WCQENKpnZ/Wa2zszmmtksM+vkLR9tZpeZWZaZrTezzWY2oZR6vjCzId77u8zsSjM7zczWmNlGM7ulUFxv7/0sM8vwU1+Ctz3vmtlj3jJf9X3urWermY0opX0/mdlqM1thZo0KlWWZ2SYzu8lbdqGZrfWWdfRT14lm5szsdG+bnLesv3e7TWaW7i07YGbJ3u18vhS8cfu8fVtgZvUL/e0tM7vCe7+dmU333k/xHhO/+XbFtr3ezB703jcz+9rMqptZLzPb4H0NtPVTT4aZfWdmK737poWf5zjRzJb5a09l8vSzD+mdZTN0+103BhyTmlpX02dNUavWzbRg8WuqXedYn2U4cuV1XHDknpv4iJavmK27hg8NKiYlpY6WLJ1RIrZx41M1f/4rIWlrVXHn47dpwltP6bKbLvEbk1wnSeNnP1GifOyLY9Tw9JND2TwU8v0P+3T5DXdEuhlVxsWPXKdbZ49Rr6Hn+fx7tZrVdf3UEbrx1Xt09fO3KzYuVp0u66lhb47UsDdH6q7Fj+iiB68Nc6ujxzPPPayly2fqzruGBBxTq1ZNzZ77ot5a8Ipef2Oi4uLilFqvrmbOfkGtWjXTon9M4zp+BJ58ZqwWLnlDt95xfdAxdevW1rLVc4qUPTxupHr16RqStkazzOcf1+pV8/S3u28OOq54WVJSohbMe0VZK+bo2WceLrJ9Skodbdr4Tvl3AFVSmYNKZtZBUmdJHSUtkdTLT+iFzrnWkpocGpDxo5mZpUpK9T4eL+lB73PcZGZ/lOQkDQug/TdL+sg5ly7peDNr6ae+POdchqTWkv5qZvF+6tvpnOssabKk1wqVZUhqJ+l677ZjJWVIulTSLWW0sZmk5t77x0h6TtJZkgZJmuItj5d0TQD9fd8510nSOkkTJMnM4rx97S1JzrkNkpLN7ERJQyQ96ZzvMcvi20p6R1J37/0Wkj6S9D9vm8+VdJmkB0pp3yLnXFdJt0u6289zRIU/n9tLsbEx6t1jgNLSUnXSyScEFHNao1N0z4gHNO6xiVq+bLWaNz/dZxmOTHkeFxyZc/v2VmxMrLp3O19paak6+eQTA4pJSqqlzMnjlJBQ8vT88MN/V9zRR4eh9dGp81mdFBMTq2H9blGd1Nr6Q4M/lIipkVhDI566S9XiqxUp79G/m7776jt9+tFn4WpulbY/O0f3jB2nn37+OdJNqRKa9W6rmNgYPXn+SCWmJqvuifVKxLTu21krpyzSc4MeUM7eH9WoSwuteW2pJgwcowkDx+izjf/WumlR8V1Z2P3l3N6KjY1Rz+4X+r1e+IoZcFFfPTPhRfX7y+Xavft79eiZrkaNTtXdw8fq8cee0/Jl76pFC67jwTj7Lz0VExOrP/e6WKlpKWpwUsnPT6XFjBp7l6pVO3z9aNe+lVJS62jJ2yvD0v5o0a/fWYqNjVXnLn2Vlpaqhg0bBBznq+yyS8/X62/MUUa381SzZoJatWxWUMejj4xU9erVfNaP8pfvLGy3SAgkU6m3pMXegYl3JP3qL9DMjpJUTdIvfkK+ktRAnkGWj71l7SS965z7n6QtklpKypNU08waBtC2Rd77md7n9VXfIcdIMnkGrUozXVIjM6tZqKy6pFhv23IlXS3pc+fchaXU87EODyp97K1jp3Pue+fcvyUdKylR0mZJl5tZbBntOmSqpC7e+x0kLZDU2swOHc9H5RnU6S3pzVLqKbKtc+5zSbXMLElSD3mO90mSDjrndjvn/k9SIF+f1pF0oJT2VXqdOrfTW3MWS5LeXbVeZ7ZvHVDMqqx12rxpmzp0bKNWrZtp08atPstwZMrzuODIdO58pubMWShJWpW1Th06tAkoJi8vX5cPGqrs7NwisZdffqHeffe90Dc8irVo30xZC1dJkras26ambZqUiMnPy9eYG8bqYO7BgrKaSTV1w73XKWd/jlp0aF5iG5S/2NgYPT7mbp+Dqyh/p5zZWFsXrpckfbLuQ53U5rQSMWteW6Ida7ZLkmocW0u5/80u+FtiarJq1UnU1x9+Hp4GR5nOndtprvd6vGrVep3ZoeQ121fMlMmvaeWKNZKkOnWO1fd7/6uslWu16dB1vFVzbdzAdTwYHTu11fy5/5AkrXl3g9q1bxVwTKf0djp48Cft2f29JOmoo47SuKfv19dffqs+Z3cLUw+iQ5f09po1a4EkaWXWWnX08RnKX5yvsv/+sE9/OvVkJSbW0h+Pq6+vvt4pSeqa0VEHDhzU7l17wtArVAWB/CM/VdIPkuQdVFjgJ26mpJ2S5jrn/H2lmS/PoMwZkt73ltXU4QGIg5Jqee+PV9nZSoXbttw596Gf+mLNLEvSF5Luc879VFqlzrl8ST/KM+DzB++2/5B0j3ew6mx5Bqv+aWa+3+0e/5TUSFJjSR8Ua1vh9u2XtFKebKBA/FdSkvd+b2/b/implbf9y7zte8k5l1dKPSW2lbRUUjd5MpbellRXUraZ1fXuh5JzIw47x8xWS3pF0sRSnqNMZjbYO3Vv8/9+zS57gzCLj4/Xd9/uliTlZOcqJaVOUDH9zz9Hv/76m/Ly8kotQ3BCcVwQnISEeH17aP/m+D4GvmJycnKVnZ1TJO7YY5M0cGB/PfVUZugbHsWqxVfT9995PuwfzDmo5LpJJWIO5h7UgZyDRcouuOY8ZS16VwteW6Re5/dUh57tw9HcKq1GQoJq1kiIdDOqjKPjj9GPu/dJkn7O/Uk16yT6jT2x5SmqnpigL7Z+UlDW+fLeWvP60pC3M1rFJ8Tr2293SZJycnJ8X7NLiWnb9gwlJSdq06ZtBWXnn/9n/fob1/FgxSdU13ffHb4u102pHVBMXFycbrtriMaOHlcQN+DivvrPjk/1zPgpOqNVM109+LLwdKISeu7ZR7R86cyC27ChV2vnodd7do5SU+v63C4hIb5EnK+ytWs3qmHDBho29K/6945PtW/ffsXFxenv99yiv93zYHg6CUmeX38L1y0SAhlUypZUQ5K86+lcUezvh7J+LpQ0V9InKt0nks6TJ4uoSP2SEryPJWmePFPMCmcLlda2G8ysm5/6Dk1/2yLPlK5SedcgSpRnYGmncy7DOZfunJtlZsdIquucGyzpEkmvllLVz/JkRx0tzzSywm0r3t+nJflfgKSoY+UdTJNnOuLNkppK6lMo5iOV3Vdf274j6RxJJzjnPpZnwKumc26vdx8eU0p9i7zTB7vpcIaUv/aVyjmX6Zxr7ZxrfUxcrbI3CLMDBw6omjdlNKFGvGJiSr6VSou587bR2rhhi3qf1bXUMgQnFMcFwTmQe7AgBT6hRrwspuTFLZAYSRpz/3CNHPWofvvtt9A1uAr46cDPOrq659RdPaG6YgJMGj2lSUO9NXW+9u3dp6yFq9SifbOyNwIqkf8d/FlHV/NMrT06vpr8LUEZn5igC0ZfpWl3TSooMzOd0v50fbK+zI+V8OPAgQOqfuhakJDg89zkLyY5OVGPjhutIdffVST+9ttGaeN776vPWWTIBONA7sFC+9nP5ycfMcNuvVYvTXld2fsPfynUpFljvfrSDO3d871mTZ+vjuk+l2OFpBuHDFf3nhcW3J6eMOXwPq6R4PM4SFJu7oEScb7Kxt4/QjcOGaGxDzylHTs+05VXXKThdw3Rc5Ne1v79Fe9Le1RegXyyXCupp/d+F3myZE7yPj5J0q5CsU9KurWM+rZISpG0z/t4g6QM72BNS3kzmLwZNlNVdPpaaW37s6S9/urzGifPej9lOV/SNudcro+/xUma5Z2qtkNlT6Xb7b1JnkGmP3gX0G4s6Xt5Bm3knPtUnml1gbhM0lIzS5F0lHOugzzZUwGvW1TKtislXSRptffxZ5KONbPjvYuD+x4yL2qfPNMXj7h9Fd22rR/qTG/ab5OmjfTVl98EFHPzrYN10cX9JEmJibW0/8ccn2U4MuV5XHBktm7drg7eKQxNmzb2eQwCiZGkTp3a6f77R+gfb7+pZs0aa+SoQE7fKO4///yPmrbxrC9ycuOTtOubXWVs4bHzi29V/4Q0SdKfmp2q3TtJk0d0+Xr75zqp9Z8kSX9ofIJ++GZviZjYuFhd9eytWvDoG9q38/uC8pPbnqYvt30atrZGo21bPyyY8ta0aSN99ZWfa3axmLi4OE199RndN+oxff31t5KkW267Thdf0l+S9zrOP5iD8sG2j9TW+9no9Can6esvdwYUk57RXlddc6nmLHxFTZqepicm3K8v/u9LndDgj5Kk5mc00TdffRu+jlRyW7ZuV8eOnkkwzZs11hdffh1wnK+y+PjqatrkNMXExKht2zPknFP3bp114/VXaPnSmWre/HQ9P+mx8HSuiov2NZWOCiBmvqQeZrZOnkGQLpIyzexaSd/JMwhxryQ553Z4f6GsrXNuo5/63tfhLCXJs9D1C946xjvnvin0TdELku4rpW0PSJpqZv0lrXbObff+4pvP+pxzS83sITP7g3Ou5NnSM+DzrjyDOz5zNZ1zuWY2UdJ6eabz/b2U9h3qr+SZ+vU/STdKWizPYNTgYrFPqvQpcK287dvlraePpCxvuz4zs3pmluic219GmyRPBpHPbc3sPXkyluSc+5+ZXSfP62CXPIOK/pxjZqskJcszeFfiOeTJAGtrZpu923zvnAs4g6miWLxwmRYveUP10lLVo1e6rrniFt0z8lY9MOZJvzG9ul4gi4nRS688rcuvHKCP//WJVixfrfff/6BEGY5MeR4XHJkFC5Zo6dKZSktLVc9eGbryimEaOep2jblvnN+Yrhn9fdbVovnhb5r/8fabRepA4Na8s05Pz3lCdVJrq23Xtrr/xgf01zuv1IuPTS11uzcnztAdj92my4Zdop9/+lkjry3tcgxUPtuXbNLNM+9TYmqyGmWcoanDxuuc2y/SonHTC2LaX9RNf2zaQL2G9levof215rWl2rpwvU5Lb65PN3xcSu0oy8IFS/X20umea0HPLrrqypt078jbdP+YJ/zGdO96vi6/YoBatGiiO+68UXfceaNemPK6pr74hqa++owuv+Iiffyv/2j5Mq7jwfjHomWa/4/XVa9eirr17Kzr/nqbRvz9Zj08drzfmLN7XKQ5sxYW/H3Owld027B7lVAjQU89+4D6nXe24uLidPXlN0WiS5XSvHlvK2vlXNVPq6fefbqqY6e/qFGjU3TxwP4aOerRUuOccyXKPvv0C02Z8oROOP44vffe+3pz+lt64cVpBfUsXzpT111/ZyS6iihjfn4YDKgwkms0rJAv0sSkWurarZPWrdmoPXu+P+IYlK+qcFx+ya/Y08GSkmqpW7fOWrt2o3bvLvnNf6AxlUXbY0+JdBPKVCOxhlp3bqkPNmzXvr37yt6gklqy7flINwFet7e+O9JNCEj1Wgk6rXNTfbrxY+XsDeQ7ucpn6t5NkW6CX0ne6/HatRsLFno+kpjKovpRFfeXTBOTaqlL1w5av3az9pby+amsmMrkvz9VvMz0pKRE9eiRrtWr3yv185GvuEC3rYh++2VnZFJswuS9+ueF7d+zZ347J+z7MiSDSmbWQtJTxYpXOedGVZD6+kgaUax4unNuoq/4AOq7UtKVxYrHO+fmVpD6RqjkekZ3O+fWV4T6ylJRB5WASKnog0pVTWUYVKoqGFSqOCrLoFJVUJEHlaqaijyoVBVVxEGlqopBpfITiUGlQKa/Bc05t02eRbYran1vy/PLZuVV31R51n+qqPU9LOnhilofAAAAAADRKFJrHYVLYD8BAwAAAAAAABTCoBIAAAAAAACCFpLpbwAAAAAAAFWdY/obAAAAAAAAUBSZSgAAAAAAACGQH+kGhBiZSgAAAAAAAAgamUoAAAAAAAAh4MSaSgAAAAAAAEARZCoBAAAAAACEQL6LdAtCi0wlAAAAAAAABI1MJQAAAAAAgBDIZ00lAAAAAAAAoCgylQAAAAAAAEKAX38DAAAAAAAAiiFTCQAAAAAAIATyI92AECNTCQAAAAAAAEEjUwkAAAAAACAEWFMJAAAAAAAAKIZBJQAAAAAAAASN6W8AAAAAAAAhwELdAAAAAAAAQDFkKgEAAAAAAIRAtGcqMaiECu/nvF8j3QSgQnHORboJKGTt3o8j3QR43d767kg3AV7jNj8U6SbAa/Ifu0a6CfCKt+j+BajKhqMBlA8GlQAAAAAAAELARfkQJmsqAQAAAAAAIGhkKgEAAAAAAIRAfnQnKpGpBAAAAAAAgOCRqQQAAAAAABAC+aypBAAAAAAAABRFphIAAAAAAEAIuEg3IMTIVAIAAAAAAEDQyFQCAAAAAAAIgfxINyDEyFQCAAAAAABA0MhUAgAAAAAACIF849ffAAAAAAAAgCIYVAIAAAAAAEDQmP4GAAAAAAAQAi7SDQgxMpUAAAAAAAAQNDKVAAAAAAAAQiA/0g0IMTKVAAAAAAAAEDQylQAAAAAAAEIg3yLdgtAiUwkAAAAAAABBY1AJAAAAAAAgBPJlYbsFwsxeMLN1Zvb3MuJSzWxrWfUxqAQAAAAAABDlzOw8SbHOuQ6S6pvZKaWEPy6pell1MqgEAAAAAAAQAi6MNzMbbGabC90GF2tOhqQZ3vsrJHXy1WYz6ybpgKRdZfWPhboBAAAAAAAqOedcpqTMUkISJO303s+W1LB4gJkdLWmkpH6S3irrORlUAgAAAAAACIEK9utvuTo8pa2GfM9eGyHpWefcj2ZlN57pbwAAAAAAANHvfR2e8tZc0hc+YnpIGmJmWZJamNmU0iokUwkAAAAAACAE8iPdgKLekrTazOpLOkvSQDMb65wr+CU451z6oftmluWcu6a0CslUAgAAv0u9einq1q2TatRIiHRTAAAA4IdzLluexbrfk9TVOfdB4QElH/EZZdXJoFIVZmZTzWyrma03s5lm9rr3cZb3Vs/Mks1skZmtMbP7Sqkrwcxmmdm7ZvaYtyzLW/daM3vFzGLNbIqZbTOz77x/bxC+HofOxImPauXKORo+fFhQMSkpdbRs2cxwNLHK4FhExqRJnn06YoT//e4rxt9248eP1dln95Ak1apVU/PmvaxFi17X9OmZiouLC00nosDzkx7Xqqy3dPeIm4KOC7QsJaWOViyfXfC4aZNGev2159ShfRstXzaL41OGix+5TrfOHqNeQ8/z+fdqNavr+qkjdOOr9+jq529XbFysOl3WU8PeHKlhb47UXYsf0UUPXhvmVldd3/+wT5ffcEekmxG1yvOa/ac/NdSMGZND1tZo98SEsVrwzjTdcsf1QcfUqVtbS9/1XBeOP+EPem36JL21+FWNHntXSNscjTKff1zvrpqnu+++Oei44mW1atXUgvmv6h+L39DMmVMUFxen6wZfrmVLZ2rZ0pnavGmJnnv2kZD2Bx7h/PW3gNrj3D7n3AznXJm/7BYIBpUwzDnXXp4Fu3p4H2d4b7sk3S5puXOuk6QMMzvOTz03S/rImyp3vJm19JZf6JzrKOkXST28qXO3SFrkfY7PQ9i3sOjbt49iY2PUtet5SktL1cknnxhQTFJSLU2ePE7x8fHhb3SU4lhEhmefxgaw34vG+NuuY8e2Sk2tq8WLl0mSBg7sp6efnqJzzrlUu3fvVa9eGeHrXCXSr+9Zio2NUZeMfkqrn6qGDX2P2fuKC7QsKSlRL7zwlBISDr9XGjU6RdcOvl1jH3hSn3/+lRo0+GO4ulzpNOvdVjGxMXry/JFKTE1W3RPrlYhp3bezVk5ZpOcGPaCcvT+qUZcWWvPaUk0YOEYTBo7RZxv/rXXTlkWg9VXP/uwc3TN2nH76+edINyUqlec1u0GD4/Xgg39TYmLNMPYgepz9l56KjY3RX3pfonr1UtTgpBOCihk19i5Vq1ZNkvT30bfriccmqt/Zg5RWv546dGoTtn5Udv36naXY2Fild+mr+mmlXMd9xPkqu+Ti8/TU+EyddfbF2r1rr3r3ztDzma+oR88L1aPnhVqzZoOmTHktzL1ENGJQCTLPku415Bn4KS5DUpb3/l8l/einmt6SFnnvZxauy1t/kqSffm9bK6L09DM1e7an66tWrVOHDiUvnr5i8vLyNWjQUOXk5IS1vdGMYxEZ6entNWvWQklSVtY6dezoa7+XjPFVdtRRR+m55x7Wl19+oz//uackKTPzVS1fvlqSVKdObe3d+304ulXppHdpr1mzvftzpe/Xv7+4QMvy8vJ06aU3KDs7t6C+GTPn68svv9FZZ3VTUnKiPv30ixD2snI75czG2rpwvSTpk3Uf6qQ2p5WIWfPaEu1Ys12SVOPYWsr9b3bB3xJTk1WrTqK+/rDSfx9TKcTGxujxMXcXGURF+SnPa3Zu7gFdfLH/DBuUrkOnNpo/921J0pp331O79i0DjumY3k4HDxzU3j2ea/NJDU/U9g/+JUn6/vv/qmYtBvoC1SW9vWbOWiBJWpm1Vh39XMd9xfkqm/T8y4c/P9Wtrb17/ltQR/369ZSSWldbtm4PZZdQRTCohAnyrPi+W9IKSRO809IO5RTXlZRtZuMkzZZU8hOwR6qkHyTJObfcOfeht3ympNWSPnbOvRtoo8xssJltNrPNv/2WW/YGERQfH69vv/VkDmZn5yg1tU5AMTk5ucrOZhCjPHEsIiMhoXrBPs3JyVFKSt2AYnyVXXbZ+fr440/0xBOT1KZNC91ww5UFdbRr11LJyYnauHFr6DtVCTz77MNaumRmwW3okL/q253fSZKyc3KU6uM4SFJCfHyJuEDL/L1XatRI0AXn/0X7fvhRzgWafF31HB1/jH7cvU+S9HPuT6pZJ9Fv7IktT1H1xAR9sfWTgrLOl/fWmteXhryd8KiRkKCarBMWMuV5zd6797/65Rdf340iEPHx8fruu92SpJycXNWpWzugmLi4ON1+14164L4nCuIWzlui24ffqJ59MtS1e2etXvVeeDpRCT337CMFU9GWLZ2poUOvLvJ6T0n1cx1PiC8R56vskDPbtVJyUqI2bNxSUHbDDVfq+edfCVXXUEy+he8WCfz6G4bJ85OC/5PnJwWHOefWFPr7fkk1nXO3m9nDkqr5qSdbnmwnmdkNknZ4yy90zn0TbKOcc5nyZDypevUTKvS/UA4cOKDq1T27pUaNBJmVHKsNJAa/H8ciMnJzDxbs04SEBMXElLyi+YrxVda8+el64YVp2r17r6ZNm6v77rtTEydOVXJyop54Yowuvvi68HWsghsyZESRx+PG3adq1atL8vxj2NdxkKTcAwdKxAVa5s/+/dm6+ppb9dKL49W6dQtt2sTAny//O/izjq52tCTp6Phq8iTylhSfmKALRl+lF244/A81M9Mp7U/XwsfeDEtbgVDjml1xHDhwoGD6WkKNBMXE+D4WxWOG3XqtXpo8Tdn7Dw/yPfX4JLU9s6VuvOmvmvHGWzp44GB4OlEJ3ThkeJHHT4y7T9WrHX69+zoOkiczr3icrzJJSk5O0lNP3a8BFx1ei8/MlNGlg+699+Fy7xOqJs7MkKTnJV0tKdbH39ZK6uO937yUOtZK6um9/2dJe8utdRXcli3bC1K2mzZtpK++KjmGFkgMfj+ORWRs3Xp4nzZr1lhffllyn/qK8VX22WdfqkGD4yVJrVo101df7VRcXJxef32iRo58RF99tTNMvap8tm75Z8HUQ3/HwV9coGW+TJjwoDp1aidJSkqqpf3795drv6LJ19s/10mt/yRJ+kPjE/TDNyUvlbFxsbrq2Vu14NE3tG/n4ameJ7c9TV9u+zRsbQVCjWt2xfHPbf9SuzM909lOb/Infe3jWusrpnNGe1117SWas/Blnd70NI17+n5J0ofb/60/HJem55+dGrY+RIMtW7erQ5Hr7tcBx/kqi4uL0xvTJumevz9U5PNTp07tyPoOs/ww3iKBTCXIObfPzFZIukZSEzM79C+CUZIekDTNzM6SVNqiAg9Immpm/SWtds5t9/cNbLRZsGCJli2bqbS0VPXqlaHLLx+qUaPu0H33Pe43pkuXfpFrcBTjWETG/PnvaPnyWUpLS1Xv3hkaNGioRo++Q6NHP+43Jj29n5xzJcry8/P1/POPa8CAc3XUUUfpkkuu15VXXqQzzmiq4cOHavjwocrMfE2zvOsG4LB589/RyhVzlJaWqj69u6pT53PV6LRTNHBgP40a/Vipcc65gMp8GTduol56cbycc1q27F395z//F64uVzrbl2zSzTPvU2JqshplnKGpw8brnNsv0qJx0wti2l/UTX9s2kC9hvZXr6H9tea1pdq6cL1OS2+uTzd8HMHWA+WLa3bF8Y9FyzTvH68pNS1F3Xp01vVX367h99ysRx4Y7zfmnJ4DNXfWooK/z1n4sm6/6V5J0pCb/qrnn31ZP/3EIvfBmDfvbWWtnKv6afXUu09Xder0FzVqdIoGDuyvUaMeLTXOOVei7K9XXayWLZvq7hE36e4RN+n5zFc1c+Z89eqVodVrmJaI8mOsfYCKrqJPf5M83853795Za9Zs1O7dvpO0AonB71cVjkVFPG8nJSV69+mGUvZ7yZhAtqvo8l2kvhcqKSkpUT26d9bqMvanr7hAyyqy69M6RroJZapeK0GndW6qTzd+rJy90ZvVNW7zQ5FuArxq/bFrpJvgV1W4ZheWeEzFXfQ9MbGWunTtoPXrNhcsun0kMZXJfw9mlx0UZklJierRI12rV79X9nW8WFyg21ZEv/6yM6qzEZ4/7rKwfXi/7pvXwr4vGVRCUMyshaSnihWvcs6NCtVzVoZBJSCcOG9XLBVpUKmqqwyDSlUFg0oVR0UeVKpqKvKgUlVUEQeVqioGlcpPJAaVmP6GoDjntknKiHAzAAAAAACo8FxUD5mxUDcAAAAAAACOAJlKAAAAAAAAIRDtCyWQqQQAAAAAAICgkakEAAAAAAAQAmQqAQAAAAAAAMWQqQQAAAAAABACLtINCDEylQAAAAAAABA0MpUAAAAAAABCIN8i3YLQIlMJAAAAAAAAQWNQCQAAAAAAAEFj+hsAAAAAAEAI5Ee6ASFGphIAAAAAAACCRqYSAAAAAABACJCpBAAAAAAAABRDphIAAAAAAEAIuEg3IMTIVAIAAAAAAEDQyFQCAAAAAAAIgXyLdAtCi0wlAAAAAAAABI1MJQAAAAAAgBDg198AAAAAAACAYshUAgAAAAAACAF+/Q0AAAAAAAAohkwlAAAAAACAEMiP8lwlBpVQ4eW7aF/aDAiOc9F9YapszKL8d2Irkal7N0W6CfCa/MeukW4CvLK/XhnpJsAro/k1kW4CCvnhp5xINwGICgwqAQAAAAAAhEC0p0iwphIAAAAAAACCxqASAAAAAAAAgsb0NwAAAAAAgBCI9tVQyVQCAAAAAABA0MhUAgAAAAAACAEW6gYAAAAAAACKIVMJAAAAAAAgBPIt0i0ILTKVAAAAAAAAEDQylQAAAAAAAEIgP8p//41MJQAAAAAAAASNTCUAAAAAAIAQiO48JTKVAAAAAAAAcATIVAIAAAAAAAiB/Eg3IMTIVAIAAAAAAEDQyFQCAAAAAAAIAX79DQAAAAAAACiGTCUAAAAAAIAQiO48JTKVAAAAAAAAcAQYVAIAAAAAAEDQmP4GAAAAAAAQAvmRbkCIkakEAAAAAACAoJGpBAAAAAAAEAL5Ub5UN5lKAAAAAAAACBqZSgAAAAAAACEQ3XlKZCoBAAAAAADgCDCoBAAAUIkkJyeqa7dOOrZ2cqSbAoTN9z/s0+U33BHpZgBA0PLDeIsEBpVQhJlNNbOtZrbezGaa2evex1neWz0zSzazRWa2xszuK6Wuz83sXTNbZ2YdvGULzezf3r9lmVn18PWufEya9JiyVs7ViBE3BRXjqywlpY6WL59dYvvZs15U8+anl2/Do1B5HYtatWpq/rxXtHjR65oxfbLi4uI0+NpBWrJkhpYsmaGNG97Ws888FPL+RJPnJz2uVVlv6e5Sjo2/uOJltWrV1Pz5r2rx4mmaOWOK4uLiQtr2yi6U56j69evps083Frw36tQ5NjSdiELPPPewli6fqTvvGhJwTK1aNTV77ot6a8Erev2NiYqLi1NqvbqaOfsFtWrVTIv+MU21OQZBmzjxUa1cOUfDhw8LKiYlpY6WLZtZJO5Pf2qoGTMmh6yt8NifnaN7xo7TTz//HOmmVAkjHr9Dk+ZN0BU3X+Y3JrlOsp6b81SJ8gZ/OlFPTns0hK2LXk8//YDOOadHucUVV/zzVWxsrD79ZIOWLpmppUtmqsnppwVdJyAxqATfhjnn2kvKldTD+zjDe9sl6XZJy51znSRlmNlxfurJc86le+Nnm9lRzrk/S3pY0gve+n4KQ3/KTd++fRQbG6uMrv1VPy1VDU8+MaAYX2VJSYl6YcqTSogvOq42cGA/ff75l/rgg4/C1KvKqTyPxcUD+2v805N19jmXavfuverdK0OZk19Vr14D1KvXAK1du1FTXpgW/k5WUv36nqXY2Bh1yeintPqpatiwQcBxvsouvri/xo/P1NlnX6Jdu/eod++M8HaoEgn1OapNmxZ65JEJBe+N77//IYy9q7z+cm5vxcbGqGf3C5WWlqqTfRwXXzEDLuqrZya8qH5/uVy7d3+vHj3T1ajRqbp7+Fg9/thzWr7sXbVowRcQwfC81mPUtet5fo+Fr5ikpFqaPHmc4uPjC+IaNDheDz74NyUm1gxjD6qm2NgYPT7mbiUkxJcdjN+ly1mdFRsbo+v7DlOd1No6rsEfSsTUTKyhvz81XNXiS343fNOoG3VUHMv2Bqtjx7aql5qiRYuWlUtccb4+XzVt2kgzZsxTz14XqmevC/XhR//+PV1AKVwY/4sEBpXgk5mZpBqSfvHx5wxJWd77f5X0Y2l1OefWS8qWFPDwt5kNNrPNZrY5Ly830M1Crkt6e82etUCStDJrrTp0bBtQjK+yvLw8XXrZjcrOOdy/5OQkPfLwvdr343516dI+DD2qvMrzWDyf+YqWL18tSapT51jt2ft9QR3169dTSkodbd26PdRdihrpXdpr1uyFkqSslevUoUObgON8lT3//OHjU7dObe3Z898w9KJyCvU5ql3blrruusu1KustPfboqDD0KDp07txOc+csliStWrVeZ3ZoHVDMlMmvaeWKNZI856bv9/5XWSvXatOmberQsY1atWqujRu2hq8jUSA9/UzNnr1IkrRqle/zk6+YvLx8DRo0VDk5OQVxubkHdPHF14en4VVcjYQE1ayREOlmVAlntG+uFQtWSZLeX7tVzdo0LRGTl5evkTfcrwM5B4qUn3NRH21ZxzkpWEcddZQmTXxUX3z5tf7yl15BxVWvXk1vvvG8li+bpfHjx/rd1tfnq3btWqpv3z5auWKOXp46QbGxseXbMVQZDCPDlwmSjpW0QNIKSRPMbL+kvc65CyXVlZRtZuMkdZd0jaTNZdT5X0lJgTbAOZcpKVOSjqn2xwqzYH58Qrx2frtLkpSTk+vzG05fMb7KcnJKDpbddNM1mjNnkaZMeV333z9CNWvU0MJFS0PXoUqsPI/FIe3atVRycqI2bjz8gej6669Q5uRXQ9eRKPDssw/r1FNOLnjcpUt7TX3pDUlSdk6Oz2MjSQnx8fp253dF4nyVHdKuXUslJSdq48YtoelIFAj1OeqdJSv14EPjlZt7QG/NnaomTU7Thx/yzWZZ4hPi9W3B/s3RSSefEFRM27ZnKCk5UZs2bSsoO//8P+vX335TXl5eaBsfZeLjD+/n7OwcnezrWPiI8fV+2LuXAW5En2rx1bR3115J0sGcgzruxJKZSgdzD5Yoq5VcS73P76lbL7lLbbv4/jIJHsU/N61atU4ff/yJxo2bqCE3XqU//vEPeu65l0psd9llF5SIi42N0Ucf7dD9Y5/QjOmT1bRJI11/wxVF6s/KWuvz89WKlWvUvccF2rVrj8aPH6uzzuqmhQv5d0coRGqto3BhUAm+DJPUSdL/JDWXZ/rbmkJ/3y+ppnPudjN7WFK1AOo8VlKlnydxIPeAqlf3dDchIV4xMSWT/XzFBLKdJLVofrpGjBir3bv3avasBerePZ1BJT/K+1gkJyfpySfu18CLBxdsb2bqkt5eI0c+EuruVGpDhowo8njcuPtUrbonJb5GQoJiYszndrkHDpSI81UmeY7PU0/er4sGDvZZFzxCfY5av/59/fKLJ4F1x47P1LBhAwaVAnDgwAFVr3Zo/yYoxnwcFz8xycmJenTcaA265IYi8bffNkp/v/dW9Tmrm+Z4s2pQtgMHDr/Wa9RIkPk7FmXEANHqpwM/6Zhqx0iSqidU93sNL+6Gu6/VpIcmK+83BrrLUvxz01NPjdWUF17X7t17Ne2NORpz33Cfg0otWjQpEbd7z161P7O10tPbKymplur/oV6J+iXfn822b/+4xDUdOBJcJeHP85KuluQrD3KtpD7e+83LqsjM2kiqLmlHubUuQrZs3a6OHTzTSZo1a6wvv/wmoJhAtpOkzz77Qg0aHC9Jatmqub76ynccyvdYxMXFadrrz+nekQ/rq692FmzfqVM7bdpEGnewtm75pzp29HxLWdrr3Vecr7K4uDhNmzZRf7+36PFBSaE+Ry1c+Jrq1UtR9erV1LNnF/3ro0p/Wg+LbVs/LJjy1rRpI5/ndl8xcXFxmvrqM7pv1GP6+utvJUm33HadLr6kvyQpMbGW9u/PDlMvosOWLdsLprz5OxaBxADRasf2/xRMeWvY+CR99/WugLZr0b6ZbvjbYE2Y+YROOb2hrr3rr6FsZlT57LPPCz7/t2rZXF/6Oef4ivvPfz7T0xOmqGevCzVq9KP62s/nJF+fr6a+NF7NmjZSTEyM+p7bR//8579C0DtIUr5c2G6RQKYSfHLO7TOzFfJMbWvinf4mSaMkPSBpmpmdJam0FRNjzWyVPBl//Zxzlf6ri/nz39GK5bOVlpaq3r0zdNmgIRo9+k6NHv2Y35jO6X3lnCtR5su4JyZp0sRHNXzEMP108CcNuIisDH/K81hcdeVAnXFGUw0fPkzDhw9TZuarmjVrgXr27KLVazZEsJeV07z572jlijlKS0tVn95d1anzuWp02ikaOLCfRhU6Pr7inHMlyq66aqBantFUI4bfpBHDb1Jm5iua6V3/B0WF+hz1wANPask70/XLL79q8uRX9Z9P/i9cXavUFi5YqreXTldaWqp69uyiq668SfeOvE33j3nCb0z3rufr8isGqEWLJrrjzht1x5036oUpr2vqi29o6qvP6PIrLtLH//qPli9bHcGeVT4LFizRsmUzlZaWql69MnT55UM1atQduu++x/3GdOnSL3INRhFTn+FXxULt3bfX6rm541WnXm2d2bWtRt14v66966+a/OiLpW53cecrCu5PmPlEmfE47KWX3tTkzHEacOG5iouL08CLr/P5uclX3I8/7teUyU/oissHKDsnV5dfPtTnc/j6zLV9+8d65ZVnZGZauHCJVqxY43NboCzmXIVZrgbwqSKtqSRJSUmJ6t69s9as2aDdu/cGHBPIdghOVT0WleG8nZSUqB7dO2t1GfvYV1yg21YUnt81qDiq6vtCko6JjYt0E/xKSqqlrt06ae3ajdqz+/sjjqksfs2vuN8jJSXV8r7WN5byHik7prLI/nplpJsAr4zm10S6CQGpmVhDbdJbadt7/9QPe/dFujkhs+n7/0S6CWFVkT9f/fK/byrWh6lydsOJA8L24X3iFzPCvi8ZVMLvZmYtJD1VrHiVc65cfhqoog0qAZHGebtiqWiDSlVZRR5Uqmoq8qBSVcOgUsVRWQaVqoqqNqhUkTGoVH4iMajE9Df8bs65bZIyItwMAAAAAAAQRgwqAQAAAAAAhECkFtAOF379DQAAAAAAAEEjUwkAAAAAACAE8iPdgBAjUwkAAAAAAABBI1MJAAAAAAAgBBxrKgEAAAAAAABFkakEAAAAAAAQAqypBAAAAAAAABRDphIAAAAAAEAIsKYSAAAAAAAAUAyZSgAAAAAAACHAmkoAAAAAAABAMWQqAQAAAAAAhEC+Y00lAAAAAAAAoAgylQAAAAAAAEIguvOUyFQCAAAAAADAEWBQCQAAAAAAAEFj+hsAAAAAAEAI5Ef5BDgylQAAAAAAABA0MpUAAAAAAABCwJGpBAAAAAAAABRFphIAAAAAAEAI5Ee6ASHGoBIqPJNFugmQdFRMbKSbAK9f83+LdBNQCOeoiqP6UUdHugnwijfeFxVFRvNrIt0EeGV9MCXSTUAhCX9Ij3QTgKjAoBIAAAAAAEAI8OtvAAAAAAAAQDFkKgEAAAAAAIQAv/4GAAAAAAAAFEOmEgAAAAAAQAhE+6+/kakEAAAAAABQBZjZC2a2zsz+7ufviWb2DzNbamZzzazUn9dlUAkAAAAAACAEnHNhu5XFzM6TFOuc6yCpvpmd4iPsUklPOOd6StolqU9pdTKoBAAAAAAAUMmZ2WAz21zoNrhYSIakGd77KyR1Kl6Hc+4559xS78O6kvaU9pysqQQAAAAAABAC+WH89TfnXKakzFJCEiTt9N7PltTQX6CZtZeU7Jx7r7TnZFAJAAAAAAAg+uVKqu69X0N+Zq+Z2bGSJkg6v6wKmf4GAAAAAAAQ/d7X4SlvzSV9UTzAuzD3DEl3O+e+LKtCBpUAAAAAAABCID+MtwC8JWmQmT0haYCkj8xsbLGYqyW1knSPmWWZ2UWlVcj0NwAAAAAAgCjnnMs2swxJPSU96pzbJemDYjETJU0MtE4GlQAAAAAAAELAhXGh7kA45/bp8C/A/W5MfwMAAAAAAEDQyFQCAAAAAAAIgfwKlqlU3shUAgAAAAAAQNDIVAIAAAAAAAgB58hUAgAAAAAAAIogUwkAAAAAACAE8iPdgBAjUwkAAAAAAABBI1MJAAAAAAAgBBy//gYAAAAAAAAURaYSAAAAAABACOSTqQQAAAAAAAAURaYSAAAAAABACDhHphIAAGGTnJyk7t07q3bt5Eg3BQAAIKT43IPKjkElFGFmNcxsrpmtNbOXzWysmX1sZu+a2XIzq29mU83sIW/8aDMb7aeuoWaWZWY/ef/f37vtTO/f3zSzqeHr3e83adKjWrlyjkaMGBZUTPGypKREvfXWVC1fPksTJjwoSYqNjdUnn6zXkiXTtWTJdJ1++p9C25ko9ezEh7VsxSzdNXxowDG1atXUnLde0vwFr+qNNycpLi4uXM2NGpMmPaaslXM1YsRNQcUUL6tXL0VvzZ2q1q1baMk7M1SnzrEafO0gLVkyQ0uWzNDGDW/r2WceCnl/KrNQnqckKSWljpYvnxW6DkSpJ58Zq4VL3tCtd1wfdEzdurW1bPWcImUPjxupXn26hqSt0e6JCWO14J1puqWUY+Evpk7d2lr67mxJ0vEn/EGvTZ+ktxa/qtFj7wppm6uCEY/foUnzJuiKmy/zG5NcJ1nPzXmqRHmDP52oJ6c9GsLWobDvf9iny2+4I9LNiAr16qVo3lsvq03rFlq6ZKbq1DnWZ1xKSh2tWD476PqPOuoozZ07Vauy3tIVV1wkSapfv57+77NNWrpkZqnPifKTLxe2WyQwqITihkn6xDnXUdIxkgZIesA5ly7pJe/fJekaM6tWWkXOuWeccxmSdjrnMpxzc71/aub9f/Nyb30I9e3bR7Gxsera9TylpaXq5JNPDCjGV9kll5ynN96Yq+7dL1DNmglq2bKZmjZtpOnT56tXr4vUq9dF+uijHeHvZCV3bt/eio2JVY9uF/x/e3cef+lc/3/88ZoxmH2IGUOoyJadwQyzGGYs/YoWJKIokkqLVN/6Zq1IZIkQ2bI1ssW3GszCkHUkIcoWyZZlxlIYr98f1/mMz3zm85lFc67rms953N3mNudc5zpnnudcrutc1+u8F4YOHdzpNupsnd0+sRMnn3QWH/7Qp3j66WcZN350+eEXY23/j4/Z+iOsOHQIq89j32i/TmfL1llnDb5xyOEcc8zJXHvtVDbacD3O+Pn5jB+/K+PH78pNN93GmWddWP6bXEw0+zg1aNBAzjzzePr06V3+m1uM7fihcfTo0ZP/N353hgwdzHvft+pCrXPoUYew9NJvf+VuPnwTBg9Zjom/m1xK/u5kxw+No2fPHnxou0+ywgpdb4uu1mm/Lb572Nc5/tifsfOOn2LoiiswYqthpb2P7mb0DiPp2bMHn9/pSyw35F28+70rzbVO/4H9+O4J32TpTo4/Xz70CyzRy1E9yvDSjJl856jjeO3f/646SrewzjprcPA3DufoY05m4rVT2Gij9eZaZ9CggZx11gn07dtnoV//wAM/w/Q7/8ToMTvzwR23pV+/vmw2bCOOPuYkxo3fhXHjd+G5555fFG9FLcyikjraHLihcXsacE27x5YBXmvc/jOwxzv8N16PiHcBb7zD51di1KjhXHrp1QBMmXIzW24598ljZ+t0tuz5519gjTVWY+DAAbz73Svy+OP/YLPNNmKnnbZj0qRfc845J9KzZ8/y3lw3MXLkFlx2WfG/7NQpf2D4iE0XaJ2fn/FLJk+aBsByy72LZ595rrzQ3cDoUcP59aW/AWDylJsYseVmC7ROZ8smTZrGbbfdxVZbbc6mwzbkllvvnP0aK664AoMHL8ddd91TwrtaPDX7ODVr1iz23PNAZs58ubw31Q1sudVmXHX5bwGYdsOtbD58kwVeZ6tRm/Pqq6/xzNPFcWmJJZbguJOO5PHHnmT7HceW9A66jxFbDeOqy38HwLQbbmHz4Rsv8DpbjtqcV195dfZ3xPtWfw/33H0fAM899y/6D+hfxlvoljYavgGTfjMVgDtvuov1h819YT1r1lt874AjeWXmK3Ms/+Bu2zP95rtKySno2bMHPz7i2++owKG5Fec909lqq80ZtumG3HLLnXOtM2vWLPbY4wBmzHj7u3fw4OW46qrzmTrlCg75xoFdvv7oUcO5tHGu9Ydb7mCTTTZg88035vP7780NU6/k2GMPXfRvSi3HopI66g+0fVu/CgwAvhMRNwBbACc2HjsF2P8d/ht3A7s1/u5UROwXEXdExB2zZtXj4qVv3948+eRTAMycOZPBg5dfoHU6W3bzzbez+urv4cADP8MDDzzECy+8xJ133s24cbsyduzHePHFGWy/vRcLC6tP3948+eTTAMyY+TKDBy+3UOtsttlGDBo0gNtv/2MpebuLPn378I/Z/4939bnPvc68nrfLxz/Em2+8waxZs2Yv+/zn9+aMn5/fzLey2Gv2cWrmzJeZMWNmeW+om+jTtzf//Gdx3Jk582WWH/yuBVqnV69efO2QAznqsONmr7fr7jvx4AN/46cnnslGm6zPvvt13VVIc+vTp88cn/Nyy3eyLTpZp1evXnz9kC/w/cOPn73e1VdO5Ovf/ALjth/D1tuM5Mapt5TzJrqhpfsszbNPPQvAqzNfZdnl5x5b5tWXX52roDRgmQFs97FxXHjar0rJKejXty/9+/WtOka3s8suH+KNN9+c47ynTWffvYcc8kUmTLiK0WN25sMf3p5llx3Ery89a3aXtmsnTmDfffeY41xrxoyXGTJ4OX73+8mMGr0To0bvxPvf/z7WW3ftUt5jK8sS/6uCRSV1NAPo17jdt3H/+5k5KjP3yMyXGo89BfwFGPMO/o3pwKcbf3cqM8/IzE0zc9OePft1tVqpXn75VXr3Lpq89+3blx49YoHW6WzZEUccwhe/+D/84Acn8sADf2PvvXflnnv+wlNPPQPAAw88xOqrv6ecN9aNvPLyq/ReeikA+vXrQ48ecx/iulpnmWUG8uPjD+OAz3+zvMDdxCsvv9Lu//GuPve515nX8w76ynf5wy13suOO2wIQEYweNZypU//Q7LezWGv2cUrvTHHcmd8+Mvc6X/rq5zj7zAuY8dLbFxPrrr8O55/9K5595jkuveQqthw1d8tAde2VV16Z3X2tb7++nW+LTtb50lc/x9k/v3CObXHCj09j0nU3ssdeH+dXF13Bq6+8Ws6b6IZee+U1lmp8N/fu27vTY1dnDvj25zjthz9n1ptzX4hLi5ODDvout/zhDj7YOO+ZnzXWWI3999uLaydOoG/f3qw4dAU+9vF9Z3dpGzd+F84664I5zrXaznv/8Ic7ePnlokD7wAN/Y/XV39u096XWYFFJHd3K24WikcCO81j3J8A7GXxmOjCMeRSV6uiuu+5hxIiiK8n666/DY489sUDrdLasd+/erLvuWvTo0YNhwzYiM/nFL05gvfXWpkePHuy003bcc8/95b25buKuu/7M8MZnvd56a3exjeZep1evXpx3/k859HvH8vjj/yg1c3cw/a572HJEcWHb1b7R2TqdLfv61w9gjz0+BsCggQN46cUZAGy11ebcfrvdG+an2ccpvTN3//FeNmt0Z/vAumvx+GNzH2c6W2fUmOF85rN7cNnV57Huemtx/MlH8ujDj7Hqe1cGYION1uWJvz9Z3hvpBv70x/vYfIuiO9sH1l2Tx/8+97bobJ2RY4bzmc99ksuuPpcPrLcWx510JAB/vucvrPTuoZx+yjmlvYfu6IF7Hpzd5W31dd7HPx9/aoGet+Hw9Tngf/bj5AnH8/4PrM7nDtmnmTGlRe7gr3+BPRvnPQMHDeTFl2Ys0PMefPAhvvPdHzJu/C4ce+ypPP/Ci52uN316u3Ot9dbh0cce55qrL2CFFQbTu/fSjB83mnvv+8sieS/q2luZpf2pgiPaqaOfAudHxM3AX4EJXa2YmXdFxNR38G88CjwIPPaOElbkqqt+z/XXX8rQoUPYbrsxfOpTX+Swww7msMN+3OU6o0btTGbOteyhhx7ljDOOY5VVVuLWW6dzySVXcsstd3LuuScTEVxzzbVMaozxowV39W8mMvHaXzF06GDGjx/D3nt/me8d+nWOOPy4LtfZesxH2fvTu7LhRutxyCEHcsghB3Lmz3/Jr399zTz+JbV31VW/Z9L1v579//ienzqQww77BocddmyX64wctROZOdeyHj16cOEFp/KZz+zOffc+wLXXFYeYceNGc+O0W6t6i4uNZh+n9M789prruOq3xUn82HEj2X+fr/Gt7x7E0Ued2OU6O267G5c1xrkCuOzq8/jal/6Xvv36csIp32fnj+5Ir1692Hevrmdc1Nx+e811XPnbXzJk6GDGbjuSz+/7db75nYM45vsndrnOB8d9gssvffs74bKrz+XrX/5fAA788j6cfsq5vPaagxb/N2743U2cevmJLLfCu9hi68049AtH8rlD9uHnP/rFPJ+3+8i9Z98+ecLx811fqpszz7qACy/8GZ/5zO7ce98D/OOJf3L4Yd/g0HbnUJ059thTOP30H3P4Yd/g0Ucf55Jfdf4dff4vJ3DVleex5Vabsfba7+e22+7iqO//hGsn/orXX3+DM37+Sx588OFmvDW1kPCXR9Xd0kuvUpv/SQcNGsg224xk2rRbefrpZxd4nQV5Xt0t0WPxGDh80KABjB07kmk33Tp7YNt3sk6dvfHWm1VHmEsr7xvBgnXTKEsrb4uBS9V34NiBgwYweusR/OGmO7qcDGBB1llcRNRrv2hv4MDG53zzPLbFAqyzuFit79CqIyyQ/gP7MWzUJvzxlj/x/LMvVB2nKabcfWbVEdRO35VGVR2hFEOHDmHLEcOYeO3U2o6L+Pp/nqjvl8YiMHKlbUq7nr3xH9eX/llaVNIiERFTOix6KTN3WhSvXaeiUitbXIpKraCORaVWVreiUiurc1Gp1dS5qNRqFpeiUiuwqFQvrVJUWhxYVFp0qigq2f1Ni0Rmjqk6gyRJkiRJdfJWRbOylcWBuiVJkiRJkrTQbKkkSZIkSZLUBLZUkiRJkiRJkjqwpZIkSZIkSVITdPfJ0WypJEmSJEmSpIVmSyVJkiRJkqQmcEwlSZIkSZIkqQNbKkmSJEmSJDVB2lJJkiRJkiRJmpNFJUmSJEmSJC00u79JkiRJkiQ1Qabd3yRJkiRJkqQ52FJJkiRJkiSpCd5yoG5JkiRJkiRpTrZUkiRJkiRJagLHVJIkSZIkSZI6sKWSJEmSJElSEzimkiRJkiRJktSBLZUkSZIkSZKaIG2pJEmSJEmSJM3JlkqSJEmSJElN8Jazv0mSJEmSJElzsqWSJEmSJElSE3T3MZUsKqn2Zr01q+oIwu0gdaV7nyYsXv712syqI6ghqg6g2Z53v6iNviuNqjqC2nnlHzdUHUHqFiwqSZIkSZIkNYFjKkmSJEmSJEkdWFSSJEmSJEnSQrP7myRJkiRJUhN094G6bakkSZIkSZKkhWZLJUmSJEmSpCZwoG5JkiRJkiSpA1sqSZIkSZIkNYFjKkmSJEmSJEkd2FJJkiRJkiSpCRxTSZIkSZIkSerAlkqSJEmSJElN4JhKkiRJkiRJUge2VJIkSZIkSWqCzLeqjtBUtlSSJEmSJEnSQrOlkiRJkiRJUhO85ZhKkiRJkiRJ0pxsqSRJkiRJktQEmbZUkiRJkiRJkuZgUUmSJEmSJEkLze5vkiRJkiRJTeBA3ZIkSZIkSVIHtlSSJEmSJElqAgfqliRJkiRJkjqwpZIkSZIkSVITvGVLJUmSJEmSJGlOFpUkqZ1llhnENtuM5F3vWqbqKJIkSYuc5zrdx3PPv8BeBxxcdQzNR5b4XxW6XVEpIs6JiAmN2xdHxKURcXlE3BQR50bEEo11fthY57CIOGwerzckIm5sd3+ViJgSEZMi4oyIiHk8d8OIeKTd/Z6N59zYyNKj8VrHNh6/passjfX2b/ceP934+67GY1MiYoWIODMi/hgR/2wse+888u0cEZPb3e/b+LxuaJdpgV6vi+c+GhHbtcs/ZmHyLQ7OOP3H3DD1Sr797YMWer2Oy3r27MlDf7uN666dwHXXTmDddddqavbuYlFugxVWGMxVV57HsGEbcd21E1huuWUZMKA/v7nqfH77fxcxYcKZ9OrVq6nvp7tYlNulzeDBy3H7bb9vSt5WccbpP+bGqVfyPwuwXTquN3jwckyZdFmzI3ZL/83n3nHZoEED+c2V5zFl0mWc8tOj53i++8j8LcpjU2ffD/vvt9fs7/E7bp/Iqacc09T3s7g66aTv88EPbrvI1uvo9NN+zNQpV/Dtb30ZKM6x/vbXW7l24gSunTiBdT/QuudYK6wwmCuvOJdhm27ItROLc53ODB68HJOu//VCv/4SSyzB5Zefw9QpV7D33rsBsOKKK/DwQ7fP/vy7+je1cF6aMZPvHHUcr/3731VHUYvrdkWlhvUbf28AfAj4a2ZuCSwF7Np47LMRsfS8XiQilgHOBfq2W7w/cEBmjgVWBtabx0tsB7w7ItZo3N8NWCozRwJPATu35Y2IHvN5LYAvdrLsS5k5pvHnqcz8LPAV4JrGskc6eU77fJtFRL/G/YOAezNzFLBKRGy8EK8313OBBL7UfqWFzFdrO++8Az179mTU6J1YcegQVl+98/pYZ+t1tmz99dbmkl9dwbbjdmHbcbvw5z//peR3tPhZ1NtgnXXW5OCDD+Poo09i4sSpbLTRenxy949ywolnsMOOu/P0U8+y3XZjyn2Ti6FFvV3a/OiY77F073ketjUPbZ/tyNE7MXQBtkv79QYNGsjZZ51An759Sk69+PtvPvfOlu25x8e44KLLGDP2o/Tv35dNNl5/9mv86Jjv0dt9pEuL+tjU2ffD6WecN/t7fNq0WznzzF+W/C7rb8stN2OFIYO55prrFsl6He280w707NmD0WN2ZuiKxbZab721+dWvrmTc+F0YN34X/nxv655jrbPOGhz8jcM5+piTmXjtFDbaaO7Lj0GDBnLWWSfQ9x0c8w888DNMv/NPjB6zMx/ccVv69evLZsM24uhjTpr9+T/33POL4q20vJ49e/DjI779jraTypWZpf2pQnctKr0eEe8C3gB+C9zQWD4NGNa4/Wdgj/m8ziyKQtCMtgWZ+Z3MvL9x913Ac/N4/nbAKcD27e5f07h9CfBs4/YSwOrAY/PJ81xEbDOfdRbGSOACYGwn+c4AXl+I1+rsubOA/hGx+sIGi4j9IuKOiLjjrbdeWdinl2L0qOFMuPQ3AEyechNbjhi2wOt1tmzzzTdhp512YMrkyznv3JPp2bNnOW9kMbaot8GkSTdy623T2WqrzRk2bENuueVOTjv9XK6/vmisuNzy7+LZZ/5VwjtbvC3q7QIwZsyWvPLKqzz91DMlvIPuafSo4Vy6gNul43qzZs1i9z0OYOaMmaXl7S7+m8+9s2X/ev4F1lxjNQYOHMDK716Rvz/+DwC2dh+Zr0V9bJrX98OKK67A4CHLM/2ue5r5lhY7SyyxBKf97Ec8+tjjfOhD4xdqvd69l+bii07n+usu5cQTj+ryuaNGD+fSX18NwJTJNzNixDA233xjdtppeyZPuoxzz2ntc6xJk6ZxW9u5zqbFuU5Hs2bNYo89DmDGjJdnLxs8eDmuuup8pk65gkO+cWCXr9/+uPWHW+5gk002YPPNN+bz++/NDVOv5NhjD130b6pF9evbl/79+s5/RanJuuvsb3dTFIPuBlYE2qoSrwIDGrdPAQ4B/q+rF8nMGQCd9XCLiN0oWuY82dlzG61/3gWcCRwDnAQMAZ5vvPb0dq/9JLAtMPdRfU4/Ab4MvNBu2ckR8RLwbGbuMp/nt8+3BvA4cBWwQ+Pv9vmuX9DXapjruY33diIdWistiMw8g6I4Ra8lV6rFcPmnnnIMa6zxvtn3R48ewdnnXAzAjBkzWW2193T6vL59+/Dkk0/NsV5nyyZPmsY223yMp556hpNO/D477DCWq6++trlvajHT7G3QZtddPswbb7zJrFmzZi/bYvNNWGbQQG69bfoifleLv2Zvl169evHd73yFj318X3596VnNfTPdyKmnHMOaXWyXmTNmsvo8tss/Gtugbb2ZM1/udF3NbVF+7p0tu/iSK9hxh2340hf34S8P/I0XXnhp9j7y0Y/vy2XuI7OV9Z3R2ffDAQd8mtNPP28Rv6PFzymnHM0a719t9v2pU2/m/vv/ynHH/YwDv/AZVl55JU499ey5nrfnnh+fa72ePXtw770PcORRx/OrS37OeuuuzecP2HuO158y5Sb69unDk//4JwAzZhbbatLkaWyz7cd56qlnOPHEozzHAnbZ5UO88eac5zptOjvmH3LIF5kw4SrOP38C0278DWeedQE/P+M4BgwYMHudiy+5gj7tjlszZrzMkMHL8bvfT+b7PziBl19+hSuuOJf11l2be/58/1z/htRdvVXRWEdl6a5FpenAp4GLgH6NP1B0Y5sBLEPR/ewvwBhgysK8eES8DziYohDUlbEURaWfUnRvW6rxb/drvMbO7XLd1S7vwHm85nTgq438bZm/lJnTFiZ/w/bA+4FvAcs3lrXPdwDwQGZOWsDXm+u5jeVXAocCb76DjLXyhQO/Ocf94487nN5LF90M+vXrS48enTf8e/nlV+Zar7Nlf7rnfl5/vWgc9sADf2P11d/X6eu1smZvgzZfPug7HHbYN/jgB8cxYcJVLLPMIE444Uh23e1zzXhbi71mb5dDDjmQn512Li+9NKPT11Hn5rVd+i7gdpnXeurcovzcO1t21JHf4gsHfouZM1/mKwftx6f33o0VVlieU91H5lLGd0Zn3w8RwZjRI/jf/z167hdvMQce+K057p9wwlGcedYFPP30s1x40WUccfg3Oy0qbbjhunOt9/QzzzJ8i00ZNWo4gwYNYMWVVpjr9QGOO+5wlu7dGyhacvToEdwzxznWQ112fWwlBx30XQ479GA+uOO2s1vhzcsaa6zGFptvwl6f2pW+fXuz4tAV+NjH951rvR13GEvv3kszY8ZM+vXrwysvv8If/nBHh3Pc91pUkrqR7nqmOJ2im9t0ilY4YxrLRwK3tVvvJ8DohXnhxjhLFwH7ZOZL81h1O+DLmTmGolvYSOAmYFzj8XHAi+3ybkDRJW9+Tmq81n9rO2DXzNwKeKbRRa19vv/H293zFkSnz83MWcA5wMaLIHOtTL/rHkZsWTSdX3/9dXjssccXeL3Olp1zzkmsv/469OjRg5122oE//em+ct7IYmxRb4ODD/4Ce+75cQAGDRzAiy8WLQAuuvA0vvPdH/L3v/+jhHe1+FvU22WbsSM54PN7c921E9hggw9w+mnHlvNGupnpd93Dlo3PdoP11+HReWyXBVlPC+a/+dw7W9anT2/WW3ctevTowWabbURmss3YkXzh83tzvfvIPC3qY1NX3w9bbbU5t912V5PfzeLpoYce4b3vXQWATTbegMf+/sQCr/fggw9x0slnMm78Lhx62I94vIvv5Lum/2n2flNsqyc45+wTWX+9tYtzrA9v39LnWAd//QvsucfHABg4aCAvLmAx+sEHH+I73/0h48bvwrHHnsrzL7zY6XrTp9/DliM2A2D99Yrj1jVXX8AKKwymd++lGT9uNPfe17pjWqk1dfcxlaKqf7hZIuIc4DDg9xSFkx8DPSm6Z/0V2JeiS9qZmTktIqYAUzLzsHm85pRGcYiIOAbYi7db4hyamVM7ec6DwBaZ+XxEfAb4APC/wC8oBvh+BNgbmAR8GLge+AYwprMsjZx7UnSV+ytwJEWxbAOgrbh1aGZOjYgxwJ6NQbE7ez9LAX8DVs3MtyLicIoi0MUUBaBlgRsz85uN9ef5eo11luv43Ij4W2auHhEDgCeAD2fmlAV5vfbq0v2to/79+zFl8uVMmjSN7bbfmq22+hArrbQCn/jERzj00B/Nc73MnGvZyiuvyPnn/ZSI4DdXX8v3vueMMfOzqLdBjx49uOjC01hqqSW5994H+NKX/4f999uLI4/85uwT0NPPOJ8JE66q6i0vFhb1dpnRbhyf666dwLbjFrinbylqeYDqRNvnPbnx2W7Z2C67f+IjfK+T7dJ+vbZtcP21E9imZp9/3f03n3vb/tB+2ZprrMaZZx7Pqqu8m1tuuZOP7bIvr7zy6uzXqcs26nJq3Aot6mPT7p/4SKffD0ce+S3uvPNurrjit1W91TnMY6Li0vXr15efn3EcgwcvR69evfjE7vszcEB/PvGJnTn0sGPnud6LL77EmT8/niFDlmfGzJfZa68vdtpNq3//fkyedBmTJk9j++22ZquRH2bld6/IeY1zrKuvnjjHvtdqBg0ayIUX/oylllySe+97gNN+di677bbTHJ9/m2snTmDc+OJ4MmTI8px++o8ZNHAAjz76OPt+9quddp1bZZWVuOrK87h+0jSGb7EJW438MCNHbsFPT/4Br7/+BmeedQE/+9k5zX6bC+SVf9ww/5VUil7Lva8+B6omWG7AGqWdLj4348HSP8tuV1RS91PXohIUX8zbbjuKG2+8haef7rphV2frLehzNW9ug3pqpe1S2wNUJ/6b7aJ3rpX2hzZ1vTpoyW1Ro6JSWQYNGsi224zkxmm3LlbbqrsYOnQIW44YxsRrp87xw1DdWFSqj+5eVFq2//tLO118fuZfLSpVpdESqL2XMnOnBXjeChQtfNp7IDP3f4c5FvXrbQic0GHx1Mx8R1MvLOrXWxB1LipJkgcoaW7d+upgMdOKRSVpQVhUqg+LSouORSWpExaVJNWZByhpbt366mAxY1FJ6pxFpfqwqLToVFFU6q6zv0mSJEmSJFWquzfk6a6zv0mSJEmSJKmJbKkkSZIkSZLUBG9188ESbKkkSZIkSZKkhWZLJUmSJEmSpCZwTCVJkiRJkiSpA1sqSZIkSZIkNcFbtlSSJEmSJEmS5mRLJUmSJEmSpCZIZ3+TJEmSJEmS5mRLJUmSJEmSpCZwTCVJkiRJkiSpA1sqSZIkSZIkNUHaUkmSJEmSJEmaky2VJEmSJEmSmsDZ3yRJkiRJkrTYi4izIuLmiPjuf7NOG4tKkiRJkiRJ3VxEfBTomZkjgBUj4v3vZJ32LCpJkiRJkiQ1QWaW9ici9ouIO9r92a9DnDHArxq3JwFbdRJ5QdaZzTGVJEmSJEmSFnOZeQZwxjxW6Qv8o3F7BrD6O1xnNotKkiRJkiRJTZBZq4G6XwZ6N273o/Peawuyzmx2f5MkSZIkSer+7uTt7mwbAI++w3Vms6WSJEmSJElSE9SqnRJcAdwYESsCOwCfiIijMvO781hni3m9YNSsKZbULUXEfo3+raqY26Je3B714baoD7dFfbgt6sNtUS9uj/pwW2hhRcQywDjghsx86p2uM3tdi0pS80XEHZm5adU55LaoG7dHfbgt6sNtUR9ui/pwW9SL26M+3BaqmmMqSZIkSZIkaaFZVJIkSZIkSdJCs6gklcN+zvXhtqgXt0d9uC3qw21RH26L+nBb1Ivboz7cFqqUYypJkiRJkiRpodlSSZIkSZIkSQvNopIkSZIkSZIWmkUlSZIkSZIkLTSLSpIkSZIkSVpoS1QdQOqOImJUV49l5g1lZml1EbFqZj5WdQ4VImJl4EDgP8AJmflCRKwGfDUzv1htutbiviHNzf2iPiJiqcz8T9U5BBHxi84WA5mZ+5Sdp9V5nFLdWFSSmuPwdrcT6A9sAswABlURqIWdDYytOoRmuxD4OcU+cVlEvAgsC5xQYaZW5b5RcxHxf5m5Y9U5Woz7RX38FrdFXfQB1gfeAO4G7gSmAxY2quFxSrUSmVl1BqnbarTA+BIwBjgHOCszZ1aZqdVExJPA1Z09lpn7lRyn5UXE1Mwc3bh9N/C5zLyt4lgtyX2j/iLi9swcVnWOVuJ+UR8R8TBwZmePZeYPSo4jICKWBXYDPg0MAx7IzLUrDdWCPE6pbmypJDVBRIyjKCb1B06k6NpjBbcaTwMXVB1Cs60cEf9D0Wy+P7BtRGwLXiRUwH2j/vzeKJ/7RX38B3iC4vtCFYqISyhaKr1O0VLpIuAQ4NEKY7Uyj1OqFYtKUnOcQXExMAv4EfCjiGjre75Gpclazx2ZObXqEJrt8C5uq3zuGzUREdcydwEpAL8vyud+UR9/y8zzqg4hAF4Fbml3f4PGnwQcU6l8HqdUK3Z/k5okIt4NbAwMoBhL6Y7MfLLaVK0nIvbq6jFPVqsVEcsB2wHbAyMz8z3VJmot7hv1ERGrdvHQgMy8p9QwLS4iegDbAK9l5rSq87S6xvZYi2I8yucpult58VKyiNgpM6+sOocKTgikurGoJDVBRHwZOIhiIMPXgL7ARhSzXZ1cZbZWExH/BIYA9/H2wJIvgBfOVYiIkRSFpHGNRYOBfYGbnOWnXO4b9RQRm1AUWrcHhti6tVwRcQ7FYMSDgPsy89BKA7WwiNgCuAx4iaKgtCywNLBzZt5dZbZWExGTMtOBoWsiIia3uzvHhECZOaiSUGppFpWkJoiIW4BRmfl6u2VLAVMzc4vqkrWmiBhM0Ux7I+CjFIWMOzNzl0qDtaCIuAtYB/gZxUxwP8rMMZWGamHuG/UQEZ+iKLZuCtwDrAt8MDMfrjRYC4qIaZm5VaPL+tTM7LJFgJorIqYBB2fmLe2WDQOOzsxtqkvWeiLiVd4e36rt4tFhHSrmhECqC8dUkprjDWDriLgxM1+NiL4UB/w3q43VeiLiAIrBJd9LMcbVLRQtMu6sMlerysyNImIIxQX0QcD7I+IKYFJmnlRpuBbjvlErHwVGAlcBvwOGWlCqTETEUIoL5p7tbmMX9tK92b6gBJCZtze6xKlct2bm1lWHUMEJgVQ3tlSSmiAi1qQ4yG9GccCfAdwKfCUzH6wyW6uJiEcaN/8OvNXuobQpd/UarQE2AcY7+1u53DfqJSKWAEZQFFzHAz2BKZn5tUqDtZhGt5I5WmLwdosM94sSRcTDFD/GdWwd0yMzV6ssWAuKiAMz85Sqc6jQ+P5umxAI5jxO2XJMpbOoJDVBuwFwO06Dm45VUr1Gc+HtPUEqX0Qckpk/atzeITN/27h9VmbuW206uW/UR0QsT1FsddrokkXERsDrmXlvRGwNPJOZ91adS3OKiEMz01lEayAiLs/Mj1Sdo5V4nFKd2HxUao5jKPo2fwMYSzHgpxXcikREn4j4fxHx04i4H/gFsFzVuVrU9u1uf6Pd7feWHUTuG3UREetHxMURcWZEbBIRf6MYW0kli4hjgEMoxhcDWBM4JiJsSVk/o6sOoNkGVR2glXicUt04ppLUBJk5tMMAuLvTGAAXsKVSiSJiEjAMuAD4DbB+ZnoiqpbnvlErpwAHA8sDv6eY0v5F4EqK7aPybN5+8oDMPA04rTFotOqlY2twVccfTsvlcUq1YlFJagIHwK2VEylax2wFrAKsEBEbOB1xZTaMiIkUFwPtb29QbayW5L5RH69n5q0AEXFf2zaIiBeqjdWSno2IQ4FrKKax7weMA9wW9WMhQ63K45RqxTGVpCZwANx6ioj3U1xEbwesl5mrVhyp5UTEe+jiQiAzHys3jdq4b1QrIp4ErqYosH6w3e0dM3OlKrO1mojoQzEz5XbAEIqJNqYBP8jMf1WZTXOKiMnOSFYPbotyeZxS3VhUkirg4JLVi4glM/N1t0W5IuJ24HjgYqe/rSf3jfJFRJfdDjNzakRs3taSSdVyvyhXRKxI0RV0LWDptuWZebMtK8sTEd/r6rHMPKLMLJo/j1Mqm93fpGo4bknFMvP1xk23Rbl2BA4EboqIs4HZM5Vk5s2VpdJs7hvly8yp81nlhxSTPqh67hcliYjDKD7vfYGzgN9RdJVeDtjMglKp2o9flcDWFNtmCmBRqX48TqlUFpWkaji4ZH24LUqUmc82ikkrAJ/j7aJSAhaV6sV9oz7cFvXhtijP2MwcBRAR12fmtxu351eE1SKWmYdHxJLAnsD+wP3Axpn5x0qDqSsep1Qqi0pSNez2Ux9uixJFxMUUUw//KDM/X3EczZv7Rn24LerDbVGeGRHxFYrWMKdGxAYUY7/9u8pQragxVf2+wJ+BY4CngD4RMcJWxrXkcUqlsqgkVcNfEOrDbVGu4zPztq4edOyYWnHfqA+3RX24LcqzJ/B14GRgeeAlisGIP1llqBY1FPi/xu0PtVtuK+N68jilUjlQt9QkEdEfeDUzZ7Vb9v7M/KuDS5YnIkZ19Vhm3uC2qJeImOQMieWIiL26eiwzz3PfqI+I+GZmHlN1jlbh4ND1EBGDM/OZTpaPzMwbq8jUqiJiKPAl4FXgxMycWXGklhYRq85rxlyPUyqbRSWpCRqzZOwK9AQupJjic5YXzOWLiMnt7ibQH9gEmJGZgyoJpS45LXF5IuKfFFMR3wfcCUwHXoCiqFRhtJYTEcOAo4CDKWZHXAoYDDybmSOrzNZqOgwO/WvaDQ6dmZtVGK3ltD9niohLMnO3jstVjoi4FjgHWAbYPDM/VW2i1uY+oLqx+5vUHGMzc92I6AV8FbguImyuXYG2AkVErEbxK9sY4GsUM8mofvyloySZOTQiBlNcMG8E7E5RyLgTsKhUrp8Ae2bmoxGxRGaOarSWObPqYC3IwaHro30XnsFdLFc5lszMCwAi4uNVhxFrRcQZnT2QmfuVHUayqCQ1xxIR8a7M/Bfwo8bJ6G8ofuFRiSJiHEUxqT9wIvDVtIlmnXmxUJKIOABYH3gvMAu4haK10p1V5mpRSwAvN263XbD1oBjHROVycOj6GBgRwym+FwZGxIjG7QHVxmpJy0fE7hTHpcHtfyjNzAuri9WyngYuqDqE1Mbub1ITRMSGwP6ZeUC7Ze8Bjs7MT1SVqxVFxCMUrV/axraafdDLzDUqCaUuOXZMeRr7BsDfgbfaPZQ2qy9XRIwHTgFeAWZQXDQvQ/E98rsqs7WaiBhEMTj0GOYcHPoHjR+KVJKI+CWwDfA6MJniONUT+Fhm9qsyW6uJiCOBXYBrgIEUx6kdgAmZ+b0qs7WiiPh5Zn6u6hxSG4tKkrq9iFiJzgddvamqTK2q0XLsSOA/wEHAvcBewAGZuWmV2TS7m+j2mXlK1VlaSUQsRXHh/D7eLmQ8lJmvVxqsBTk4dH1ExK8oWvD1o5jC/mGKsa4mZeZBVWZrNRFxDsVYb9MoBuvuC4ykmJDmMxVGa1mNH7Bfz8z7ImIMxRh891YaSi3L7m+SurWIOJTiF+d9gJ9TDLq6IbAc4KCr5fsh8FGKlhiXAW80/v5glaFaVUT0AcZSdO/ZBngGmFRpqNb0V+DPwO+B32XmAxXnaWUXU+wTcwwODRzetlylWSkztwSIiEeBU4GRmflilaFa1GqdTBrw04iYVkmaFhcRxwCrAqdTTLaxFnBwRNzTNg6cVCaLSlITRMQVzN3nP7BbSRW2aTfo6qTM/J/GbQddrcbMzPw7QEQ8S7F9HKukAhExCRhGMS7Db4D1M3N0talaU2au0vjVeVvghIhYBbgR+H1mXl5puNbj4ND10bvdmErPU7SSWSciyMybq43Wcp6NiKOBGyi66faj+MFurlZ9KsXmmTmm7U5mngacZpFPVbGoJDXHt4EfUfT7t/tCtRx0tV42iIiJFBcJawNXRURbwXV8tdFazokU+8JWwCrAChGxQWbeXW2s1pSZf4yIh4GHKFru7UHRHc6iUrkcHLo+7gb2a3e7bQyZBCwqletTFJOefI5i4pMZwK3AYRVmamXPNlriX0NRcO0HjANeqDSVWpZjKklNEhHvA2Zk5nNVZ2ll8xh09fuZ+Xx1yVpTRKxKu8HS2xYDZOZj5ScSQES8n6LAtB2wXmauWnGklhIR36foftgfuBaYCEzOzNcqDdaCHBxaUt01uq4fRPGdPYSiyOe5rSpjUUmqQET8rP3McCpXYzDi7TLz1KqztJqI+CtzFpXaZuY7IjMvriZV64qIyMaJQGOGyl7AY7awLFdE7E/R1e3RDsv7Z+bMalK1JgeHlrQ48txWVbL7m1SNNasO0EraDUa8HcWYJQ5GXJHMfH/HZRHRj6JFgEWlEkXEgRRdGraIiE8DBwNPALcD/1thtJaTmae33Y6ITShajW1P8Qv0GlXlalEODi2p9jy3VZ1YVJLUrXUyGPEGDkZcO7NwENwq7AkMb3QR/Q4wLDNfjAinTS9ZRHyK4sJgU+AeYF3gg5n5cKXBWpODQ0uqNc9tVTcWlSR1dw5GXCOddH+jcf8HFcRpda8Bm1AMvPqLRkFpZWDJamO1pI8CI4GrgN8BQy0oVcbBoSXVnee2qhXHVJIqEBGTM3PrqnO0GgcjroeIWB5YMjP/ERFrAU9npjOWlCwi1qHo5vYE8C1gA+AU4JDMtLVSySJiCWAExfFpPMXg0FMy82uVBpMk1ZbntqoDi0pSE0TE5pl56zweH5KZT5eZSXOKiCUz8/WIODQzD686T6uIiC8DHwd+kpmXR8QRFCdD52bmKdWmU3vuG9VqFF/HZ+YF8/tOkSTJc1tVxaKS1AQRMSkzx1adQ/PntipXRNyQmaM6LOsBTMvMERXFUifcN+rDbSFJWlB+Z6hsjqkkNceGETGxw7IAMjPHVxFIXXKA6HK9GhF7Af9HMQhuP2Ac8O9KU6kz7hv14baQJC0ovzNUKotKUnM8wNuDe6rebK5Zrr2AI4EjgMHADOAmYN8qQ6lT7hv14baQJC0ovzNUKotKUnNcl5mPVR1CC8Rfc0qUmc8A+3f1eETsn5mnlxhJXXPfqA+3hSRpQfmdoVL1qDqA1E19LyJ2jogtASJij4jYsepQrSgi5jcLxlfKyKEFtlvVAVqF+8Zi5XdVB5Ak1YPf36obi0pSc/wC2AJ4pXH/ZWDHiDi3ukgt6+x5PZiZd5cVRAvEX9fK475RExFxe0TsHhGd/v+fmceUnUmSVFt+f6tWnP1NaoLOZria13I1T0Q8CVzd2WOZuV/JcTQfzlhSHveN+oiI5YEDgfEUFwv3tj2WmTdXlUuSVD9+f6tuHFNJao77I+Js4BrmnOHqiUpTtaangQuqDqEFZkul8rhv1ERmPtv4zliBYpKHtqJSAhaVJEnt+f2tWrGoJDXHARRjw/w/YAhvz3B1cJWhWtQdmTm16hAqRMRSmfmfeaxydGlh5L5RExFxMTAI+FFmfr7iOJKkevP7W7Vi9zepCSKiJ7At8HpmTm4sC+BjmXlppeFaUERENg52EfEeoFdm/rXaVK3J7m31EREDMnNGJ8s3cDyGckXEZpl5WyfL+2fmzCoySZLqKSIGN2bT7bh8ZGbeWEUmtTaLSlITRMQlFIN09wOeAh4CPgtcn5lfqTBay4mIA4FPZeYWEfFpitZiT1D8yvPdSsO1oIh4GDizs8cy8wclx2lpEXEPcClwQma+FBEbAocBS2fm9lVma2URsQmwfePPkMxco+JIkqQaaf8DXURckpm7dVwulcnub1JzrJyZIxqtkx4BTgVGZuaL1cZqSXsCwyNiEPAdYFhmvhgR/pJTjf8A/6g6hADYCPgEMDUinqcohP9vZv6x0lQtKCI+BWwHbArcA6wLfDAzH640mCSpjtqPPzm4shRSg0UlqTmWjojhFAf954FpwDoR4Uw+5XsN2IRi8NtfNApKKwNLVhurZf0tM8+tOoQAGAXsSjEo9CRgX2C3iHiys2b1aqqPAiOBq4DfAUMtKEmSujAwIkZQXGfMcbvaWGpVdn+TmqAxi09nMjP3KTVMi4uIdYD/pejy9i1gA+AU4BD7nZcvIkZ19Vhm3lBmllYXEb8EjsrMvzTuB8UEA9/IzE0qDdeCImIJYARFi6XxQE9gSmZ+rdJgkqRaaVxntF3ER+N24HWGKmJRSapARFyemR+pOkcrcDDDeomINylOfO4H7gQepnFilJlHVBit5UTEkMx8upPloyzwVS8ilgfGZ6bTRkuSZouIvWlXSGr/WGaeV0kotbQeVQeQWtSgqgO0kIvbbjQGUG9zeAVZBP2B4cCJwEyKLlhjGstVrovabnTYNw4rP0pri4gVIuLSiHgmIl6PiKeA44HfVp1NklQ7Y4CtG3+PAfYBzqFomS+VzqKSVA2bCJanq8EMo+OKKsUYihOh0cBqFGOOTWn8UbncN+rjdOAOYH2gL7AhcDfFJA+SJM2WmZ/JzM8A5wPLAm9RjM3nbKGqhAN1S+ruBjYGTe8BDGh/u9pYLeuaxt/3A88AS/P2r23XdPEcNYf7Rn0sk5lHt7v/FPDjiJhSUR5JUk1FxOeALwLPAT8BpjceGgo8WVUutS7HVJIqEBGTM3PrqnO0gnkMmk7jVx5VLCL6AFtnpkWlErlv1EdEPE/RUmmOxcDGmfmuCiJJkmoqIiZ3WNR+oO6xFURSi7OoJDVBROyUmVdWnUMQEZtn5q1V59CcImJdilmutgdWBW6ykFEvEfGzzDyg6hytICJW7eqxzHzM7xRJ0oLy+1tls/ub1BwHAV4A1MMPAX+1qYmI+AVFd7e7gWuBfpnpGAD1tGbVAVpFZj42n1X8TpEkLSi/v1Uqi0pSc2wREQ92WNbWLNUL6HJtGBETOyxr2xbjqwjU4u4GVqAYpPtJoG9EDMrMFytNJdWbg6dLkqRasqgkNcetjplUGw8An6s6hAqZeSJwYkQsBYwCXgNuiIg3M3PjatNJteVYBZIkqZYsKknNcWnVATTbdQvQtUQly8z/UHR/uxYgIlZq/L1/Zp5eZTbNZuuY+nBbSJIWlN8ZKpUDdUtNEBGH0sUvy5l5RMlx1E5E9AJGAttn5iFV59GcImKSM5eUpzFg+nOZ+VS7ZSMz88aIGJKZT1cYTw0RsXdmnlt1DklS9SLie109lplH+P2tstlSSWqetl8JkmJg4tHAFMCiUskiYnWKWca2B7YArgE6TseqevDXtZJExGnAKsC7IuLPwEGZ+TJwODDWE9LyRMRbtJsSuv1jmdnTgpIkqZ0dgXWBx4A7gOnAn4E3APz+VtksKklNkJmHR8SSwJ7A/sD9wMaZ+cdKg7WgiHgYWAk4Bfgm8NPM3LvaVJoHm8+WZ/XM3BYgInYDro+IfSrO1JIyswdARPQAPgx8CZgJ/KTKXJKk+snMLSIigNWBbYFPAxtQTIayeYXR1KLs/iY1QUT8ANiX4leDU4DZXUsy8+aqcrWiiNiAooXSeOBdwLIUF2xTnXGsfiJisoPclyMiJgP7Z+aDjfvvAy4EVs7MlSoN12IiYiDwWWAvilaUJ2Xmw9WmkiTVUUQcA6xP0UDkSeCuxp87Gy2OpVJZVJKaICLO7rBodreGzLQlQEUioi8wFtgO2DYz16o4UsuJiB6Z+VYny4dk5tMRsV1m/r6KbK0mIlamKCp9t92yZYBvZua3qkvWeiLiZaAPRTeGl5jzO2N8ldkkSfXS+FGovdkX9I5LqSpYVJKaoFG82A94MDOviYiDgVeBszPztWrTCSAiPpKZl1edo9VExN3A8cD5mflWRKwAfAsYlZkbV5tOqkZErNq4OaDxZ0bjD85eKUlqr9H1bWfg2cycFhF7AC8Av00v7lWBHlUHkLqp8yiKSPc27k+l+BX6osoSqaNvVx2gRW0NrAxMi4hfABdTDGC/SZWhpIqtDEwErqfogjgZ+F1juSRJ7f0C2Axo6+r2MsXg3R17SkilcKBuqTlWyMzT2+5k5u3A7RHxkQozaU7OMlaNZYGhwCvAP4B1gCFAL+D1CnO1nIi4gaLYPaP9YoouVzafL9dxwIcz84G2BRGxDsUFgoOuSpLaWy0zP9N2JzOvBK6MiBsrzKQWZlFJao7rI2IS8H/A80A/YBzFeBkqUUSM6Gwx0LfsLAKKX9eOyMzrACLiaOCLwJ3AelUGa0G7AOcAu2XmjPmsq+YK4JkOy57BFuWSpLnd3xi/9RrmvM54vNJUalmOqSQ1SUQMpxgQeghFS4CbMvOqalO1nk4GTZ+t/a88Kk9ELE3RGuY/7Zatkpl/rzBWS4qIQcCbzhZTrUYr1lOAJyha8fUHVgS+kJlXVBhNklQzEdED2I05rzOmAWe0P7eSymJRSapARFyemXaFq4GI+FlmHlB1jlYREYcA+zbuHgbcBxwMrJmZm1WVS3PzOFWeiFgFWBroTVFQmgH8G/i3xVZJ0rxERC9gFLBdZh5SdR61Hru/SdUYVHUAzbZm1QFazG7A2hQX0I9QdHs7vq07nGplUNUBWsijFFNC3w/cDbT/pXmfKgJJkuorIlYHtm/82YKiK9zkSkOpZVlUkqphE0G1qpcy8y3g1Yi4NzN3rDqQuuRxqjxrAxs0/qwDDASeBqZXGUqSVD8R8TCwEkW36W8CP83MvatNpVZmUUmSVKb3RMSDFAMT92x3OzNzjWqjSZXpwZwzUgbOUClJ6txHKFoojQcuAJaNiJ2AqZn5YpXB1JosKknV8GKhPtwW5RqbmY9WHUILxH2jPPdStAz7C/An4EWKz3+TCjNJkmooM++m6Cp9TET0BcZSDNp9DLBWldnUmhyoW2qCiPheV49l5hFlZml1EdET2BZ4PTMnN5YF8LHMvDQihmTm05WGbCERMSkzx1adQx6n6iQiVu3qscx8LCJ2yswry8wkSVo8OQmNymZLJak52v/Cn8DWwGhgCuDFWrkupJiiu19j2u6HgM8C1wOXWlAq3VoRcUZnD2TmfmWHaXEep2oiMx+bzyoHARaVJEkLwkloVCqLSlITZObhEbEksCewP8WMPhtn5h8rDdaaVs7MEY3WSY8ApwIj7XNemacp+v+rYh6nFit2RZQkSbVkUUlqgoj4AbAv8GeK/s1PAX0iYkRm3lxpuNazdEQMp7goex6YBqwTEbgtKnFHZk6tOoQ8Ti1mHKtAkiTVkmMqSU0QEWe3uzvHTpaZ+5Qcp6U1tkXy9i/9s7eH26J+IuLQzDy86hytoIvjVNtMfO4bNRIRkzNz66pzSJLqz+8Mlc2WSlJz7Ad8nGJsko8CgxrLreKWbz9gF2AybovFweiqA7SQzwK7AsOAAcAM4FZgQpWh1Klzqg4gSaqPiFgXeC4zn2q3bGRm3gh8orpkakU9qg4gdVMXUQyS9wbFeCW3N277q0H5LgLWwG2xuHDsmPKcCXyYYvD6m4CHKQqvZ1YZqhVFxIiIuKRxe9OIuD8i7o2InQEy89xKA0qSaiMiTgN+BFwZEWdFRL/GQ4cDOAmNymZLJak5hmTmxwEi4pTMnAhMbLtAUKncFosXW5CV5/2ZuVWHZT+NiJsqSdPafgLsERFLAOdS/Mr8V+A64IoKc0mS6mf1zNwWICJ2A66PCLutqzIWlaTmmB4RV1B0f5sREQcB2wF3VRmqRXW1Lf5YYSZ1zZZK5XkmIn4I3Ai8AvQDxlAM2K1yvUExO+W+wLTMvLsxY2XPamNJkmqoZ0SskZkPZuYlEXE7cCGwctXB1JosKklNkJkHRcQ2FBdoawJLAr8DRlaZqxV12BZrAS8Bp2XmVZUGa1ERsRfw+3k0zf5KiXFa3SyKz3tNoD8wE9gB+E2FmVrVicC9FNtgx4j4AHApcGqlqSRJdbQXsD/wXYDMfDgidgC+WWkqtSxnf5OaICKWBEZRXKBtA6wKnARMzswpFUZreRGxErA9sF1m7lp1nlYTEccBY4G3gInA7ylaZrxZabAWFBEXA+tTtJK5G7gTmA48lpl/rzJbK4qI3sCbmflG4zukV2a+UnUuSZKkebGoJDVBRMykaJ10CvB9YEJmjq02VWtqV+DbHtgWC3y1EBHLUxSXtgN2oigs7VRtqtYUEcsCuwGfppgJ7oHMXLvSUC0mIr4IHEAxgcr/ZOblFUeSJElaIBaVpCaIiGWA8RQXzMOB5SiKS5My809VZms1nRT4Ls1MZ36rUESsRVHgGwesDdxC0SXugkqDtZjGbGPrA69TtFSaTjHu26OZ+ViV2VpNRNwKbAH0Aa7JzDHVJpIk1VVE3EDxfTGj/WIg/RFbVbCoJJUgItajKDCNz8zxVedpJRb46iUiHgEepej69rvMdPD6ikTE2V08lJnpLDIliohJbRcC7W9LktRRRAwBzgF2y8wZ81ldajqLSpJaigW+akVEP+DdwH8y85F2yzfNzDuqSyZVJyJeBZ6g+KV5pXa3MzPXqDKbJKl+ImIQxTh8L1edRbKoJKmlRcTlmfmRqnO0iog4EVgPWAq4HPgr8DXgebeDWl2j6DoAeDEzX606jyRp8eO5rcq2RNUBJKlig6oO0GKGZ+ZmEbEERWuMy4DPZeaDFeeSKhMRawIXAGsBLwIDI+KPwB7OxCdJWkiDqg6g1tKj6gCSVDGba5brzYgYCgwGHgaOAl6OiBWrjSVV6hTgx0D/zHw3RWulE4FTK00lSVoceW6rUtlSSZJUpv9QtMhou/1LGmPHAA5OrFbVKzMvbruTxdgEl0bEFyvMJEmSNF8WlSS1uqg6QIv5f8AewKvARZk5q+I8Uh0Mi4gHeft4lI3btuCTJC0sz21VKgfqltStRcT3unosM48oM4sgIq4EbgWWBXpn5oEVR5JqLyL2z8zTq84hSaqe57aqG8dUktTdRbs/AFsDhwFjKsrT6pbJzB9k5sHAulWHkRYTu1UdQJJUG57bqlbs/iapW8vMwyNiSWBPYH/gfmDjzPxjpcFaV7+IGE5xItQvIka0PZCZN1cXS6o1uzJIkgDPbVU/dn+T1K1FxA+AfYE/U8yw9FTbYxYxyhcRZ3fxUGbmPqWGkRYTETEpMx3IXpLkua1qx6KSpG6tQxFjjgOeRYz6iYifZeYBVeeQ6iQiJmfm1lXnkCRVr4tz28Af6FQRu79J6u72Az4OTAE+CgxqLLeiXk9rVh1AqkJELE1xQfCfdsuGZuY/gaOrSyZJqpnPArsCw4ABwAyKSVAmVBlKrcuBuiV1dxdRFCreoOh7fnvjtr/6S6qFiPgmcDfwp4jYPSI2iIjzgSsBMvP3lQaUJNXJmcCHgYeAm4CHKX44PbPKUGpdtlSS1N0NycyPA0TEKZk5EZgYETtXG0uSZtsVWBtYGngEuBM4PjOvqzSVJKmO3p+ZW3VY9tOIuKmSNGp5FpUkdXfTI+IKiu5vMyLiIGA74K4qQ6lLznKlVvRSZr4FvBoR92XmjlUHkiTV1jMR8UPgRuAVoB8whnYDdktlcqBuSd1eRGxD8WW7HLAkRTeTkZm5S5W5WlVEbMGc4wDclpm3Nh4bkplPV5lPKltEPELRLReKH/ze5O1BV9eoLJgkqXYiYgLw/4DfAv2BmcAOwG8yc9cqs6k12VJJUrcWEUtSXJz1AYYDqwJPUEzBqpJFxDHAVsDNwKsU2+OTETE1M79lQUmtKDPfGxHvBjbm7WLrHZn5ZLXJJEk1NIuiq/RqFD+U3gn8BHisylBqXbZUktStRcRMitZJpwDfByZk5thqU7WuiLgpM7fsZPnNmTmiikxS1SLiy8CXgenAa0BfYCPghMw8ucpskqR6iohlgd2AT1O0AH8gM9euNJRaki2VJHV3qwDjKcZRmgYsFxFfASZl5p+qDNaiXomI/Zl7HIBXqgwlVeyTwDqZ+XrbgohYCpgKWFSSJM0WEZcA6wOvU7RUugg4BHi0wlhqYbZUktRSImI9igLT+MwcX3WeVhMRQ4DDgc0pxgGYAdwCHG7XN7WqiLgROAq4MTNfjYi+FMXWb3cyw48kqYVFxNldPJSZuU+pYSQsKkmSShQRm7cNyi2pEBFrAidSFFv7URRbbwW+mpkPVJlNkiRpXiwqSZJKExGTHNNKmltEbAS8npn3RsTWwDOZeW/VuSRJkubFMZUkSWXaMCImdljWNnW63RHVkhqzIq4CnAHcC6wJfD0i7snMb1caTpIkaR5sqSRJKk1E/AH4RGePZaZT4aolRcSUzBzTyfJpjqkkSZLqzJZKkqQyXWfxSJrLsxFxKHAN8DzFuErjgBcqTSVJkjQftlSSJNVGRPwsMw+oOodUpojoAxxEMTPlEIqBuqcBP8jMf1WZTZIkaV4sKkmSasOBvKW5RcShmXl41TkkSZI66lF1AEmSJM3T6KoDSJIkdcaikiRJUr1F1QEkSZI6Y1FJklQnXjxLc3OsAkmSVEsWlSRJpYqINSNilcbtPSLi8xHRv/HwJyqMJtWVxVZJklRLDtQtSSpNRJwArAkMBB4HlgBeAoZm5g4VRpMqExGjunosM2+IiA0y8+4yM0mSJC0Ii0qSpNJExJTMHBMRPYH7M3ONxvI/ZObwiuNJlYiIye3uJtAf2ASYkZmDKgklSZK0AOz+Jkkq05IRMRQYAjwXEUMjYiXgrYpzSZXJzK0zc2vgs8CfgF7A14CVKw0mSZI0H7ZUkiSVptEio+2LJxq3A4oL66pySVWKiHHAlyhaKJ0IXJmeoEmSpMWARSVJUmkiYo8OixL4Z2ZO7mx9qRVExCMU+8IsOhRd27qISpIk1ZFFJUlSaSLi0E4Wvw9YMjN3LzuPVBcR8W5gY2AAMAO4IzOfrDaVJEnSvFlUkiRVLiKmZuboqnNIVYiILwMHAXcCrwF9gY2AEzLz5CqzSZIkzcsSVQeQJLWOiBjRyeL3AT3LziLVyCeBtTPz9bYFEbEUMBWwqCRJkmrLopIkqUyHAv8CdgWuBF4CngY6jrUktZI3gK0j4sbMfDUi+gJjgDerjSVJkjRvFpUkSWXqCVwD/AHYLDP3qTiPVAefpZj17aKI6E8xptKtgPuHJEmqNcdUkiSVJiKmZOaYjrelVhYRe7Xd7PBQZuZ5ZeeRJElaULZUkiSVaXBEfJLi4rntNgCZeWF1saRKHQMMAe6jGKx7OvBCpYkkSZIWgC2VJEmliYhDu3goM/OIUsNINRIRg4ENKGZ9+ygwGLgzM3epNJgkSdI8WFSSJEmqUEQcAKwPvBeYBfyVorXSnZl5b5XZJEmS5sWikiRJUoUi4pHGzb8Db7V7KDNzbAWRJEmSFohFJUmSpBqLiEMz8/Cqc0iSJHXUo+oAkiRJmqfRVQeQJEnqjEUlSZKkeouqA0iSJHXGopIkSVK9OVaBJEmqJYtKkiRJ9WZLJUmSVEsWlSRJkioWEWtGxHs7LNu0cfMr5SeSJEmaP2d/kyRJqlBEnAisBywFXA78Ffga8HxmfqTKbJIkSfOyRNUBJEmSWtzwzNwsIpYAngAuAz6XmQ9WnEuSJGmeLCpJkiRV682IGEoxdtLDwFEAEbFiZj5ZaTJJkqR5sPubJElShSJiMm/P8BaN2wFkZo6tLJgkSdJ82FJJkiSpWmd2uJ/AP4Ep5UeRJElacLZUkiRJqlBEHNrJ4vcBS2bm7mXnkSRJWlAWlSRJkmooIqZm5uiqc0iSJHXF7m+SJEkViogRnSx+H9Cz7CySJEkLw6KSJElStb4KjAdeB64G9gaeA2ylJEmSas3ub5IkSRWKiMuAicDvM/ORiNiMoqC0ZWbuXGk4SZKkebCoJEmSVKGIuCkzt1zQ5ZIkSXVh9zdJkqRqTYqIScD/Ac8D/YBxwB2VppIkSZoPWypJkiRVLCKGA9sBQ4AZwE2ZeVW1qSRJkubNopIkSZIkSZIWWo+qA0iSJEmSJGnxY1FJkiRJkiRJC82ikiRJkiRJkhaaRSVJkiRJkiQttP8PebQEf2kig0QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr_plt(X_resampled2, list(X_resampled2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9fc1780-48d4-49ea-afb4-36b9fe682679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_csv('cleaned_test1.csv', index_col = 0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08814202-f180-4352-959d-382228a4a0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGN_CNT_RCT_12_MON</th>\n",
       "      <th>ICO_CUR_MON_ACM_TRX_TM</th>\n",
       "      <th>NB_RCT_3_MON_LGN_TMS_AGV</th>\n",
       "      <th>AGN_AGR_LATEST_AGN_AMT</th>\n",
       "      <th>ICO_CUR_MON_ACM_TRX_AMT</th>\n",
       "      <th>PUB_TO_PRV_TRX_AMT_CUR_YEAR</th>\n",
       "      <th>MON_12_EXT_SAM_NM_TRSF_OUT_CNT</th>\n",
       "      <th>MON_12_EXT_SAM_AMT</th>\n",
       "      <th>MON_12_TRX_AMT_MAX_AMT_PCTT</th>\n",
       "      <th>CUR_YEAR_PUB_TO_PRV_TRX_PTY_CNT</th>\n",
       "      <th>...</th>\n",
       "      <th>REG_DT</th>\n",
       "      <th>OPN_TM</th>\n",
       "      <th>HLD_FGN_CCY_ACT_NBR</th>\n",
       "      <th>CAGR_YEARLY_AGV_TRX_CNT</th>\n",
       "      <th>MON_12_EXR_SAM_NET_AMT</th>\n",
       "      <th>ENCASH_CNTR_RATIO</th>\n",
       "      <th>MON_12_ACT_NET_50_UP_CNT_PTY_QTY</th>\n",
       "      <th>AGN_CUR_YEAR_WAGE_RATIO</th>\n",
       "      <th>MON_12_ACM_NET_ACT_CNT</th>\n",
       "      <th>DMD_GAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4602.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>205.3</td>\n",
       "      <td>87638.20</td>\n",
       "      <td>25246672.6</td>\n",
       "      <td>6102.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2865.87</td>\n",
       "      <td>2061.68</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.113021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-880.0</td>\n",
       "      <td>-0.126089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2642.0</td>\n",
       "      <td>2292.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>759421.45</td>\n",
       "      <td>324547596.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>-1200816.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2066.52</td>\n",
       "      <td>1640.39</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-0.108507</td>\n",
       "      <td>1200818.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>0.736807</td>\n",
       "      <td>-3780.0</td>\n",
       "      <td>-0.406949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90162.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>178.7</td>\n",
       "      <td>60669325.80</td>\n",
       "      <td>147173922.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>506.52</td>\n",
       "      <td>486.19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-740.0</td>\n",
       "      <td>-0.012881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3562.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>128.7</td>\n",
       "      <td>4099888.10</td>\n",
       "      <td>1825444.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>943.61</td>\n",
       "      <td>934.58</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.449134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.842087</td>\n",
       "      <td>-2570.0</td>\n",
       "      <td>-2.663800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>412.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>58.7</td>\n",
       "      <td>9000002.00</td>\n",
       "      <td>9000002.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>282.00</td>\n",
       "      <td>182.32</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-170.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGN_CNT_RCT_12_MON  ICO_CUR_MON_ACM_TRX_TM  NB_RCT_3_MON_LGN_TMS_AGV  \\\n",
       "0              4602.0                   102.0                     205.3   \n",
       "1              2642.0                  2292.0                    1442.0   \n",
       "2             90162.0                   102.0                     178.7   \n",
       "3              3562.0                   402.0                     128.7   \n",
       "4               412.0                    12.0                      58.7   \n",
       "\n",
       "   AGN_AGR_LATEST_AGN_AMT  ICO_CUR_MON_ACM_TRX_AMT  \\\n",
       "0                87638.20               25246672.6   \n",
       "1               759421.45              324547596.7   \n",
       "2             60669325.80              147173922.3   \n",
       "3              4099888.10                1825444.0   \n",
       "4              9000002.00                9000002.0   \n",
       "\n",
       "   PUB_TO_PRV_TRX_AMT_CUR_YEAR  MON_12_EXT_SAM_NM_TRSF_OUT_CNT  \\\n",
       "0                       6102.0                             2.0   \n",
       "1                          2.0                           632.0   \n",
       "2                          2.0                             2.0   \n",
       "3                          2.0                             2.0   \n",
       "4                          2.0                             2.0   \n",
       "\n",
       "   MON_12_EXT_SAM_AMT  MON_12_TRX_AMT_MAX_AMT_PCTT  \\\n",
       "0                 2.0                          3.1   \n",
       "1          -1200816.4                          2.3   \n",
       "2                 2.0                          2.1   \n",
       "3                 2.0                          2.2   \n",
       "4                 2.0                          2.0   \n",
       "\n",
       "   CUR_YEAR_PUB_TO_PRV_TRX_PTY_CNT  ...   REG_DT   OPN_TM  \\\n",
       "0                             12.0  ...  2865.87  2061.68   \n",
       "1                             42.0  ...  2066.52  1640.39   \n",
       "2                             42.0  ...   506.52   486.19   \n",
       "3                             42.0  ...   943.61   934.58   \n",
       "4                             42.0  ...   282.00   182.32   \n",
       "\n",
       "   HLD_FGN_CCY_ACT_NBR  CAGR_YEARLY_AGV_TRX_CNT  MON_12_EXR_SAM_NET_AMT  \\\n",
       "0                 12.0                -0.113021                     0.0   \n",
       "1                 32.0                -0.108507               1200818.4   \n",
       "2                  2.0                 0.012132                     0.0   \n",
       "3                  2.0                -0.449134                     0.0   \n",
       "4                  2.0                 0.019048                     0.0   \n",
       "\n",
       "   ENCASH_CNTR_RATIO  MON_12_ACT_NET_50_UP_CNT_PTY_QTY  \\\n",
       "0                1.0                              10.0   \n",
       "1                1.0                             -60.0   \n",
       "2                1.0                              50.0   \n",
       "3                1.0                              10.0   \n",
       "4                1.0                             -10.0   \n",
       "\n",
       "   AGN_CUR_YEAR_WAGE_RATIO  MON_12_ACM_NET_ACT_CNT   DMD_GAP  \n",
       "0                 1.000000                  -880.0 -0.126089  \n",
       "1                 0.736807                 -3780.0 -0.406949  \n",
       "2                 1.000000                  -740.0 -0.012881  \n",
       "3                 0.842087                 -2570.0 -2.663800  \n",
       "4                 1.000000                  -170.0  0.000000  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9ef364-b4cf-4116-bd1c-1de668a08e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.drop(columns='ICO_CUR_MON_ACM_TRX_TM', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9fd67c-30c5-465f-b132-a80564649778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(columns='ICO_CUR_MON_ACM_TRX_TM', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e0426d2-db8c-4c6f-8980-79530109e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb  # 模型\n",
    "import pandas as pd  # 数据处理包\n",
    "import numpy as np  # 数据处理包\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split  # 分层五折验证包、寻找最优参函数、切分数据\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix  # 准确率、roc计算、auc计算、混淆矩阵\n",
    "import gc  # 处理缓存，有兴趣的可以搜搜怎么使用\n",
    "import warnings  # 忽略普通警告，不打印太多东西\n",
    "\n",
    "\n",
    "\n",
    "def just_num_leaves(X, y, start_num=10, end_num=101, step=10):\n",
    "    \"\"\"\n",
    "    功能: 找到最优num_leaves参数，以此类推找出全部的最优参\n",
    "    why: 最优参数组能让模型效果更好，一般提升在0~5%左右，如果提升超过5%，那么就要考虑特征是否选取正确，是否有过多的噪音数据。\n",
    "    X: 数据X（无标签/df型）\n",
    "    y: 数据y（标签/df型）\n",
    "    start_num: 开始值\n",
    "    end_num: 最大值\n",
    "    step: 步数\n",
    "    return: 最佳num_leaves\n",
    "    \"\"\"\n",
    "    param_dic = {'num_leaves': range(start_num, end_num, step),\n",
    "                # 'learning_rate': [2e-2, 2e-3, 1e-4],\n",
    "                }\n",
    "    gscv = GridSearchCV(estimator=lgb.LGBMClassifier(max_depth=20, min_data_in_bin=5, max_bin=200,\n",
    "                                                     min_child_samples=90, n_estimators=20000,\n",
    "                                                     objective='binary', boosting_type='gbdt', learning_rate=0.02,\n",
    "                                                     lambda_l2=5),\n",
    "                       param_grid=param_dic, scoring='roc_auc', cv=5)\n",
    "    gscv.fit(X, y)\n",
    "    print(\"best_pocrams:{0}\".format(gscv.best_params_))\n",
    "    print(\"best_score:{0}\".format(gscv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ae12f885-5bcd-4109-b790-6061d6fcef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_resampled2.iloc[:35000, :], X_resampled2.iloc[35000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9a5fa483-a8c2-4018-bdf3-7f7e7f590157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LABEL\n",
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        1.0\n",
       "4        1.0\n",
       "...      ...\n",
       "59995    1.0\n",
       "59996    1.0\n",
       "59997    1.0\n",
       "59998    1.0\n",
       "59999    1.0\n",
       "\n",
       "[60000 rows x 1 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_resampled = pd.DataFrame(y_resampled)\n",
    "y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fb49a251-6161-49be-ac0a-f5392beab9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y_resampled.iloc[:35000, :], y_resampled.iloc[35000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "48be64ae-0216-42ab-9bde-b4a56da8ef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set=5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\TRACYT~1\\AppData\\Local\\Temp/ipykernel_11828/3760901862.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjust_num_leaves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_resampled2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\TRACYT~1\\AppData\\Local\\Temp/ipykernel_11828/26451112.py\u001b[0m in \u001b[0;36mjust_num_leaves\u001b[1;34m(X, y, start_num, end_num, step)\u001b[0m\n\u001b[0;32m     26\u001b[0m                                                      lambda_l2=5),\n\u001b[0;32m     27\u001b[0m                        param_grid=param_dic, scoring='roc_auc', cv=5)\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mgscv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"best_pocrams:{0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgscv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"best_score:{0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgscv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    965\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m         super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n\u001b[0m\u001b[0;32m    968\u001b[0m                     \u001b[0meval_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_sample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m                     \u001b[0meval_class_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_init_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m         self._Booster = train(\n\u001b[0m\u001b[0;32m    749\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    290\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3019\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3020\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot update due to null objective function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3021\u001b[1;33m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[0;32m   3022\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "just_num_leaves(X_resampled2, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "43bde126-a6de-4144-8994-8d0f64a050df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb  # 模型\n",
    "import pandas as pd  # 数据处理包\n",
    "import numpy as np  # 数据处理包\n",
    "from sklearn import metrics  # 混淆句子\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split  # 分层五折验证包、寻找最优参函数、切分数据\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix  # 准确率、roc计算、auc计算、混淆矩阵\n",
    "import itertools  # 处理混淆矩阵\n",
    "import gc  # 处理缓存，有兴趣的可以搜搜怎么使用\n",
    "import warnings  # 忽略普通警告，不打印太多东西\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def train_5_cross(df_pre, X,y, X_test_v1,y_test_v1, thresholds=0.45, id_1='id', csv_name=0):\n",
    "    \"\"\"\n",
    "    功能: 五折训练并输出名单\n",
    "    why: 5折一般是效果比较稳定的，用于线下做的。\n",
    "    X: 训练数据X（无标签/df型）\n",
    "    y: 训练数据y（标签/df型）\n",
    "    X_test_v1: 预测数据X（无标签/df型）\n",
    "    y_test_v1: 预测数据y（无标签/df型）\n",
    "    thresholds: 阈值选择，默认0.45高精确率\n",
    "    csv_name: 保存csv的名称，默认不保存\n",
    "    returen:\n",
    "        模型，客户名单及情况\n",
    "    \"\"\"\n",
    "    vali_auc_num=0  # 验证集AUC\n",
    "    vali_recall_num=0  # 验证集召回率\n",
    "    vali_precision_num=0  # 验证集精确率\n",
    "    test_auc_num=0  # 预测集AUC\n",
    "    test_recall_num=0  # 预测集召回率\n",
    "    test_precision_num=0  # 预测集精确率\n",
    "    y_pred_input = np.zeros(len(X_test_v1))  # 相应大小的零矩阵\n",
    "    print(\"=============开始训练================\")\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)  # 分层采样, n_splits为几折\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print(\"第 {} 次训练...\".format(fold_+1))\n",
    "        train_x, trai_y = X.loc[trn_idx], y.loc[trn_idx]\n",
    "        vali_x, vali_y = X.loc[val_idx], y.loc[val_idx]\n",
    "        \n",
    "        # 以下为调过参的lgb模型\n",
    "        clf = lgb.LGBMClassifier(max_depth=20, min_data_in_bin=5, max_bin=200,\n",
    "                                min_child_samples=90, num_leaves=20, n_estimators=20000,\n",
    "                                objective='binary', boosting_type='gbdt', learning_rate=0.002,\n",
    "                                lambda_l2=5)\n",
    "        clf.fit(train_x, trai_y, eval_set=[(train_x, trai_y), (vali_x, vali_y)], verbose=0,\n",
    "               early_stopping_rounds=100, eval_metric='roc_auc')\n",
    "        \n",
    "        # 不懂的去GitHub看搜LightGBM的参数解释\n",
    "        \n",
    "        # ===============验证集AUC操作===================\n",
    "        y_prb = clf.predict_proba(vali_x)[:,1]  # 获取预测概率\n",
    "        # fpr:在实际为正的样本中，被正确判断为正的比例。tpr:在实际为负的样本中，被正确判断为负的比例。thres为阈值\n",
    "        fpr, tpr, thres = roc_curve(vali_y, y_prb)\n",
    "        vali_roc_auc = auc(fpr, tpr)  # 获取验证集auc\n",
    "        vali_auc_num += vali_roc_auc  # 将本次auc加入总值里\n",
    "        print(\"vali auc = {0:.4}\".format(vali_roc_auc))  # 本次auc的值\n",
    "        # ===============预测集AUC操作===================\n",
    "        y_prb_test = clf.predict_proba(X_test_v1)[:,1]  # 获取预测概率\n",
    "        fpr, tpr, thres = roc_curve(y_test_v1, y_prb_test)\n",
    "        test_roc_auc = auc(fpr, tpr)\n",
    "        test_auc_num += test_roc_auc\n",
    "        print(\"test auc = {0:.4}\".format(test_roc_auc))\n",
    "        \n",
    "        # ===============验证metric操作===================\n",
    "        y_pre_proba = clf.predict_proba(vali_x.values)\n",
    "        y_predictions = y_pre_proba[:, 1]>thresholds  # 取阈值多少以上的为True\n",
    "        cnf_matrix = confusion_matrix(vali_y, y_predictions)  # 建立矩阵\n",
    "        np.set_printoptions(precision=2)  # 控制在两位数\n",
    "        vali_recall = '{0:.3f}'.format(cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))  # 召回率\n",
    "        vali_precision = '{0:.3f}'.format(cnf_matrix[1,1]/(cnf_matrix[0,1]+cnf_matrix[1,1]))  # 精确率\n",
    "        print(\"vali_metric: \", vali_recall, vali_precision)\n",
    "        vali_recall_num += float(vali_recall)  # 将本次召回率加入总值里\n",
    "        vali_precision_num += float(vali_precision)  # 将本次精确率加入总值里\n",
    "        # ===============预测metric操作===================\n",
    "        y_pre_proba_test = clf.predict_proba(X_test_v1.values)\n",
    "        y_predictions_test = y_pre_proba_test[:, 1]>thresholds  # 取阈值多少以上的为True\n",
    "        cnf_matrix_test = confusion_matrix(y_test_v1, y_predictions_test)  # 建立矩阵\n",
    "        np.set_printoptions(precision=2)  # 控制在两位数\n",
    "        test_recall = '{0:.3f}'.format(cnf_matrix_test[1,1]/(cnf_matrix_test[1,0]+cnf_matrix_test[1,1]))  # 召回率\n",
    "        test_precision = '{0:.3f}'.format(cnf_matrix_test[1,1]/(cnf_matrix_test[0,1]+cnf_matrix_test[1,1]))  # 精确率\n",
    "        print(\"test_metric: \", test_recall, test_precision)\n",
    "        test_recall_num += float(test_recall)  # 将本次召回率加入总值里\n",
    "        test_precision_num += float(test_precision)  # 将本次精确率加入总值里\n",
    "        y_pred_input += y_pre_proba_test[:, 1]  # 将每次的预测的结果写入数组中\n",
    "        \n",
    "    print(\"5折泛化，验证集AUC：{0:.3f}\".format(vali_auc_num/5))  # 前面是做了5次相加，所以这次要除以5\n",
    "    print(\"5折泛化，预测集AUC：{0:.3f}\".format(test_auc_num/5))\n",
    "    \n",
    "    print(\"5折泛化，验证集recall：{0:.3f}\".format(vali_recall_num/5))\n",
    "    print(\"5折泛化，验证集precision：{0:.3f}\".format(vali_recall_num/5))\n",
    "    \n",
    "    print(\"5折泛化，预测集recall：{0:.3f}\".format(test_recall_num/5))\n",
    "    print(\"5折泛化，预测集precision：{0:.3f}\".format(test_recall_num/5))\n",
    "    \n",
    "    print(\"================开始输出名单==================\")\n",
    "    y_pred_input_end = y_pred_input / 5  # 前面是做了5次相加，所以这次要除以5\n",
    "    y_pred_input_precision = y_pred_input_end > thresholds  # 获取高精确率的标签\n",
    "    submission = pd.DataFrame({\n",
    "                              \"概率\": y_pred_input_end,\n",
    "                              \"高精确\": y_pred_input_precision})\n",
    "    if csv_name != 0:\n",
    "        submission.to_csv(\"%s预测名单.csv\" % csv_name, index=False)  # 保存\n",
    "    print(\"================输出名单名单==================\")\n",
    "    print(submission.head(5))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b13be877-3d9e-4c4e-a9a3-d8475d674167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id]\n",
       "Index: []"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre = pd.DataFrame(columns=['id'], )\n",
    "df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9401d565-6ad9-4f69-b045-995e10a7177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============开始训练================\n",
      "第 1 次训练...\n",
      "vali auc = 0.8634\n",
      "test auc = 0.8665\n",
      "vali_metric:  0.669 0.639\n",
      "test_metric:  0.668 0.968\n",
      "第 2 次训练...\n",
      "[LightGBM] [Warning] lambda_l2 is set=5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5\n",
      "vali auc = 0.8662\n",
      "test auc = 0.8668\n",
      "vali_metric:  0.631 0.637\n",
      "test_metric:  0.664 0.968\n",
      "第 3 次训练...\n",
      "[LightGBM] [Warning] lambda_l2 is set=5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5\n",
      "vali auc = 0.8689\n",
      "test auc = 0.8666\n",
      "vali_metric:  0.653 0.650\n",
      "test_metric:  0.668 0.967\n",
      "第 4 次训练...\n",
      "[LightGBM] [Warning] lambda_l2 is set=5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5\n",
      "vali auc = 0.8736\n",
      "test auc = 0.8678\n",
      "vali_metric:  0.665 0.630\n",
      "test_metric:  0.672 0.968\n",
      "第 5 次训练...\n",
      "[LightGBM] [Warning] lambda_l2 is set=5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5\n",
      "vali auc = 0.8619\n",
      "test auc = 0.8665\n",
      "vali_metric:  0.643 0.638\n",
      "test_metric:  0.665 0.967\n",
      "5折泛化，验证集AUC：0.867\n",
      "5折泛化，预测集AUC：0.867\n",
      "5折泛化，验证集recall：0.652\n",
      "5折泛化，验证集precision：0.652\n",
      "5折泛化，预测集recall：0.667\n",
      "5折泛化，预测集precision：0.667\n",
      "================开始输出名单==================\n",
      "================输出名单名单==================\n",
      "         概率    高精确\n",
      "0  0.103967  False\n",
      "1  0.277608  False\n",
      "2  0.584799   True\n",
      "3  0.033688  False\n",
      "4  0.037144  False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(lambda_l2=5, learning_rate=0.002, max_bin=200, max_depth=20,\n",
       "               min_child_samples=90, min_data_in_bin=5, n_estimators=20000,\n",
       "               num_leaves=20, objective='binary')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_5_cross(df_pre, X_train,y_train, X_test,y_test, thresholds=0.45, id_1='id', csv_name=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "249e8754-67d9-436d-b654-348ef052092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # 数据处理包\n",
    "import numpy as np  # 数据处理包\n",
    "from sklearn import metrics  # 混淆矩阵\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split  # 分层五折验证包、寻找最优参函数、切分数据\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix  # 准确率、roc计算、auc计算、混淆矩阵\n",
    "import itertools  # 处理混淆矩阵\n",
    "import gc  # 处理缓存，有兴趣的可以搜搜怎么使用\n",
    "import warnings  # 忽略普通警告，不打印太多东西\n",
    "\n",
    "\n",
    "def metrics_ks(y, y_predicted):\n",
    "    \"\"\"\n",
    "    功能: 计算模型性能指标：ks， 基于ks找到最佳threshold值\n",
    "    why: ks值越高，则模型效果越好\n",
    "    y: 数据y（标签/df型）\n",
    "    y_predicted: 概率值， 公式为：= clf.predict_proba(X)[:, 1]\n",
    "    return:\n",
    "        ks值\n",
    "        thres_ks值\n",
    "    \"\"\"\n",
    "    fpr, tpr, thres = metrics.roc_curve(y, y_predicted, pos_label=1)\n",
    "    ks = abs(fpr - tpr).max()  # abs:返回数字绝对值的函数\n",
    "    tmp = abs(fpr - tpr)\n",
    "    index_ks = np.where(tmp==ks)  # np.where: 返回符合条件的下标函数\n",
    "    thres_ks = thres[index_ks]\n",
    "    return ks, thres_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3ca54f9c-1fc1-4c4e-a721-273f1c2c15b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(lambda_l2=5, learning_rate=0.002, max_bin=200, max_depth=20,\n",
       "               min_child_samples=90, min_data_in_bin=5, n_estimators=20000,\n",
       "               num_leaves=20, objective='binary')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submission = pd.read_csv('E:/dasein_py/Data Analysis/招商银行/test B/1预测名单.csv')\n",
    "clf = lgb.LGBMClassifier(lambda_l2=5, learning_rate=0.002, max_bin=200, max_depth=20,\n",
    "               min_child_samples=90, min_data_in_bin=5, n_estimators=20000,\n",
    "               num_leaves=20, objective='binary')\n",
    "clf.fit(X_train, y_train, eval_set=[(X_train, y_train)], verbose=0,\n",
    "               early_stopping_rounds=100, eval_metric='roc_auc')\n",
    "# y_predicted = clf.predict_proba(X_test)[:, 1]\n",
    "# metrics_ks(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bf4ed4bd-b71c-4ec5-add6-d661c9df7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入五折工具及各个模型\n",
    "from sklearn.model_selection import StratifiedKFold  # 数据切分、分层五折验证包\n",
    "import lightgbm as lgb  # lgb模型 ,安装的方法是在anaconda promote里，直接pip install lightgbm 即可\n",
    "import xgboost as xgb  # xgb模型，安装的方法是在anaconda promote里，直接pip install xgboost 即可，和lightgbm一样\n",
    "# 设置skf\n",
    "data_seed = 2020\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=data_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e152b213-4027-45ec-b307-228a097c8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb和xgb的参数\n",
    "lgb_params = {\n",
    "    'boosting': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.002,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'feature_fraction': 0.8,\n",
    "    'feature_fraction_seed': 300, ### 特征抽样的随机种子\n",
    "    'bagging_seed': 3, ### 数据抽样的随机种子,取10个不同的，然后对结果求平均,todo:求出10个结果，然后求平均\n",
    "    #'is_unbalance': True   #### 第一种方法：设置is_unbalance为True，表明传入的数据集是类别不平衡的\n",
    "    #'scale_pos_weight': 98145/1855###负样本数量/正样本数量 -> scale_pos_weight * 正样本 == 负样本\n",
    "}\n",
    "xgb_params = {\n",
    "    'booster': 'gbtree',  ##提升类型\n",
    "    'objective': 'binary:logistic',  ###目标函数\n",
    "    'eval_metric': 'auc',  ##评价函数\n",
    "    'eta': 0.1,  ### 学习率 ，一般0.0几\n",
    "    'max_depth': 6,  ###树最大深度\n",
    "    'min_child_weight': 1,  ###最小样本二阶梯度权重, 取值是整数\n",
    "    'subsample': 0.9,  ###训练数据采样 ,取值0.0~1.0之间\n",
    "    'colsample_bytree': 0.9,  ###训练特征采样，取值0.0~1.0之间\n",
    "    'lambda': 1,  ## l2正则，取值是整数\n",
    "    'alpha': 0,   ### l1正则，取值整数\n",
    "    'silent': 1   ### 取值1控制xgboost训练信息不输出\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a099dc1d-88d5-4512-96ac-f3eec1e1d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_train = pd.DataFrame()  # 定义df数据，以便做融合\n",
    "blend_test = pd.DataFrame()  # 定义df数据，以便做融合\n",
    "feats = list(X_train.columns)\n",
    "import pandas as pd  # 数据处理包\n",
    "import numpy as np  # 数据处理包\n",
    "from sklearn.metrics import roc_auc_score  # roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9cd7b13f-65b4-4c78-8ba0-90cbed57d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training fold:  1\n",
      "[LightGBM] [Info] Number of positive: 24000, number of negative: 24000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 48000, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.874756\n",
      "[100]\tvalid_0's auc: 0.875711\n",
      "[150]\tvalid_0's auc: 0.876639\n",
      "[200]\tvalid_0's auc: 0.877612\n",
      "[250]\tvalid_0's auc: 0.87824\n",
      "[300]\tvalid_0's auc: 0.8786\n",
      "[350]\tvalid_0's auc: 0.878884\n",
      "[400]\tvalid_0's auc: 0.879421\n",
      "[450]\tvalid_0's auc: 0.879726\n",
      "[500]\tvalid_0's auc: 0.88003\n",
      "[550]\tvalid_0's auc: 0.880566\n",
      "[600]\tvalid_0's auc: 0.880882\n",
      "[650]\tvalid_0's auc: 0.881217\n",
      "[700]\tvalid_0's auc: 0.881643\n",
      "[750]\tvalid_0's auc: 0.88197\n",
      "[800]\tvalid_0's auc: 0.882345\n",
      "[850]\tvalid_0's auc: 0.882705\n",
      "[900]\tvalid_0's auc: 0.883123\n",
      "[950]\tvalid_0's auc: 0.883482\n",
      "[1000]\tvalid_0's auc: 0.88379\n",
      "[1050]\tvalid_0's auc: 0.884241\n",
      "[1100]\tvalid_0's auc: 0.884577\n",
      "[1150]\tvalid_0's auc: 0.884939\n",
      "[1200]\tvalid_0's auc: 0.885305\n",
      "[1250]\tvalid_0's auc: 0.885672\n",
      "[1300]\tvalid_0's auc: 0.886026\n",
      "[1350]\tvalid_0's auc: 0.886363\n",
      "[1400]\tvalid_0's auc: 0.886708\n",
      "[1450]\tvalid_0's auc: 0.886987\n",
      "[1500]\tvalid_0's auc: 0.887278\n",
      "[1550]\tvalid_0's auc: 0.887654\n",
      "[1600]\tvalid_0's auc: 0.888034\n",
      "[1650]\tvalid_0's auc: 0.888367\n",
      "[1700]\tvalid_0's auc: 0.888642\n",
      "[1750]\tvalid_0's auc: 0.889033\n",
      "[1800]\tvalid_0's auc: 0.88936\n",
      "[1850]\tvalid_0's auc: 0.88973\n",
      "[1900]\tvalid_0's auc: 0.890091\n",
      "[1950]\tvalid_0's auc: 0.890408\n",
      "[2000]\tvalid_0's auc: 0.890668\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.890668\n",
      "auc score:  0.8906683888888889\n",
      "training fold:  2\n",
      "[LightGBM] [Info] Number of positive: 24000, number of negative: 24000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 48000, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.876992\n",
      "[100]\tvalid_0's auc: 0.878702\n",
      "[150]\tvalid_0's auc: 0.879493\n",
      "[200]\tvalid_0's auc: 0.879986\n",
      "[250]\tvalid_0's auc: 0.88049\n",
      "[300]\tvalid_0's auc: 0.880632\n",
      "[350]\tvalid_0's auc: 0.880935\n",
      "[400]\tvalid_0's auc: 0.881244\n",
      "[450]\tvalid_0's auc: 0.881513\n",
      "[500]\tvalid_0's auc: 0.881805\n",
      "[550]\tvalid_0's auc: 0.8823\n",
      "[600]\tvalid_0's auc: 0.882677\n",
      "[650]\tvalid_0's auc: 0.882904\n",
      "[700]\tvalid_0's auc: 0.883233\n",
      "[750]\tvalid_0's auc: 0.883547\n",
      "[800]\tvalid_0's auc: 0.883884\n",
      "[850]\tvalid_0's auc: 0.88421\n",
      "[900]\tvalid_0's auc: 0.884656\n",
      "[950]\tvalid_0's auc: 0.884929\n",
      "[1000]\tvalid_0's auc: 0.885232\n",
      "[1050]\tvalid_0's auc: 0.885567\n",
      "[1100]\tvalid_0's auc: 0.885816\n",
      "[1150]\tvalid_0's auc: 0.886101\n",
      "[1200]\tvalid_0's auc: 0.886424\n",
      "[1250]\tvalid_0's auc: 0.886784\n",
      "[1300]\tvalid_0's auc: 0.887114\n",
      "[1350]\tvalid_0's auc: 0.887383\n",
      "[1400]\tvalid_0's auc: 0.887731\n",
      "[1450]\tvalid_0's auc: 0.888039\n",
      "[1500]\tvalid_0's auc: 0.888355\n",
      "[1550]\tvalid_0's auc: 0.888669\n",
      "[1600]\tvalid_0's auc: 0.888967\n",
      "[1650]\tvalid_0's auc: 0.889279\n",
      "[1700]\tvalid_0's auc: 0.889518\n",
      "[1750]\tvalid_0's auc: 0.889898\n",
      "[1800]\tvalid_0's auc: 0.890139\n",
      "[1850]\tvalid_0's auc: 0.890445\n",
      "[1900]\tvalid_0's auc: 0.89079\n",
      "[1950]\tvalid_0's auc: 0.891077\n",
      "[2000]\tvalid_0's auc: 0.891315\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.891315\n",
      "auc score:  0.8913145833333334\n",
      "training fold:  3\n",
      "[LightGBM] [Info] Number of positive: 24000, number of negative: 24000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 48000, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.871199\n",
      "[100]\tvalid_0's auc: 0.872708\n",
      "[150]\tvalid_0's auc: 0.873663\n",
      "[200]\tvalid_0's auc: 0.87449\n",
      "[250]\tvalid_0's auc: 0.875343\n",
      "[300]\tvalid_0's auc: 0.875675\n",
      "[350]\tvalid_0's auc: 0.876106\n",
      "[400]\tvalid_0's auc: 0.876555\n",
      "[450]\tvalid_0's auc: 0.87693\n",
      "[500]\tvalid_0's auc: 0.877288\n",
      "[550]\tvalid_0's auc: 0.877703\n",
      "[600]\tvalid_0's auc: 0.878116\n",
      "[650]\tvalid_0's auc: 0.878416\n",
      "[700]\tvalid_0's auc: 0.878812\n",
      "[750]\tvalid_0's auc: 0.879157\n",
      "[800]\tvalid_0's auc: 0.879506\n",
      "[850]\tvalid_0's auc: 0.879927\n",
      "[900]\tvalid_0's auc: 0.880386\n",
      "[950]\tvalid_0's auc: 0.880668\n",
      "[1000]\tvalid_0's auc: 0.88099\n",
      "[1050]\tvalid_0's auc: 0.881388\n",
      "[1100]\tvalid_0's auc: 0.881637\n",
      "[1150]\tvalid_0's auc: 0.881961\n",
      "[1200]\tvalid_0's auc: 0.882282\n",
      "[1250]\tvalid_0's auc: 0.882633\n",
      "[1300]\tvalid_0's auc: 0.883028\n",
      "[1350]\tvalid_0's auc: 0.883385\n",
      "[1400]\tvalid_0's auc: 0.883755\n",
      "[1450]\tvalid_0's auc: 0.884049\n",
      "[1500]\tvalid_0's auc: 0.884349\n",
      "[1550]\tvalid_0's auc: 0.884681\n",
      "[1600]\tvalid_0's auc: 0.88511\n",
      "[1650]\tvalid_0's auc: 0.885405\n",
      "[1700]\tvalid_0's auc: 0.885703\n",
      "[1750]\tvalid_0's auc: 0.886106\n",
      "[1800]\tvalid_0's auc: 0.886499\n",
      "[1850]\tvalid_0's auc: 0.886817\n",
      "[1900]\tvalid_0's auc: 0.887139\n",
      "[1950]\tvalid_0's auc: 0.887484\n",
      "[2000]\tvalid_0's auc: 0.887755\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.887755\n",
      "auc score:  0.8877550416666666\n",
      "training fold:  4\n",
      "[LightGBM] [Info] Number of positive: 24000, number of negative: 24000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 48000, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.874895\n",
      "[100]\tvalid_0's auc: 0.876273\n",
      "[150]\tvalid_0's auc: 0.877435\n",
      "[200]\tvalid_0's auc: 0.878263\n",
      "[250]\tvalid_0's auc: 0.87888\n",
      "[300]\tvalid_0's auc: 0.879339\n",
      "[350]\tvalid_0's auc: 0.879815\n",
      "[400]\tvalid_0's auc: 0.880132\n",
      "[450]\tvalid_0's auc: 0.880509\n",
      "[500]\tvalid_0's auc: 0.880867\n",
      "[550]\tvalid_0's auc: 0.881372\n",
      "[600]\tvalid_0's auc: 0.881675\n",
      "[650]\tvalid_0's auc: 0.88186\n",
      "[700]\tvalid_0's auc: 0.882198\n",
      "[750]\tvalid_0's auc: 0.882511\n",
      "[800]\tvalid_0's auc: 0.882861\n",
      "[850]\tvalid_0's auc: 0.883139\n",
      "[900]\tvalid_0's auc: 0.883607\n",
      "[950]\tvalid_0's auc: 0.883882\n",
      "[1000]\tvalid_0's auc: 0.884187\n",
      "[1050]\tvalid_0's auc: 0.884592\n",
      "[1100]\tvalid_0's auc: 0.884835\n",
      "[1150]\tvalid_0's auc: 0.88517\n",
      "[1200]\tvalid_0's auc: 0.885517\n",
      "[1250]\tvalid_0's auc: 0.885915\n",
      "[1300]\tvalid_0's auc: 0.886243\n",
      "[1350]\tvalid_0's auc: 0.8866\n",
      "[1400]\tvalid_0's auc: 0.887008\n",
      "[1450]\tvalid_0's auc: 0.887284\n",
      "[1500]\tvalid_0's auc: 0.887649\n",
      "[1550]\tvalid_0's auc: 0.888018\n",
      "[1600]\tvalid_0's auc: 0.888384\n",
      "[1650]\tvalid_0's auc: 0.888704\n",
      "[1700]\tvalid_0's auc: 0.888939\n",
      "[1750]\tvalid_0's auc: 0.889257\n",
      "[1800]\tvalid_0's auc: 0.889607\n",
      "[1850]\tvalid_0's auc: 0.889918\n",
      "[1900]\tvalid_0's auc: 0.890313\n",
      "[1950]\tvalid_0's auc: 0.890636\n",
      "[2000]\tvalid_0's auc: 0.890988\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.890988\n",
      "auc score:  0.890988486111111\n",
      "training fold:  5\n",
      "[LightGBM] [Info] Number of positive: 24000, number of negative: 24000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 48000, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.87145\n",
      "[100]\tvalid_0's auc: 0.872983\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid_0's auc: 0.873481\n",
      "auc score:  0.8734807916666667\n"
     ]
    }
   ],
   "source": [
    "# 训练lgb，用作第一层模型中的其中一个\n",
    "test_pred_lgb = 0  # 预测结果存放对象\n",
    "cv_score_lgb = []  # 存放每次auc的对象\n",
    "X, y = X_resampled2, y_resampled\n",
    "train_feats = np.zeros(X.shape[0])  # 整体训练的样本数量\n",
    "for idx, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    print('training fold: ', idx + 1)  # 遍历的第几次\n",
    "    train_x, valid_x = X.loc[train_idx], X.loc[test_idx]  # 拆分成训练集和验证集\n",
    "    train_y, valid_y = y.loc[train_idx], y.loc[test_idx]  # 拆分成训练集和验证集\n",
    "    dtrain = lgb.Dataset(train_x, train_y, feature_name=feats)  # 组成训练集\n",
    "    dvalid = lgb.Dataset(valid_x, valid_y, feature_name=feats)  # 组成验证集\n",
    "    model = lgb.train(lgb_params, dtrain, num_boost_round=2000, valid_sets=dvalid, early_stopping_rounds=50, verbose_eval=50)  # 定义lgb模型\n",
    "\n",
    "    valid_pred = model.predict(valid_x, num_iteration=model.best_iteration)  # 当前模型最佳参数并预测，num_iteration：选择最优的lgb\n",
    "    train_feats[test_idx] = valid_pred  # 每次把验证集的结果填入，做训练的结果集，由于是5折，所以每次都是1/5的数据，把它们当作lgb训练集特征\n",
    "    auc_score = roc_auc_score(valid_y, valid_pred)  # 计算auc\n",
    "    print('auc score: ', auc_score)\n",
    "    cv_score_lgb.append(auc_score)  # 存放验证集auc值\n",
    "    test_pred_lgb += model.predict(X_pred2, num_iteration=model.best_iteration)  # 预测结果并累加，做预测的结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ade29e85-e3b2-4c99-ba1b-4f151a60c7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73, 0.1 , 0.81, ..., 0.67, 0.85, 0.8 ])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats  # 训练的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3fe90983-a668-47b0-8e76-36409efce081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16, 0.14, 0.37, ..., 0.68, 0.27, 0.71])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_lgb /= 5\n",
    "test_pred_lgb  # 测试的结果，由于测试的结果是5折每次的累加，所以需要除于5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "162881f6-456e-4e79-870e-406023d2e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练结果和预测结果加入到blend数据集\n",
    "blend_train['lgb_feat'] = train_feats\n",
    "blend_test['lgb_feat'] = test_pred_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ea31ab55-7ef5-46c2-a318-ea88a4cb7206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training fold:  1\n",
      "[11:59:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-auc:0.86038\n",
      "[50]\teval-auc:0.89315\n",
      "[100]\teval-auc:0.90106\n",
      "[150]\teval-auc:0.90556\n",
      "[200]\teval-auc:0.90732\n",
      "[250]\teval-auc:0.91012\n",
      "[300]\teval-auc:0.91166\n",
      "[350]\teval-auc:0.91267\n",
      "[400]\teval-auc:0.91375\n",
      "[450]\teval-auc:0.91497\n",
      "[500]\teval-auc:0.91592\n",
      "[550]\teval-auc:0.91671\n",
      "[600]\teval-auc:0.91781\n",
      "[650]\teval-auc:0.91847\n",
      "[700]\teval-auc:0.91892\n",
      "[750]\teval-auc:0.91974\n",
      "[800]\teval-auc:0.92029\n",
      "[850]\teval-auc:0.92055\n",
      "[900]\teval-auc:0.92130\n",
      "[950]\teval-auc:0.92169\n",
      "[1000]\teval-auc:0.92190\n",
      "[1050]\teval-auc:0.92211\n",
      "[1100]\teval-auc:0.92241\n",
      "[1150]\teval-auc:0.92258\n",
      "[1200]\teval-auc:0.92284\n",
      "[1219]\teval-auc:0.92267\n",
      "auc score:  0.9228366666666666\n",
      "training fold:  2\n",
      "[12:00:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-auc:0.85907\n",
      "[50]\teval-auc:0.89342\n",
      "[100]\teval-auc:0.90031\n",
      "[150]\teval-auc:0.90484\n",
      "[200]\teval-auc:0.90742\n",
      "[250]\teval-auc:0.90989\n",
      "[300]\teval-auc:0.91220\n",
      "[350]\teval-auc:0.91356\n",
      "[400]\teval-auc:0.91492\n",
      "[450]\teval-auc:0.91553\n",
      "[500]\teval-auc:0.91620\n",
      "[550]\teval-auc:0.91651\n",
      "[600]\teval-auc:0.91679\n",
      "[650]\teval-auc:0.91710\n",
      "[700]\teval-auc:0.91741\n",
      "[750]\teval-auc:0.91800\n",
      "[800]\teval-auc:0.91823\n",
      "[850]\teval-auc:0.91888\n",
      "[900]\teval-auc:0.91925\n",
      "[950]\teval-auc:0.91966\n",
      "[1000]\teval-auc:0.92005\n",
      "[1050]\teval-auc:0.92035\n",
      "[1100]\teval-auc:0.92068\n",
      "[1150]\teval-auc:0.92098\n",
      "[1200]\teval-auc:0.92129\n",
      "[1250]\teval-auc:0.92119\n",
      "[1263]\teval-auc:0.92131\n",
      "auc score:  0.9213726527777779\n",
      "training fold:  3\n",
      "[12:00:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-auc:0.85586\n",
      "[50]\teval-auc:0.89058\n",
      "[100]\teval-auc:0.89846\n",
      "[150]\teval-auc:0.90304\n",
      "[200]\teval-auc:0.90577\n",
      "[250]\teval-auc:0.90859\n",
      "[300]\teval-auc:0.91011\n",
      "[350]\teval-auc:0.91202\n",
      "[400]\teval-auc:0.91413\n",
      "[450]\teval-auc:0.91511\n",
      "[500]\teval-auc:0.91626\n",
      "[550]\teval-auc:0.91700\n",
      "[600]\teval-auc:0.91779\n",
      "[650]\teval-auc:0.91846\n",
      "[700]\teval-auc:0.91927\n",
      "[750]\teval-auc:0.91976\n",
      "[800]\teval-auc:0.92043\n",
      "[850]\teval-auc:0.92099\n",
      "[900]\teval-auc:0.92134\n",
      "[950]\teval-auc:0.92178\n",
      "[1000]\teval-auc:0.92215\n",
      "[1050]\teval-auc:0.92231\n",
      "[1100]\teval-auc:0.92259\n",
      "[1150]\teval-auc:0.92273\n",
      "[1200]\teval-auc:0.92270\n",
      "[1250]\teval-auc:0.92308\n",
      "[1300]\teval-auc:0.92302\n",
      "[1350]\teval-auc:0.92326\n",
      "[1400]\teval-auc:0.92322\n",
      "[1414]\teval-auc:0.92326\n",
      "auc score:  0.9233013750000001\n",
      "training fold:  4\n",
      "[12:01:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-auc:0.86013\n",
      "[50]\teval-auc:0.89354\n",
      "[100]\teval-auc:0.90149\n",
      "[150]\teval-auc:0.90572\n",
      "[200]\teval-auc:0.90849\n",
      "[250]\teval-auc:0.91102\n",
      "[300]\teval-auc:0.91337\n",
      "[350]\teval-auc:0.91535\n",
      "[400]\teval-auc:0.91587\n",
      "[450]\teval-auc:0.91661\n",
      "[500]\teval-auc:0.91793\n",
      "[550]\teval-auc:0.91891\n",
      "[600]\teval-auc:0.91950\n",
      "[650]\teval-auc:0.92027\n",
      "[700]\teval-auc:0.92092\n",
      "[750]\teval-auc:0.92162\n",
      "[800]\teval-auc:0.92207\n",
      "[850]\teval-auc:0.92251\n",
      "[900]\teval-auc:0.92320\n",
      "[950]\teval-auc:0.92346\n",
      "[1000]\teval-auc:0.92408\n",
      "[1050]\teval-auc:0.92430\n",
      "[1100]\teval-auc:0.92455\n",
      "[1150]\teval-auc:0.92475\n",
      "[1200]\teval-auc:0.92466\n",
      "[1206]\teval-auc:0.92462\n",
      "auc score:  0.9247466944444445\n",
      "training fold:  5\n",
      "[12:02:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-auc:0.85248\n",
      "[50]\teval-auc:0.88954\n",
      "[100]\teval-auc:0.89693\n",
      "[150]\teval-auc:0.90152\n",
      "[200]\teval-auc:0.90502\n",
      "[250]\teval-auc:0.90773\n",
      "[300]\teval-auc:0.90887\n",
      "[350]\teval-auc:0.91063\n",
      "[400]\teval-auc:0.91241\n",
      "[450]\teval-auc:0.91371\n",
      "[500]\teval-auc:0.91462\n",
      "[550]\teval-auc:0.91547\n",
      "[600]\teval-auc:0.91626\n",
      "[650]\teval-auc:0.91679\n",
      "[700]\teval-auc:0.91731\n",
      "[750]\teval-auc:0.91782\n",
      "[800]\teval-auc:0.91867\n",
      "[850]\teval-auc:0.91900\n",
      "[900]\teval-auc:0.91949\n",
      "[950]\teval-auc:0.91968\n",
      "[1000]\teval-auc:0.91988\n",
      "[1050]\teval-auc:0.92003\n",
      "[1100]\teval-auc:0.92019\n",
      "[1150]\teval-auc:0.92053\n",
      "[1200]\teval-auc:0.92071\n",
      "[1250]\teval-auc:0.92085\n",
      "[1300]\teval-auc:0.92126\n",
      "[1350]\teval-auc:0.92192\n",
      "[1400]\teval-auc:0.92210\n",
      "[1450]\teval-auc:0.92214\n",
      "[1500]\teval-auc:0.92230\n",
      "[1550]\teval-auc:0.92253\n",
      "[1587]\teval-auc:0.92251\n",
      "auc score:  0.9225666527777778\n"
     ]
    }
   ],
   "source": [
    "# 训练xgb，用作第一层模型中的其中一个\n",
    "test_pred_xgb = 0 # 预测结果存放对象\n",
    "cv_score_xgb = []  # 存放每次auc的对象\n",
    "train_feats_xgb = np.zeros(X.shape[0])  # 整体训练的样本数量\n",
    "for idx, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    print('training fold: ', idx + 1) # 遍历的第几次\n",
    "    train_x, valid_x = X.loc[train_idx], X.loc[test_idx]  # 拆分成训练集和验证集\n",
    "    train_y, valid_y = y.loc[train_idx], y.loc[test_idx]  # 拆分成训练集和验证集\n",
    "    dtrain = xgb.DMatrix(train_x, train_y, feature_names=feats)  # 组成训练集\n",
    "    dvalid = xgb.DMatrix(valid_x, valid_y, feature_names=feats)  # 组成验证集\n",
    "    watchlist = [(dvalid, 'eval')]\n",
    "    model = xgb.train(xgb_params, dtrain, num_boost_round=2000, evals=watchlist, early_stopping_rounds=50, verbose_eval=50)  # 定义xgb模型\n",
    "\n",
    "    valid_pred = model.predict(dvalid, ntree_limit=model.best_iteration)  # 当前模型最佳参数并预测，ntree_limit：选择最优的xgb\n",
    "    train_feats_xgb[test_idx] = valid_pred  # 每次把验证集的结果填入，做训练的结果集，由于是5折，所以每次都是1/5的数据\n",
    "    auc_score = roc_auc_score(valid_y, valid_pred)  # 计算auc\n",
    "    print('auc score: ', auc_score)\n",
    "    cv_score_xgb.append(auc_score)  # 存放验证集auc值\n",
    "    dtest = xgb.DMatrix(X_pred2,feature_names=feats)  ##同时指定特征名字\n",
    "    test_pred_xgb += model.predict(dtest, ntree_limit=model.best_iteration)  # 预测结果并累加，做预测的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2d1290d4-bcb7-46c1-98b6-f017b5ccf86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83, 0.11, 0.84, ..., 0.66, 0.94, 0.99])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats_xgb  # 训练的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "47fff5dd-90e6-4ae9-8c9e-f3af99571f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练结果和预测结果加入到blend数据集\n",
    "blend_train['xgb_feat'] = train_feats_xgb\n",
    "blend_test['xgb_feat'] = test_pred_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "11e5dea5-7348-439b-8375-65dff0b0634c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgb_feat</th>\n",
       "      <th>xgb_feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.733542</td>\n",
       "      <td>0.832418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.104764</td>\n",
       "      <td>0.108267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.811674</td>\n",
       "      <td>0.837787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407046</td>\n",
       "      <td>0.572145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.459481</td>\n",
       "      <td>0.405450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lgb_feat  xgb_feat\n",
       "0  0.733542  0.832418\n",
       "1  0.104764  0.108267\n",
       "2  0.811674  0.837787\n",
       "3  0.407046  0.572145\n",
       "4  0.459481  0.405450"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "88315dff-ed48-4eb9-ac08-c9f99ec51528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgb_feat</th>\n",
       "      <th>xgb_feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.160672</td>\n",
       "      <td>0.110751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.140335</td>\n",
       "      <td>0.021491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.372588</td>\n",
       "      <td>0.443294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.137228</td>\n",
       "      <td>0.034265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.735225</td>\n",
       "      <td>3.921711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lgb_feat  xgb_feat\n",
       "0  0.160672  0.110751\n",
       "1  0.140335  0.021491\n",
       "2  0.372588  0.443294\n",
       "3  0.137228  0.034265\n",
       "4  0.735225  3.921711"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7623f3e4-e76f-4cdf-b22d-fb13e54b761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression  # 载入lr模型，这里lr模型用作第二层模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8858c2cb-ec72-4e06-a8f3-81a694d0abe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(blend_train.values, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2cf44776-7116-4c60-a7d1-e0d9f5975dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13,  5.78]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coef_  # 特征权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "eaa3c937-1dab-4021-b5de-95d67f59a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_lr = lr_model.predict_proba(blend_test.values)[:,1]  # 第二层模型预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7bf0d6b6-9922-4f87-a849-a3b82318932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaderboard_score(test_df,prediction):\n",
    "    \"\"\"\n",
    "    定义评分函数\n",
    "    test_df: 测试集\n",
    "    prediction: 预测结果\n",
    "    reture: 输出结果分数\n",
    "    \"\"\"\n",
    "    label = test_df['LABEL'].values  # 拿出真实样本\n",
    "    assert len(prediction) == len(label)  # 断言其长度相等\n",
    "    print('stacking auc score: ', roc_auc_score(label, prediction))  # 计算评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a28f18e4-f85c-4874-93a1-04581f390e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8868414583333333, 0.006799913711467977)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cv_score_lgb), np.std(cv_score_lgb) # lgb验证集评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cf99301a-ad49-458a-81e8-cb006af8a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_result = 0.5 * test_pred_lgb + 0.5 * test_pred_xgb  # 模型平均加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "216f3fef-df53-47f2-a7e7-2a5017696e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_pred_lr)\n",
    "pd.DataFrame(test_pred_lr).to_csv('B9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fcf87d0c-4279-4466-9bdc-9a60285d938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacking auc score:  0.9975566408032103\n"
     ]
    }
   ],
   "source": [
    "get_leaderboard_score(y_test, test_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "04aa4387-ca96-4fe6-a7b0-6cd62446cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e9ed4a4-754e-4464-829d-891185ed9e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 26)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557bd67-8620-4ebd-903a-ad179cc9fe1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
